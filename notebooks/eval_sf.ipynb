{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "36041ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SF_EVAL', '.models', 'hfcache']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, librosa, numpy as np, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\" / \"hfcache\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "print(str(CACHE_DIR).split('/')[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "91fa8d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    GPT2LMHeadModel, AutoTokenizer\n",
    ")\n",
    "\n",
    "SR = 16_000\n",
    "BATCH_SIZE = 5\n",
    "WHISPER_ID = \"openai/whisper-tiny.en\"\n",
    "GPT2_ID = \"cwestnedge/gpt2-large-pubmed\"\n",
    "SHARED_VOCAB = 50257\n",
    "ALPHA = 0.3\n",
    "INIT_W_STEPS = 2\n",
    "MAX_STEPS = 256\n",
    "\n",
    "MANIFEST = \"../data/output/manifest.jsonl\"\n",
    "AUDIO_DIR = \"../data/output\"  \n",
    "\n",
    "device = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "272ebece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4):   0%|          | 0/30 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 30/30 [00:01<00:00, 27.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# def build_dataset(pattern: str) -> Dataset:\n",
    "#     def process_audio(batch):\n",
    "#         batch[\"audio\"] = [\n",
    "#             librosa.load(p, sr=SR, mono=True)[0].astype(np.float32)\n",
    "#             for p in batch[\"path\"]\n",
    "#         ]\n",
    "#         return batch\n",
    "\n",
    "#     paths = glob.glob(pattern)\n",
    "#     ds = Dataset.from_dict({\"path\": paths})\n",
    "#     ds = ds.map(process_audio, batched=True, batch_size=BATCH_SIZE, num_proc=4).remove_columns(\"path\")\n",
    "#     return ds\n",
    "\n",
    "def build_dataset(manifest_path: str) -> Dataset:\n",
    "    with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "\n",
    "    def add_audio(batch):\n",
    "        batch[\"audio\"] = [\n",
    "            librosa.load(f\"{AUDIO_DIR}/{fname}\", sr=SR, mono=True)[0].astype(np.float32)\n",
    "            for fname in batch[\"file\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return ds.map(add_audio, batched=True, batch_size=BATCH_SIZE, num_proc=4)\n",
    "\n",
    "def collate(batch):\n",
    "    audio = [b[\"audio\"] for b in batch]      # list[np.ndarray]\n",
    "    refs = [b[\"text\"]  for b in batch]      # ground-truth\n",
    "    uuids = [b[\"uuid\"]  for b in batch]      # for debugging / join\n",
    "    return audio, refs, uuids\n",
    "\n",
    "ds = build_dataset(MANIFEST)\n",
    "loader = DataLoader(\n",
    "    ds, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    collate_fn=collate,\n",
    "    num_workers=0, \n",
    "    pin_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "def8bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR).to(device).eval()\n",
    "\n",
    "gpt_tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR).to(device).eval()\n",
    "\n",
    "EOS_ID = processor.tokenizer.eos_token_id\n",
    "WHISPER_SPECIALS = set(processor.tokenizer.all_special_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "90cfc665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def fuse_whisper_gpt(wav: np.ndarray, alpha: float = 0.25) -> str:\n",
    "    \"\"\"Decode a single waveform with shallow fusion.\"\"\"\n",
    "    feats = processor(wav, sampling_rate=SR, return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "    dec_ids = torch.tensor([[whisper.config.decoder_start_token_id]], device=device)\n",
    "    gpt_ids = torch.empty(1, 0, dtype=torch.long, device=device)\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        w_logits = whisper(\n",
    "            feats, \n",
    "            decoder_input_ids=dec_ids, \n",
    "            use_cache=True\n",
    "            ).logits[:, -1] # (1, Vw)\n",
    "\n",
    "        if step < INIT_W_STEPS: # allow <s> and task token\n",
    "            next_id = w_logits.argmax(-1, keepdim=True)\n",
    "\n",
    "        else:\n",
    "            if gpt_ids.numel() == 0: # first LM step\n",
    "                gpt_ids = dec_ids\n",
    "\n",
    "            g_logits = gpt2(gpt_ids).logits[:, -1] # (1, Vg)\n",
    "\n",
    "            w_lp = F.log_softmax(w_logits[:, :SHARED_VOCAB], dim=-1)\n",
    "            g_lp = F.log_softmax(g_logits, dim=-1)\n",
    "            fused = w_lp + alpha * g_lp  # (1, Vg)\n",
    "            next_id = fused.argmax(-1, keepdim=True)\n",
    "\n",
    "        dec_ids = torch.cat([dec_ids, next_id], dim=-1)\n",
    "\n",
    "        # refresh GPT‑2 context (strip Whisper special tokens)\n",
    "        cleaned = [t for t in dec_ids[0].tolist() if t not in WHISPER_SPECIALS]\n",
    "        gpt_ids = torch.tensor([cleaned], device=device)\n",
    "\n",
    "        if next_id.item() == EOS_ID:\n",
    "            break\n",
    "\n",
    "    return processor.batch_decode(dec_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ae0c4a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "@torch.no_grad()\n",
    "def fuse_whisper_gpt1(wav: np.ndarray, alpha: float=0.25, variance_match: bool = False) -> str:\n",
    "    \"\"\"Decode a single waveform with shallow fusion.\"\"\"\n",
    "    feats = processor(wav, sampling_rate=SR, return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "    dec_ids = torch.tensor([[whisper.config.decoder_start_token_id]], device=device)\n",
    "    gpt_ids = torch.empty(1, 0, dtype=torch.long, device=device)\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        w_logits = whisper(\n",
    "            feats, \n",
    "            decoder_input_ids=dec_ids, \n",
    "            use_cache=True\n",
    "            ).logits[:, -1] # (1, Vw)\n",
    "\n",
    "        if step < INIT_W_STEPS: # allow <s> and task token\n",
    "            next_id = w_logits.argmax(-1, keepdim=True)\n",
    "\n",
    "        else:\n",
    "            if gpt_ids.numel() == 0: # first LM step\n",
    "                gpt_ids = dec_ids\n",
    "\n",
    "            g_logits = gpt2(gpt_ids).logits[:, -1] # (1, Vg)\n",
    "\n",
    "            w_lp = F.log_softmax(w_logits[:, :SHARED_VOCAB], dim=-1)\n",
    "            g_lp = F.log_softmax(g_logits, dim=-1)\n",
    "\n",
    "            if variance_match and alpha > 0 : \n",
    "                w_lp = w_lp/w_lp.std(unbiased=False)\n",
    "                g_lp = g_lp/g_lp.std(unbiased=False).clamp_min(1e-6)\n",
    "\n",
    "            fused = w_lp.clone()\n",
    "            fused[:,:SHARED_VOCAB] += alpha * g_lp\n",
    "            next_id = fused.argmax(-1, keepdim=True)\n",
    "\n",
    "        \n",
    "        dec_ids = torch.cat([dec_ids, next_id], dim=-1)\n",
    "\n",
    "        # refresh GPT‑2 context (strip Whisper special tokens)\n",
    "        cleaned = [t for t in dec_ids[0].tolist() if t not in WHISPER_SPECIALS]\n",
    "        gpt_ids = torch.tensor([cleaned], device=device)\n",
    "\n",
    "        if next_id.item() == EOS_ID:\n",
    "            break\n",
    "\n",
    "    return processor.batch_decode(dec_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "54151076",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_mask = torch.tensor(\n",
    "    processor.tokenizer.get_special_tokens_mask(list(range(processor.tokenizer.vocab_size)),\n",
    "                                        already_has_special_tokens=True),\n",
    "    dtype=torch.bool, device=device\n",
    ")                                 # 1 = special, incl. those < 50 257\n",
    "\n",
    "@torch.no_grad()\n",
    "def fuse_whisper_gpt_fast(wav, alpha=0.25, var_match=False):\n",
    "    feats = processor(wav, sampling_rate=SR, return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "    dec_ids = torch.tensor([[whisper.config.decoder_start_token_id]], device=device)\n",
    "    gpt_ids = torch.empty_like(dec_ids, dtype=torch.long)[:, 0:0]\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        w_logits = whisper(feats, decoder_input_ids=dec_ids, use_cache=True).logits[:, -1]\n",
    "\n",
    "        if step < INIT_W_STEPS:\n",
    "            next_id = w_logits.argmax(-1, keepdim=True)\n",
    "        else:\n",
    "            if gpt_ids.numel() == 0:\n",
    "                # strip *all* whisper specials via tokenizer mask\n",
    "                keep = ~special_mask[dec_ids[0]]\n",
    "                gpt_ids = dec_ids[0, keep].unsqueeze(0)\n",
    "\n",
    "            g_logits = gpt2(gpt_ids).logits[:, -1]\n",
    "\n",
    "            w_lp = F.log_softmax(w_logits[:, :SHARED_VOCAB], dim=-1)\n",
    "            g_lp = F.log_softmax(g_logits, dim=-1)\n",
    "\n",
    "            if var_match and alpha:\n",
    "                w_lp = w_lp / w_lp.std(unbiased=False)\n",
    "                g_lp = g_lp / g_lp.std(unbiased=False).clamp_min(1e-6)\n",
    "\n",
    "            fused = w_lp + alpha * g_lp\n",
    "            next_id = fused.argmax(-1, keepdim=True)\n",
    "\n",
    "        dec_ids = torch.cat([dec_ids, next_id], dim=-1)\n",
    "\n",
    "        # feed GPT only if the token is still < 50 257\n",
    "        if next_id.item() < SHARED_VOCAB:\n",
    "            gpt_ids = torch.cat([gpt_ids, next_id], dim=-1)\n",
    "\n",
    "        if next_id.item() == processor.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return processor.batch_decode(dec_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "5e7e3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_txt, fusion_txt, gt_text = [], [], []\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for wavs, refs, ids in loader: \n",
    "        # VANILLA WHISPER\n",
    "        feats = processor(\n",
    "            wavs, \n",
    "            sampling_rate=SR,\n",
    "            return_tensors=\"pt\", \n",
    "            padding=True\n",
    "        ).input_features.to(device) # (B, 80, T_max)\n",
    "\n",
    "        # greedy decode\n",
    "        vanilla_ids = whisper.generate(feats)\n",
    "        vanilla_txt.extend(\n",
    "            processor.batch_decode(vanilla_ids, skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "        # SHALLOW FUSION\n",
    "        for wav in wavs:\n",
    "            # fused = fuse_whisper_gpt1(wav, alpha=0.0, variance_match=False)\n",
    "            fused = fuse_whisper_gpt_fast(wav, alpha=0.3)\n",
    "            # fused = fuse_whisper_gpt(wav, alpha=0.3)\n",
    "            fusion_txt.append(fused)\n",
    "\n",
    "        gt_text.extend(refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "b6707655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Clip 0 ---\n",
      "Source : The echocardiogram shows an ejection fraction of thirty-five percent with global hypokinesis.\n",
      "Whisper: The echocardiogram shows an ejection fraction of 35% with global hypo-kinesis.\n",
      "Fusion : The echocardiogram shows an ejection fraction of 35% with global hypokinesis.\n",
      "\n",
      "--- Clip 1 ---\n",
      "Source : Post-operative pathology confirmed a stage two-A adenocarcinoma of the sigmoid colon.\n",
      "Whisper: Post-operative pathology confirmed a stage 2A adenocarcinoma of the sigmoid colon.\n",
      "Fusion : Post-operative pathology confirmed a stage 2A adenocarcinoma of the sigmoid colon.\n",
      "\n",
      "--- Clip 2 ---\n",
      "Source : Her hemoglobin A-one-C has stabilized at seven point one percent after switching to semaglutide.\n",
      "Whisper: Her hemoglobin A1C has stabilized at 7.1% after switching to seem a glutide.\n",
      "Fusion : Her hemoglobin A1c has stabilized at 7.1% after switching to semaglutide.\n",
      "\n",
      "--- Clip 3 ---\n",
      "Source : Magnetic resonance imaging revealed a three-centimeter demyelinating plaque in the periventricular white matter.\n",
      "Whisper: Magnetic resonance imaging revealed a 3 cm demyelinating plaque in the periventricular white matter.\n",
      "Fusion : Magnetic resonance imaging revealed a 3 cm demyelinating plaque in the periventricular white matter.\n",
      "\n",
      "--- Clip 4 ---\n",
      "Source : We started ceftriaxone, two grams given intravenously every twenty-four hours, for suspected bacterial meningitis.\n",
      "Whisper: We started Seftrac Zone 2 grams given intravenously every 24 hours for suspected bacterial meningitis.\n",
      "Fusion : We started Seftrac's own two grams given intravenously every 24 hours for suspected bacterial meningitis.\n",
      "\n",
      "--- Clip 5 ---\n",
      "Source : His B-N-P is one thousand two hundred forty picograms per milliliter, consistent with decompensated heart failure.\n",
      "Whisper: His BNP is 1,240 picograms per milliliter, consistent with decompensated heart failure.\n",
      "Fusion : His BNP is 1,240 picograms per milliliter, consistent with decompensated heart failure.\n",
      "\n",
      "--- Clip 6 ---\n",
      "Source : She is allergic to fluoroquinolones and developed Stevens-Johnson syndrome after taking ciprofloxacin.\n",
      "Whisper: She is allergic to fluoroquineolones and develops Steven's Johnson syndrome after taking Cybrofloxysin.\n",
      "Fusion : She is allergic to fluoroquineolones and develops Stevens-Johnson syndrome after taking Siprofloxacin.\n",
      "\n",
      "--- Clip 7 ---\n",
      "Source : Give zero point four milligrams of sublingual nitroglycerin as needed if chest pain is not relieved by rest.\n",
      "Whisper: give 0.4 milligrams of sublingual nitroglycerin as needed if chest pain is not relieved by rest.\n",
      "Fusion : give 0.4 mg of sublingual nitroglycerin as needed, if chest pain is not relieved by rest.\n",
      "\n",
      "--- Clip 8 ---\n",
      "Source : We documented a Mallampati class three airway before intubation.\n",
      "Whisper: We documented a molympati class 3 airway before intubation.\n",
      "Fusion : We documented a Mollum-Pottie-class 3-airway before intubation.\n",
      "\n",
      "--- Clip 9 ---\n",
      "Source : Colonoscopy identified a sessile serrated lesion in the transverse colon, removed with a cold snare.\n",
      "Whisper: Colonoscopy identified a sesile serrated lesion in the transverse colon removed with a cold snare.\n",
      "Fusion : Colonoscopy identified a sessile serrated lesion in the transverse colon, removed with a cold snare.\n",
      "\n",
      "--- Clip 10 ---\n",
      "Source : The infant’s Apgar scores were eight and nine at one and five minutes, respectively.\n",
      "Whisper: The infants' app-gar scores were 8' 9' 1' and 5' respectively.\n",
      "Fusion : The infants' Apgar scores were 8-9 at 1-5 minutes, respectively.\n",
      "\n",
      "--- Clip 11 ---\n",
      "Source : Start methylprednisolone, one gram intravenously each day, for acute optic neuritis.\n",
      "Whisper: Start methyl-prednisolone, one gram intravenously each day for acute optic neuritis.\n",
      "Fusion : Start methyl-prednisolone, one gram intravenously each day for acute optic neuritis.\n",
      "\n",
      "--- Clip 12 ---\n",
      "Source : Arterial blood gas shows a P-A-O-two of fifty-five millimeters of mercury on room air, indicating moderate hypoxemia.\n",
      "Whisper: Artarial Blood Gas shows a PIO 2 of 55 millimeters of mercury on room air indicating moderate hypoxemia.\n",
      "Fusion : Artarial blood gas shows a PIO 2 of 55 mm of mercury on room air, indicating moderate hypoxemia.\n",
      "\n",
      "--- Clip 13 ---\n",
      "Source : Current Procedural Terminology code nine three three zero six for transthoracic echocardiography was partially denied because modifiers were missing.\n",
      "Whisper: Current procedural terminology code 93306 for trans-thoracic echocardiography was partially denied because modifiers were missing.\n",
      "Fusion : Current procedural terminology code 93306. For trans-thoracic echocardiography, was partially denied because modifiers were missing.\n",
      "\n",
      "--- Clip 14 ---\n",
      "Source : The member’s deductible was met, so the three-hundred-twenty-dollar coinsurance on H-C-P-C-S code J seventeen forty should be waived.\n",
      "Whisper: The members deductible was met, so the $320 coin insurance on HCPCS code J1740 should be waived.\n",
      "Fusion : The members' deductible was met, so the $320 coinsurance on HCPCS, Code J1740, should be waived.\n",
      "\n",
      "--- Clip 15 ---\n",
      "Source : The patient presents with intermittent angina and a positive troponin I of zero point three six nanograms per milliliter.\n",
      "Whisper: The patient presents with intermittent angina and a positive troponin 1 of 0.36 nanograms per milliliter.\n",
      "Fusion : The patient presents with intermittent angina and a positive troponin 1 of 0.36 nanograms per milliliter.\n",
      "\n",
      "--- Clip 16 ---\n",
      "Source : International Classification of Diseases, tenth revision, code J forty-five point nine zero nine was flagged as unspecified asthma; a documentation update was requested.\n",
      "Whisper: International classification of diseases 10th revision code J45.909 was flagged as unspecified asthma. A documentation update was requested.\n",
      "Fusion : International classification of diseases, 10th revision, code J45.909, was flagged as unspecified asthma. A documentation update was requested.\n",
      "\n",
      "--- Clip 17 ---\n",
      "Source : Doppler ultrasound detected a non-compressible femoral vein, consistent with deep venous thrombosis.\n",
      "Whisper: Doppler ultrasound detected a non-compressible femoral vein consistent with deep venous thrombosis.\n",
      "Fusion : Doppler ultrasound detected a non-compressible femoral vein, consistent with deep venous thrombosis.\n",
      "\n",
      "--- Clip 18 ---\n",
      "Source : The pharmacy rejected the G-L-P-one authorization because the prior authorization expired on June thirtieth, twenty twenty-five.\n",
      "Whisper: The pharmacy rejected the GOP1 authorization because the prior authorization expired on June 30, 2025.\n",
      "Fusion : The pharmacy rejected the GOP-1 authorization because the prior authorization expired on June 30, 2025.\n",
      "\n",
      "--- Clip 19 ---\n",
      "Source : Diagnosis-related group three-thirty payment was reduced due to a coding discrepancy with a secondary diagnosis of hypokalemia.\n",
      "Whisper: Diagnosis-related group 330 payment was reduced due to a coding discrepancy with a secondary diagnosis of hypocalemia.\n",
      "Fusion : Diagnosis-related group 330 payment was reduced due to a coding discrepancy with a secondary diagnosis of hypokalemia.\n",
      "\n",
      "--- Clip 20 ---\n",
      "Source : The E-O-B shows a coordination-of-benefits adjustment after the Medicare crossover.\n",
      "Whisper: The EOB shows a coordination of benefits adjustment after the Medicare crossover.\n",
      "Fusion : the EOB shows a coordination of benefits adjustment after the Medicare crossover.\n",
      "\n",
      "--- Clip 21 ---\n",
      "Source : We need operative notes to support Current Procedural Terminology code two nine eight eight one for arthroscopic medial meniscectomy.\n",
      "Whisper: We need operative notes to support current procedural terminology code 29881 for arthroscopic medial menisectomy.\n",
      "Fusion : We need operative notes to support current procedural terminology code, 29881 for arthroscopic medial menisectomy.\n",
      "\n",
      "--- Clip 22 ---\n",
      "Source : Modifier twenty-five was omitted on the evaluation and management service, causing bundling with the injection.\n",
      "Whisper: Modifier 25 was omitted on the Evaluation and Management Service, causing bundling with the injection.\n",
      "Fusion : Modifier 25 was omitted on the evaluation and management service, causing bundling with the injection.\n",
      "\n",
      "--- Clip 23 ---\n",
      "Source : Denial code C-O one ninety-seven cites missing pre-certification for the lumbar laminectomy.\n",
      "Whisper: Denial Code CO197 cites missing pre-certification for the Lumbar Laminectomy.\n",
      "Fusion : Denial Code CO-197 cites missing pre-certification for the Lumbar Laminectomy.\n",
      "\n",
      "--- Clip 24 ---\n",
      "Source : The appeals team requests a radiology report to validate Current Procedural Terminology code seven four one seven seven for computed tomography of the abdomen and pelvis with contrast.\n",
      "Whisper: The Appeals Team requests a radiology report to validate current procedural terminology code 7-4-1777 for computed tomography of the abdomen and pelvis with contrast.\n",
      "Fusion : The Appeals Team requests a radiology report to validate current procedural terminology, code 7-4-1-7-7, for computed tomography of the abdomen and pelvis with contrast.\n",
      "\n",
      "--- Clip 25 ---\n",
      "Source : An out-of-network penalty applied because N-P-I one two one five nine eight three seven four six lacks a single-case agreement.\n",
      "Whisper: and out of network penalty applied because NPI1215983746 lacks a single case agreement.\n",
      "Fusion : and out of network penalty, applied because NPI1-215-983746, lacks a single-case agreement.\n",
      "\n",
      "--- Clip 26 ---\n",
      "Source : The ambulance claim billed A zero four two eight, but mileage code A zero four two five was missing, triggering partial payment.\n",
      "Whisper: The ambulance claimed build A0428, but mileage code A0425 was missing, triggering partial payment.\n",
      "Fusion : The ambulance claimed \"build a 0-4-2-8\" but mileage code A 0-4-2-5 was missing, triggering partial payment.\n",
      "\n",
      "--- Clip 27 ---\n",
      "Source : The prosthetic hip device falls under H-C-P-C-S code L eight six nine nine, which is not covered without a K modifier.\n",
      "Whisper: The prosthetic hip device falls under HCPCS code L8699, which is not covered without a K modifier.\n",
      "Fusion : The prosthetic hip device falls under HCPCS code L8-699, which is not covered without a K-modifier.\n",
      "\n",
      "--- Clip 28 ---\n",
      "Source : The high-cost threshold was exceeded; specialty medication coded J three four nine zero requires National Drug Code submission.\n",
      "Whisper: The high cost threshold was exceeded. Specialty medication coded J3490 requires national drug code submission.\n",
      "Fusion : The high-cost threshold was exceeded. Specialty medication coded J-34-90 requires national drug code submission.\n",
      "\n",
      "--- Clip 29 ---\n",
      "Source : Claim edits routed Current Procedural Terminology nine six three seven two to incidental when billed with nine nine two one four on the same date of service.\n",
      "Whisper: Claim edits routed, current procedural terminology 96372 to incidental when billed with 99214 on the same date of service.\n",
      "Fusion : Claimed edits routed, current procedural terminology 96372 to incidental, when billed with 99214 on the same date of service.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (g, w, f) in enumerate(zip(gt_text, vanilla_txt, fusion_txt)):\n",
    "    print(f\"--- Clip {i} ---\")\n",
    "    print(\"Source :\", g.strip())\n",
    "    print(\"Whisper:\", w.strip())\n",
    "    print(\"Fusion :\", f.strip())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda838b8",
   "metadata": {},
   "source": [
    "### BONEYARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "25472a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def detect_shared_prefix(wtok, gtok, hint=50257):\n",
    "    # fast: compare the bytes of the first `hint` tokens\n",
    "    for i in range(hint):\n",
    "        if wtok.convert_ids_to_tokens(i) != gtok.convert_ids_to_tokens(i):\n",
    "            raise ValueError(\n",
    "                f\"Token {i} differs between Whisper and GPT‑2 – \"\n",
    "                \"prefix assumption broken. Use the full w2g map.\"\n",
    "            )\n",
    "    return hint\n",
    "\n",
    "SHARED_VOCAB = detect_shared_prefix(processor.tokenizer, gpt_tok)  # ➜ 50257\n",
    "SHARED_VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e42e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False,  ..., False, False,  True], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "special_mask = torch.tensor(\n",
    "    processor.tokenizer.get_special_tokens_mask(list(range(processor.tokenizer.vocab_size)),\n",
    "                                        already_has_special_tokens=True),\n",
    "    dtype=torch.bool, device=device\n",
    ")                                 # 1 = special, incl. those < 50 257\n",
    "\n",
    "@torch.no_grad()\n",
    "def fuse_whisper_gpt_fast(wav, alpha=0.25, var_match=False):\n",
    "    feats = processor(wav, sampling_rate=SR,\n",
    "                      return_tensors=\"pt\").input_features.to(device)\n",
    "\n",
    "    dec_ids = torch.tensor([[whisper.config.decoder_start_token_id]],\n",
    "                           device=device)\n",
    "    gpt_ids = torch.empty_like(dec_ids, dtype=torch.long)[:, 0:0]\n",
    "\n",
    "    for step in range(MAX_STEPS):\n",
    "        w_logits = whisper(feats, decoder_input_ids=dec_ids,\n",
    "                           use_cache=True).logits[:, -1]\n",
    "\n",
    "        if step < INIT_W_STEPS:\n",
    "            next_id = w_logits.argmax(-1, keepdim=True)\n",
    "        else:\n",
    "            if gpt_ids.numel() == 0:\n",
    "                # strip *all* whisper specials via tokenizer mask\n",
    "                keep = ~special_mask[dec_ids[0]]\n",
    "                gpt_ids = dec_ids[0, keep].unsqueeze(0)\n",
    "\n",
    "            g_logits = gpt2(gpt_ids).logits[:, -1]\n",
    "\n",
    "            w_lp = F.log_softmax(w_logits[:, :SHARED_VOCAB], dim=-1)\n",
    "            g_lp = F.log_softmax(g_logits, dim=-1)\n",
    "\n",
    "            if var_match and alpha:\n",
    "                w_lp = w_lp / w_lp.std(unbiased=False)\n",
    "                g_lp = g_lp / g_lp.std(unbiased=False).clamp_min(1e-6)\n",
    "\n",
    "            fused = w_lp + alpha * g_lp\n",
    "            next_id = fused.argmax(-1, keepdim=True)\n",
    "\n",
    "        dec_ids = torch.cat([dec_ids, next_id], dim=-1)\n",
    "\n",
    "        # feed GPT only if the token is still < 50 257\n",
    "        if next_id.item() < SHARED_VOCAB:\n",
    "            gpt_ids = torch.cat([gpt_ids, next_id], dim=-1)\n",
    "\n",
    "        if next_id.item() == processor.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    return processor.batch_decode(dec_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ee855329",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([50256, 50257, 50258, 50259, 50260, 50261, 50262, 50263, 50264,\n",
       "       50265, 50266, 50267, 50268, 50269, 50270, 50271, 50272, 50273,\n",
       "       50274, 50275, 50276, 50277, 50278, 50279, 50280, 50281, 50282,\n",
       "       50283, 50284, 50285, 50286, 50287, 50288, 50289, 50290, 50291,\n",
       "       50292, 50293, 50294, 50295, 50296, 50297, 50298, 50299, 50300,\n",
       "       50301, 50302, 50303, 50304, 50305, 50306, 50307, 50308, 50309,\n",
       "       50310, 50311, 50312, 50313, 50314, 50315, 50316, 50317, 50318,\n",
       "       50319, 50320, 50321, 50322, 50323, 50324, 50325, 50326, 50327,\n",
       "       50328, 50329, 50330, 50331, 50332, 50333, 50334, 50335, 50336,\n",
       "       50337, 50338, 50339, 50340, 50341, 50342, 50343, 50344, 50345,\n",
       "       50346, 50347, 50348, 50349, 50350, 50351, 50352, 50353, 50354,\n",
       "       50355, 50356, 50357, 50358, 50359, 50360, 50361, 50362])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(processor.tokenizer.all_special_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944665e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LogitsProcessor\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class GPT2FusionProcessor(LogitsProcessor):\n",
    "#     def __init__(self, alpha, gpt2, shared_vocab, specials, variance_match):\n",
    "#         self.alpha         = alpha\n",
    "#         self.gpt2          = gpt2\n",
    "#         self.shared_vocab  = shared_vocab\n",
    "#         self.specials      = specials\n",
    "#         self.variance      = variance_match\n",
    "\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         \"\"\"\n",
    "#         `scores` is the logit vector Whisper just produced for each batch item.\n",
    "#         We add α·LM_log_probs on the shared slice.\n",
    "#         \"\"\"\n",
    "#         if self.alpha == 0:\n",
    "#             return scores  # vanilla path – identical to Whisper\n",
    "\n",
    "#         # build GPT‑2 context (strip specials) for each item in batch\n",
    "#         new_scores = scores.clone()\n",
    "#         for b, ids in enumerate(input_ids):\n",
    "#             ctx = [t for t in ids.tolist() if t not in self.specials] or ids.tolist()\n",
    "#             g_logits = self.gpt2(torch.tensor([ctx], device=scores.device)\n",
    "#                                  ).logits[:, -1, :]  # (1, Vg)\n",
    "\n",
    "#             w_lp = F.log_softmax(scores[b], -1)\n",
    "#             g_lp = F.log_softmax(g_logits[0], -1)\n",
    "\n",
    "#             if self.variance:\n",
    "#                 w_lp = w_lp / w_lp.std(unbiased=False)\n",
    "#                 g_lp = g_lp / g_lp.std(unbiased=False).clamp_min(1e-6)\n",
    "\n",
    "#             w_lp[:self.shared_vocab] += self.alpha * g_lp\n",
    "#             new_scores[b] = w_lp\n",
    "\n",
    "#         return new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5dfe1b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob, numpy as np, soundfile as sf, scipy.signal as sps\n",
    "# from IPython.display import Audio, display\n",
    "\n",
    "# SR_TARGET = 16_000\n",
    "# LOWPASS_HZ = 3_000\n",
    "# WAV_PATH = glob.glob(\"../data/output/*.wav\")[0]   # adjust if needed\n",
    "\n",
    "# def butter_lowpass(wav, cutoff_hz, sr=SR_TARGET):\n",
    "#     nyq = sr / 2\n",
    "#     b, a = sps.butter(6, cutoff_hz / nyq, btype=\"low\")\n",
    "#     return sps.filtfilt(b, a, wav).astype(np.float32)\n",
    "\n",
    "# wav, sr = sf.read(WAV_PATH, dtype=\"float32\")\n",
    "# wav = wav.mean(axis=1) if wav.ndim == 2 else wav\n",
    "# if sr != SR_TARGET:\n",
    "#     wav = sps.resample_poly(wav, SR_TARGET, sr)\n",
    "\n",
    "# lp = butter_lowpass(wav, LOWPASS_HZ)\n",
    "# sf.write(\"lowpass_only.wav\", lp, SR_TARGET)\n",
    "\n",
    "# print(\"Original :\", WAV_PATH)\n",
    "# print(\"Low-pass :\", \"lowpass_only.wav\")\n",
    "\n",
    "# display(Audio(wav, rate=SR_TARGET, autoplay=False))\n",
    "# display(Audio(lp,  rate=SR_TARGET, autoplay=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
