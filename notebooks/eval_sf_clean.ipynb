{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7588133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SHALLOW_FUSION_EVAL', 'SF_EVAL', '.models']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, librosa, numpy as np, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "print(str(CACHE_DIR).split('/')[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3be2e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    GPT2LMHeadModel, AutoTokenizer\n",
    ")\n",
    "\n",
    "SR = 16_000\n",
    "BATCH_SIZE = 5\n",
    "WHISPER_ID = \"openai/whisper-tiny.en\"\n",
    "GPT2_ID = \"cwestnedge/gpt2-medium-pubmed\"\n",
    "\n",
    "MANIFEST = \"../data/output/manifest.jsonl\"\n",
    "AUDIO_DIR = \"../data/output\"  \n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d8f72bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder start token ID: 50257\n",
      "Decoder start token: <|startoftranscript|>\n",
      "\n",
      "Complete prefix: [50257, 50362]\n",
      "Decoded: '<|startoftranscript|><|notimestamps|>'\n",
      "Total prefix length: 2\n"
     ]
    }
   ],
   "source": [
    "# fast tokenizers will show token mismatch between models and will be auto loaded when we run on colab A100 set flag to false to avoid annoyingness\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "EOS_ID = gpt2_tok.eos_token_id # 50256 (unchanged)\n",
    "\n",
    "print(\"Decoder start token ID:\", whisper.generation_config.decoder_start_token_id)\n",
    "print(\"Decoder start token:\", processor.decode([whisper.generation_config.decoder_start_token_id]))\n",
    "\n",
    "PREFIX_TOK_IDS = [whisper.generation_config.decoder_start_token_id]\n",
    "for position, token_id in whisper.generation_config.forced_decoder_ids:\n",
    "    PREFIX_TOK_IDS.append(token_id)\n",
    "\n",
    "print(f\"\\nComplete prefix: {PREFIX_TOK_IDS}\")\n",
    "print(f\"Decoded: '{processor.decode(PREFIX_TOK_IDS)}'\")\n",
    "print(f\"Total prefix length: {len(PREFIX_TOK_IDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8c5dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gpt2_tok.get_vocab())):\n",
    "    a = processor.tokenizer.decode([i])\n",
    "    b = gpt2_tok.decode([i])\n",
    "    if a != b:\n",
    "        print(f\"Token mismatch at index {i}\\nwhisper token: [{a}]\\n   gpt2 token: [{b}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be085a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 100/100 [00:06<00:00, 16.49 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 18.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(manifest_path: str, batch_size: int, num_proc: int = 4) -> Dataset:\n",
    "    with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "\n",
    "    def add_audio(batch):\n",
    "        batch[\"audio\"] = [\n",
    "            librosa.load(f\"{AUDIO_DIR}/{fname}\", sr=SR, mono=True)[0].astype(np.float32)\n",
    "            for fname in batch[\"file\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return ds.map(add_audio, batched=True, batch_size=batch_size, num_proc=num_proc)\n",
    "\n",
    "def encode_audio(batch):\n",
    "    # batch[\"audio\"] is List[np.ndarray], each at its natural length\n",
    "    feats = processor.feature_extractor(\n",
    "        batch[\"audio\"], # for whatever reason processor doesnt support PT tensors so numpy array or list for now.\n",
    "        sampling_rate=SR,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=processor.feature_extractor.n_samples,  # n_samples = chunk_length * sampling_rate\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    #  input_features : Tensor (B, T_max, 80)\n",
    "    #  attention_mask : Tensor (B, T_max)\n",
    "    batch[\"input_features\"] = feats.input_features\n",
    "    batch[\"attention_mask\"] = feats.attention_mask\n",
    "    return batch\n",
    "\n",
    "ds = build_dataset(MANIFEST, batch_size=BATCH_SIZE).select(range(10))\n",
    "\n",
    "# choosing NOT to overwrite ds with removed fields so we can eval on text field later,\n",
    "# could also create a collator and pass fields we care about through, but seems like \n",
    "# too much extra code tbh, indices will still match if we dont shuffle\n",
    "ds_processed = ds.map(\n",
    "    encode_audio, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    batched=True,\n",
    "    remove_columns=['uuid', 'file', 'chunk_id', 'orig_id', 'label', 'text', 'audio']\n",
    "    )\n",
    "\n",
    "ds_processed.set_format(type=\"torch\", columns=[\"input_features\",\"attention_mask\"])\n",
    "loader = DataLoader(ds_processed, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c0fc3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "\n",
    "## this works\n",
    "# class ShallowFusion(LogitsProcessor):\n",
    "#     def __init__(self, lm, pad_id, eos_id, alpha=0.25, warmup=3):\n",
    "#         super().__init__()\n",
    "#         self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "#         self.pad_id = pad_id  # 50257 \n",
    "#         self.eos_id = eos_id  # 50256\n",
    "#         self.alpha = alpha \n",
    "#         self.warmup = warmup\n",
    "#         self.step = 0 \n",
    "    \n",
    "#     def reset(self):\n",
    "#         self.step = 0\n",
    "\n",
    "#     @torch.inference_mode()\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         if self.step < self.warmup: \n",
    "#             self.step += 1 \n",
    "#             return scores\n",
    "#         self.step += 1 \n",
    "\n",
    "#         # Find where valid tokens start (first token < eos_id)\n",
    "#         valid_mask = input_ids < self.eos_id\n",
    "        \n",
    "#         # Find the first valid token position for each sequence\n",
    "#         # This skips special tokens at the beginning\n",
    "#         first_valid = valid_mask.long().argmax(dim=1, keepdim=True)\n",
    "        \n",
    "#         # Create indices for gathering\n",
    "#         batch_size, seq_len = input_ids.shape\n",
    "#         batch_indices = torch.arange(batch_size, device=input_ids.device).unsqueeze(1)\n",
    "        \n",
    "#         # Gather only the valid portion of each sequence\n",
    "#         gather_indices = first_valid + torch.arange(seq_len - first_valid.max(), \n",
    "#                                                     device=input_ids.device).unsqueeze(0)\n",
    "#         gather_indices = gather_indices.clamp(max=seq_len-1)\n",
    "        \n",
    "#         # Extract subsequences starting from first valid token\n",
    "#         clean_ids = input_ids.gather(1, gather_indices)\n",
    "#         clean_mask = valid_mask.gather(1, gather_indices)\n",
    "        \n",
    "#         # Apply your original logic on clean sequences\n",
    "#         clean_ids[~clean_mask] = self.pad_id\n",
    "#         attention_mask = clean_mask.long()\n",
    "        \n",
    "#         lm_logits = self.lm(\n",
    "#             input_ids=clean_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         ).logits[:, -1, :]\n",
    "\n",
    "#         lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "        \n",
    "#         fused = scores.clone()\n",
    "#         fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        \n",
    "#         return fused\n",
    "\n",
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, eos_id, prefix_tokens, alpha=0.25, warmup=3):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "        self.eos_id = eos_id\n",
    "        self.prefix_tokens = prefix_tokens  # <|startoftranscript|><|notimestamps|>\n",
    "        self.alpha = alpha \n",
    "        self.warmup = warmup\n",
    "        self.step = 0 \n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        if self.step < self.warmup: \n",
    "            self.step += 1 \n",
    "            return w_lp\n",
    "\n",
    "        prefix_len = len(self.prefix_tokens)\n",
    "        prefix_ids = torch.tensor(self.prefix_tokens, device=input_ids.device)\n",
    "        \n",
    "        # make sure that the actual prefix matches expectation \n",
    "        if self.step == self.warmup:  # (only need to check once)\n",
    "            prefix_matches = (input_ids[0, :prefix_len] == prefix_ids).all()\n",
    "            if not prefix_matches:\n",
    "                print(f\"WARNING: prefix mismatch:\\nexpected {self.prefix_tokens}, got {input_ids[0, :prefix_len].tolist()}\")\n",
    "        \n",
    "        # make sure we dont have special tokens emitted AFTER decoder prefix\n",
    "        has_special_after_prefix = (input_ids[:, prefix_len:] > self.eos_id).any()\n",
    "        if has_special_after_prefix:\n",
    "            print(f\"WARNING: special tokens found after prefix at step {self.step}\")\n",
    "        \n",
    "        clean_ids = input_ids[:, prefix_len:]\n",
    "        lm_logits = self.lm(input_ids=clean_ids).logits[:, -1, :]\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "        \n",
    "        # FUSION STEP \n",
    "        # P_fused(y|x) = log P_ASR(y|x) + alpha × log P_LM(y)\n",
    "        fused = w_lp.clone()\n",
    "        fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        \n",
    "        # optional normalization step\n",
    "        # fused -= torch.logsumexp(fused, dim=-1, keepdim=True)\n",
    "        self.step += 1\n",
    "        return fused\n",
    "\n",
    "  \n",
    "fusion_proc = ShallowFusion(\n",
    "    lm=gpt2,\n",
    "    eos_id=EOS_ID,\n",
    "    prefix_tokens=PREFIX_TOK_IDS,\n",
    "    alpha=0.25,\n",
    "    warmup=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "61d2dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 2/2 [00:17<00:00,  8.88s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "from tqdm import tqdm \n",
    "\n",
    "fused = []\n",
    "base = []\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        fused_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            length_penalty=1.0,\n",
    "            return_timestamps=False,\n",
    "            return_token_timestamps=False\n",
    "            \n",
    "        )\n",
    "    decoded = processor.batch_decode(fused_ids, skip_special_tokens=True)\n",
    "    fused.extend(decoded)\n",
    "    fusion_proc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3f466239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 2/2 [00:08<00:00,  4.14s/it]\n"
     ]
    }
   ],
   "source": [
    "class NoOpLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    A simple logits processor that performs no modifications to the logits.\n",
    "    \"\"\"\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Return the scores unchanged\n",
    "\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        return scores\n",
    "\n",
    "\n",
    "noop_processor = NoOpLogitsProcessor()\n",
    "vanilla = []\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        vanilla_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([noop_processor]),\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "    decoded = processor.batch_decode(vanilla_ids, skip_special_tokens=True)\n",
    "    vanilla.extend(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f06c8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"vanilla\":[i.strip() for i in vanilla], \n",
    "        \"fused\":[i.strip() for i in fused], \n",
    "        \"reference\":ds['text'],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3afaf542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base  WER (punct-insensitive): 0.09818350930115635\n",
      "Fused WER (punct-insensitive): 0.08210960281548517\n"
     ]
    }
   ],
   "source": [
    "from jiwer import (\n",
    "    Compose,\n",
    "    ToLowerCase,\n",
    "    RemovePunctuation,\n",
    "    RemoveMultipleSpaces,\n",
    "    Strip,\n",
    "    ReduceToListOfListOfWords,\n",
    "    wer\n",
    ")\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "# helper to handle both str and list[str]\n",
    "def _map(func, x):\n",
    "    return [func(t) for t in x] if isinstance(x, list) else func(x)\n",
    "\n",
    "def remove_diacritics(x):\n",
    "    return _map(unidecode, x)\n",
    "\n",
    "def split_hyphens_and_slashes(x):\n",
    "    # replace any dash or slash with a space so we never glue words together\n",
    "    return _map(lambda t: re.sub(r\"[-–—/]\", \" \", t), x)\n",
    "\n",
    "def normalize_nums(x):\n",
    "    # unify 12–16 → 12-16\n",
    "    return _map(lambda t: re.sub(r\"(\\d)[-–—-](\\d)\", r\"\\1-\\2\", t), x)\n",
    "\n",
    "transform = Compose([\n",
    "    ToLowerCase(),\n",
    "    remove_diacritics,\n",
    "    split_hyphens_and_slashes,    # ← split first\n",
    "    normalize_nums,\n",
    "    RemovePunctuation(),           # now drop commas, periods, etc.\n",
    "    RemoveMultipleSpaces(),\n",
    "    Strip(),\n",
    "    ReduceToListOfListOfWords(),   # produce [[“word”,…],…]\n",
    "])\n",
    "\n",
    "def compute_wer(ref, hyp):\n",
    "    return wer(\n",
    "        ref, hyp,\n",
    "        reference_transform=transform,\n",
    "        hypothesis_transform=transform,\n",
    "    )\n",
    "\n",
    "# rename & score\n",
    "results_df = results_df.rename(columns={\"gt\": \"reference\"})\n",
    "results_df[\"wer_base\"]  = results_df.apply(\n",
    "    lambda r: compute_wer(r[\"reference\"], r[\"vanilla\"]), axis=1\n",
    ")\n",
    "results_df[\"wer_fused\"] = results_df.apply(\n",
    "    lambda r: compute_wer(r[\"reference\"], r[\"fused\"]), axis=1\n",
    ")\n",
    "\n",
    "print(\"Base  WER (punct-insensitive):\", results_df[\"wer_base\"].mean())\n",
    "print(\"Fused WER (punct-insensitive):\", results_df[\"wer_fused\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c1ac6469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vanilla</th>\n",
       "      <th>fused</th>\n",
       "      <th>reference</th>\n",
       "      <th>wer_base</th>\n",
       "      <th>wer_fused</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>With diverticulitis, no pneumoparatonea is ide...</td>\n",
       "      <td>With diverticulitis, no pneumoparatonea is ide...</td>\n",
       "      <td>with diverticulitis. No pneumoperitoneum is id...</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>administered. The lung bases are clear. The li...</td>\n",
       "      <td>administered. The lung bases are clear. The li...</td>\n",
       "      <td>administered. The lung bases are clear. The li...</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.078431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Preoperative diagnosis, refractory dyspepsia, ...</td>\n",
       "      <td>Preoperative diagnosis, refractory dyspepsyia,...</td>\n",
       "      <td>Preoperative Diagnosis: Refractory dyspepsia. ...</td>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.057692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and bird-type symptoms, including complication...</td>\n",
       "      <td>and bird-type symptoms, including complication...</td>\n",
       "      <td>and GERD type symptoms including complications...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.060000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>IV-Demorol and Versid-Forsidation, when adequa...</td>\n",
       "      <td>IV-demeral and v. sedation, when adequate leve...</td>\n",
       "      <td>IV Demerol and Versed for sedation. When adequ...</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gidney, adrenal, abdomen, and pelvis, CT scan,...</td>\n",
       "      <td>Gidney, adrenal, abdomen, and pelvis, CT scan,...</td>\n",
       "      <td>kidney, adrenal, abdomen and pelvis, ct scan, ...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exam, CT scan of the abdomen and pelvis withou...</td>\n",
       "      <td>Exam, CT scan of the abdomen and pelvis withou...</td>\n",
       "      <td>Exam: CT scan of the abdomen and pelvis withou...</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the inferior pole of the right kidney. There i...</td>\n",
       "      <td>the inferior pole of the right kidney. There i...</td>\n",
       "      <td>the inferior pole of the right kidney. There i...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>stomach, it was insiphlated and the scope was ...</td>\n",
       "      <td>stomach, it was insipulated and the scope was ...</td>\n",
       "      <td>stomach. It was insufflated and the scope was ...</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Back into the Antrum, a retroflexion was attem...</td>\n",
       "      <td>Back into the Antrum, a retroflexion was attem...</td>\n",
       "      <td>back into the antrum. A retroflexion was attem...</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             vanilla  \\\n",
       "5  With diverticulitis, no pneumoparatonea is ide...   \n",
       "1  administered. The lung bases are clear. The li...   \n",
       "6  Preoperative diagnosis, refractory dyspepsia, ...   \n",
       "4  and bird-type symptoms, including complication...   \n",
       "8  IV-Demorol and Versid-Forsidation, when adequa...   \n",
       "0  Gidney, adrenal, abdomen, and pelvis, CT scan,...   \n",
       "2  Exam, CT scan of the abdomen and pelvis withou...   \n",
       "3  the inferior pole of the right kidney. There i...   \n",
       "7  stomach, it was insiphlated and the scope was ...   \n",
       "9  Back into the Antrum, a retroflexion was attem...   \n",
       "\n",
       "                                               fused  \\\n",
       "5  With diverticulitis, no pneumoparatonea is ide...   \n",
       "1  administered. The lung bases are clear. The li...   \n",
       "6  Preoperative diagnosis, refractory dyspepsyia,...   \n",
       "4  and bird-type symptoms, including complication...   \n",
       "8  IV-demeral and v. sedation, when adequate leve...   \n",
       "0  Gidney, adrenal, abdomen, and pelvis, CT scan,...   \n",
       "2  Exam, CT scan of the abdomen and pelvis withou...   \n",
       "3  the inferior pole of the right kidney. There i...   \n",
       "7  stomach, it was insipulated and the scope was ...   \n",
       "9  Back into the Antrum, a retroflexion was attem...   \n",
       "\n",
       "                                           reference  wer_base  wer_fused  \\\n",
       "5  with diverticulitis. No pneumoperitoneum is id...  0.160000   0.060000   \n",
       "1  administered. The lung bases are clear. The li...  0.117647   0.039216   \n",
       "6  Preoperative Diagnosis: Refractory dyspepsia. ...  0.173077   0.230769   \n",
       "4  and GERD type symptoms including complications...  0.080000   0.060000   \n",
       "8  IV Demerol and Versed for sedation. When adequ...  0.120000   0.100000   \n",
       "0  kidney, adrenal, abdomen and pelvis, ct scan, ...  0.111111   0.111111   \n",
       "2  Exam: CT scan of the abdomen and pelvis withou...  0.040000   0.040000   \n",
       "3  the inferior pole of the right kidney. There i...  0.080000   0.080000   \n",
       "7  stomach. It was insufflated and the scope was ...  0.080000   0.080000   \n",
       "9  back into the antrum. A retroflexion was attem...  0.020000   0.020000   \n",
       "\n",
       "       diff  \n",
       "5  0.100000  \n",
       "1  0.078431  \n",
       "6  0.057692  \n",
       "4  0.020000  \n",
       "8  0.020000  \n",
       "0  0.000000  \n",
       "2  0.000000  \n",
       "3  0.000000  \n",
       "7  0.000000  \n",
       "9  0.000000  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['diff'] = abs(results_df.wer_base - results_df.wer_fused)\n",
    "top_diffs = results_df.sort_values(by='diff', ascending=False).head(10)\n",
    "top_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "87f577d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GT:    with diverticulitis. No pneumoperitoneum is identified. There is no ascites or focal fluid collection. The aorta is normal in contour and caliber. There is no adenopathy. Degenerative changes are present in the lumbar spine. Impression: Findings consistent with diverticulitis. Please see report above.gastroenterology, extrahepatic ductal dilatation, gallbladder, glands, pancreas, spleen,\n",
      "Base:  With diverticulitis, no pneumoparatonea is identified. There is no ascites or focal fluid collection. The aorta is normal in contour and caliber. There is no adnopathy. Degenerative changes are present in the lumbar spine. Impression. Findings consistent with diverticulitis. Please see report above. Gastroenterology. Extraheptotic ductal dilatation. Galbladder. Glans. Pancreus. Spleen.\n",
      "Fused: With diverticulitis, no pneumoparatonea is identified. There is no ascites or focal fluid collection. The aorta is normal in contour and caliber. There is no adenopathy. Degenerative changes are present in the lumbar spine. Impression, findings consistent with diverticulitis. Please see report above. Gastroenterology, extrahepatic ductal dilatation, gallbladder, glands, pancreas, spleen,\n",
      "base err - fused err: 0.1\n",
      "\n",
      "GT:    administered. The lung bases are clear. The liver is enlarged and decreased in attenuation. There are no focal liver masses. There is no intra or extrahepatic ductal dilatation. The gallbladder is slightly distended. The adrenal glands, pancreas, spleen, and left kidney are normal. A 12-mm simple cyst is present in\n",
      "Base:  administered. The lung bases are clear. The liver is enlarged and decreased in attenuation. There are no focal liver masses. There's no intra or extra-heptactic ductile dilatation. The gallbladder is slightly distended. The adrenal glands, pancreas, spleen, and left kidney are normal. A 12-millimeter simple cyst is present in.\n",
      "Fused: administered. The lung bases are clear. The liver is enlarged and decreased in attenuation. There are no focal liver masses. There is no intrap or extrahepatic ductal dilatation. The gallbladder is slightly distended. The adrenal glands, pancreas, spleen, and left kidney are normal. A 12-millimeter simple cyst is present in.\n",
      "base err - fused err: 0.0784313725490196\n",
      "\n",
      "GT:    Preoperative Diagnosis: Refractory dyspepsia. Postoperative Diagnosis: 1. Hiatal hernia. 2. Reflux esophagitis. Procedure Performed: Esophagogastroduodenoscopy with pseudo and esophageal biopsy. Anesthesia: Conscious sedation with Demerol and Versed. Specimen: Esophageal biopsy. Complications: None. History: The patient is a 52-year-old female morbidly obese black female who has a long history of reflux\n",
      "Base:  Preoperative diagnosis, refractory dyspepsia, postoperative diagnosis, one, high-atal hernia, two, reflux esophageitis, procedure performed, esophagogastrodoad anoscopy with pseudo and esophageal biopsy, anesthesia, conscious sedation with dimmeral and verced, specimen esophageal biopsy, complications, none. History, the patient is a 52 year old female, morbidly obese black female, who has a long history of reflux.\n",
      "Fused: Preoperative diagnosis, refractory dyspepsyia, postoperative diagnosis, one, high-at-el hernia, two, reflux esophagitis, procedure performed, esophagogastro-duodenoscopy with pseudo-endosophageal biopsy, anesthesia, conscious sedation with dimmeral and verced, specimen, esophageal biopsy, complications, none, history, the patient is a 52-year-old female, morbidly obese, black female, who has a long history of reflux.\n",
      "base err - fused err: -0.05769230769230771\n",
      "\n",
      "GT:    and GERD type symptoms including complications such as hoarseness and chronic cough. She has been on multiple medical regimens and continues with dyspeptic symptoms. Procedure: After proper informed consent was obtained, the patient was brought to the endoscopy suite. She was placed in the left lateral position and was given\n",
      "Base:  and bird-type symptoms, including complications such as hoarseness and chronic cough. She has been on multiple medical regimens and continues with despepptic symptoms. For seizure, after proper informed consent was obtained, the patient was brought to the endoscopy suite. She was placed in the left lateral position and was given.\n",
      "Fused: and bird-type symptoms, including complications such as hoarseness and chronic cough. She has been on multiple medical regimens and continues with dyspeptic symptoms. For seizure, after proper informed consent was obtained, the patient was brought to the endoscopy suite. She was placed in the left lateral position and was given\n",
      "base err - fused err: 0.020000000000000004\n",
      "\n",
      "GT:    IV Demerol and Versed for sedation. When adequate level of sedation achieved, the gastroscope was inserted into the hypopharynx and the esophagus was easily intubated. At the GE junction, a hiatal hernia was present. There were mild inflammatory changes consistent with reflux esophagitis. The scope was then passed into the\n",
      "Base:  IV-Demorol and Versid-Forsidation, when adequate level of sedation achieved, the gastriscope was inserted into the hypopharynx. And the esophagus was easily intubated. At the GE junction, a hyatal hernia was present. There were mild inflammatory changes consistent with reflux esophagitis. The scope was then passed into the\n",
      "Fused: IV-demeral and v. sedation, when adequate level of sedation achieved, the gastroscope was inserted into the hypopharynx. And the esophagus was easily intubated. At the GE junction, a hyalurnia was present. There were mild inflammatory changes consistent with reflux esophagitis. The scope was then passed into the\n",
      "base err - fused err: 0.01999999999999999\n",
      "\n",
      "GT:    kidney, adrenal, abdomen and pelvis, ct scan, intravenous, abdomen,\n",
      "Base:  Gidney, adrenal, abdomen, and pelvis, CT scan, intravenous, abdomen.\n",
      "Fused: Gidney, adrenal, abdomen, and pelvis, CT scan, intravenous, abdomen.\n",
      "base err - fused err: 0.0\n",
      "\n",
      "GT:    Exam: CT scan of the abdomen and pelvis without and with intravenous contrast. Clinical Indication: Left lower quadrant abdominal pain. Comparison: None. Findings: CT scan of the abdomen and pelvis was performed without and with intravenous contrast. Total of 100 mL of Isovue was administered intravenously. Oral contrast was also\n",
      "Base:  Exam, CT scan of the abdomen and pelvis without and with intravenous contrast. Clinical indication, left lower quadrant abdominal pain. Comparison, none, findings. CT scan of the abdomen and pelvis was performed without and with intravenous contrast. Total of 100 milliliters of ICIVU was administered intravenously. Oral contrast was also.\n",
      "Fused: Exam, CT scan of the abdomen and pelvis without and with intravenous contrast. Clinical indication, left lower quadrant abdominal pain. Comparison, none, findings. CT scan of the abdomen and pelvis was performed without and with intravenous contrast. Total of 100 milliliters of ICU was administered intravenously. Oral contrast was also\n",
      "base err - fused err: 0.0\n",
      "\n",
      "GT:    the inferior pole of the right kidney. There is no hydronephrosis or hydroureter. The appendix is normal. There are multiple diverticula in the rectosigmoid. There is evidence of focal wall thickening in the sigmoid colon (image #69) with adjacent fat stranding in association with a diverticulum. These findings are consistent\n",
      "Base:  the inferior pole of the right kidney. There is no hydronaphrosis or hydrouretor. The appendix is normal. There are multiple diverticula in the rectosigmoid. There is evidence of focal wall thickening in the sigmoid colon, image number 69, with adjacent fat stranding and association with a diverticulum. These findings are consistent.\n",
      "Fused: the inferior pole of the right kidney. There is no hydronaphrosis or hydrouretor. The appendix is normal. There are multiple diverticula in the rectosigmoid. There is evidence of focal wall thickening in the sigmoid colon, image number 69, with adjacent fat stranding and association with a diverticulum. These findings are consistent.\n",
      "base err - fused err: 0.0\n",
      "\n",
      "GT:    stomach. It was insufflated and the scope was coursed along the greater curvature to the antrum. The pylorus was patent. There was evidence of bile reflux in the antrum. The duodenal bulb and sweep were examined and were without evidence of mass, ulceration, or inflammation. The scope was then brought\n",
      "Base:  stomach, it was insiphlated and the scope was coarse to long the greater curvature to the antrum. The pylorus was patent. There was evidence of bile reflux in the antrum. The duodenal bulb and sweep were examined and were without evidence of mass, ulceration, or inflammation. The scope was then brought.\n",
      "Fused: stomach, it was insipulated and the scope was coarse to long the greater curvature to the antrum. The pylorus was patent. There was evidence of bile reflux in the antrum. The duodenal bulb and sweep were examined and were without evidence of mass, ulceration, or inflammation. The scope was then brought.\n",
      "base err - fused err: 0.0\n",
      "\n",
      "GT:    back into the antrum. A retroflexion was attempted multiple times, however, the patient was having difficulty holding the air and adequate retroflexion view was not visualized. The gastroscope was then slowly withdrawn. There were no other abnormalities noted in the fundus or body. Once again at the GE junction, esophageal\n",
      "Base:  Back into the Antrum, a retroflexion was attempted multiple times. However, the patient was having difficulty holding the air and adequate retroflexion view was not visualized. The gastroscope was then slowly withdrawn. There were no other abnormalities noted in the fundus or body. Once again, at the GE junction, Isophageal.\n",
      "Fused: Back into the Antrum, a retroflexion was attempted multiple times. However, the patient was having difficulty holding the air and adequate retroflexion, view, was not visualized. The gastroscope was then slowly withdrawn. There were no other abnormalities noted in the fundus or body. Once again, at the GE junction, isophageal.\n",
      "base err - fused err: 0.0\n"
     ]
    }
   ],
   "source": [
    "print_str = '''\n",
    "GT:    {}\n",
    "Base:  {}\n",
    "Fused: {}\n",
    "base err - fused err: {}'''\n",
    "\n",
    "for idx, row in top_diffs.iterrows():\n",
    "    row_str = print_str.format(\n",
    "        row['reference'], \n",
    "        row['vanilla'], \n",
    "        row['fused'],\n",
    "        row['wer_base'] - row['wer_fused']\n",
    "    )\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32e310",
   "metadata": {},
   "source": [
    "# BONEYARD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332dfbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, librosa, numpy as np, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "print(str(CACHE_DIR).split('/')[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ff81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.added_tokens_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3161254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    GPT2LMHeadModel, AutoTokenizer\n",
    ")\n",
    "\n",
    "SR = 16_000\n",
    "BATCH_SIZE = 5\n",
    "WHISPER_ID = \"openai/whisper-tiny.en\"\n",
    "GPT2_ID = \"cwestnedge/gpt2-small-pubmed\"\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\").resolve()\n",
    "MANIFEST = \"../data/output/manifest.jsonl\"\n",
    "AUDIO_DIR = \"../data/output\"  \n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR, use_fast=True)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR, use_fast=True)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "# gpt2_tok = processor.tokenizer \n",
    "# gpt2.resize_token_embeddings(len(gpt2_tok))\n",
    "\n",
    "if gpt2_tok.pad_token is None:\n",
    "    gpt2_tok.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "    gpt2.resize_token_embeddings(len(gpt2_tok))\n",
    "\n",
    "PAD_ID = gpt2_tok.pad_token_id      # e.g. 50257\n",
    "VS = gpt2_tok.vocab_size     \n",
    "\n",
    "def build_dataset(manifest_path: str, batch_size: int, num_proc: int = 4) -> Dataset:\n",
    "    with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "\n",
    "    def add_audio(batch):\n",
    "        batch[\"audio\"] = [\n",
    "            librosa.load(f\"{AUDIO_DIR}/{fname}\", sr=SR, mono=True)[0].astype(np.float32)\n",
    "            for fname in batch[\"file\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return ds.map(add_audio, batched=True, batch_size=batch_size, num_proc=num_proc)\n",
    "\n",
    "def encode_audio(batch):\n",
    "    # batch[\"audio\"] is List[np.ndarray], each at its natural length\n",
    "    feats = processor.feature_extractor(\n",
    "        batch[\"audio\"], # for whatever reason processor doesnt support PT tensors so numpy array or list for now.\n",
    "        sampling_rate=SR,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=processor.feature_extractor.n_samples,  # n_samples = chunk_length * sampling_rate\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    #  input_features : Tensor (B, T_max, 80)\n",
    "    #  attention_mask : Tensor (B, T_max)\n",
    "    batch[\"input_features\"] = feats.input_features\n",
    "    batch[\"attention_mask\"] = feats.attention_mask\n",
    "    return batch\n",
    "\n",
    "ds = build_dataset(MANIFEST, batch_size=BATCH_SIZE).select(range(20))\n",
    "\n",
    "# choosing NOT to overwrite ds with removed fields so we can eval on text field later,\n",
    "# could also create a collator and pass fields we care about through, but seems like \n",
    "# too much extra code tbh, indices will still match if we dont shuffle\n",
    "ds_processed = ds.map(\n",
    "    encode_audio, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    batched=True,\n",
    "    remove_columns=['uuid', 'file', 'chunk_id', 'orig_id', 'label', 'text', 'audio']\n",
    "    )\n",
    "\n",
    "ds_processed.set_format(type=\"torch\", columns=[\"input_features\",\"attention_mask\"])\n",
    "loader = DataLoader(ds_processed, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "for i in range(len(gpt2_tok.get_vocab())):\n",
    "    a = processor.tokenizer.decode([i])\n",
    "    b = gpt2_tok.decode([i])\n",
    "    if a != b:\n",
    "        print(f\"Token mismatch at index {i}\\nwhisper token: {a}\\n   gpt2 token: {b} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0635234f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder start token ID: 50257\n",
      "BOS token ID: 50256\n",
      "Suppress tokens: [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 357, 366, 438, 532, 685, 705, 796, 930, 1058, 1220, 1267, 1279, 1303, 1343, 1377, 1391, 1635, 1782, 1875, 2162, 2361, 2488, 3467, 4008, 4211, 4600, 4808, 5299, 5855, 6329, 7203, 9609, 9959, 10563, 10786, 11420, 11709, 11907, 13163, 13697, 13700, 14808, 15306, 16410, 16791, 17992, 19203, 19510, 20724, 22305, 22935, 27007, 30109, 30420, 33409, 34949, 40283, 40493, 40549, 47282, 49146, 50257, 50357, 50358, 50359, 50360, 50361]\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoder start token ID:\", whisper.config.decoder_start_token_id)\n",
    "print(\"BOS token ID:\", processor.tokenizer.bos_token_id)\n",
    "print(\"Suppress tokens:\", whisper.config.suppress_tokens if hasattr(whisper.config, 'suppress_tokens') else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09978102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_scores']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# from transformers import LogitsProcessor, LogitsProcessorList\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "class HelloWorldLP(LogitsProcessor):\n",
    "    def __init__(self, alpha=0.0, warmup=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.step  = 0\n",
    "        self.warmup = warmup\n",
    "\n",
    "    def reset(self): self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        if self.step < self.warmup: \n",
    "            self.step+=1 \n",
    "            return scores\n",
    "        self.step+=1\n",
    "        s = input_ids[:, 2:]\n",
    "\n",
    "        print(input_ids)\n",
    "        print()\n",
    "        print(s)\n",
    "        print('-'*50)\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, eos_id, alpha=0.25, warmup=3):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "        self.eos_id = eos_id # should be 50256\n",
    "        self.alpha = alpha \n",
    "        self.warmup = warmup\n",
    "        self.step = 0 \n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # w_lp = torch.log_softmax(scores, dim=-1)\n",
    "\n",
    "        if self.step < self.warmup: \n",
    "            self.step+=1 \n",
    "            return scores\n",
    "        self.step+=1 \n",
    "\n",
    "        whisper_start_ids = [50257, 50362]\n",
    "        lm_input_ids = input_ids[:, len(whisper_start_ids):]\n",
    "\n",
    "        lm_logits = self.lm(\n",
    "            input_ids=lm_input_ids,\n",
    "        ).logits[:,-1,:] # just want next token logits\n",
    "\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "        \n",
    "        fused = scores.clone()\n",
    "        fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        # optional normalization step\n",
    "        # fused -= torch.logsumexp(fused, dim=-1, keepdim=True)\n",
    "        return fused\n",
    "    \n",
    "hw_proc = HelloWorldLP(warmup=2)\n",
    "\n",
    "fusion_proc = ShallowFusion(\n",
    "    lm=gpt2,\n",
    "    eos_id=50256,\n",
    "    alpha=0.15,\n",
    "    warmup=2\n",
    ")\n",
    "\n",
    "# batch = next(iter(loader))\n",
    "# feats = batch['input_features'].to(DEVICE)\n",
    "# masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    out1 = whisper.generate(\n",
    "        input_features=feats,\n",
    "        attention_mask=masks,\n",
    "        logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        return_timestamps=False,\n",
    "        num_beams=2,\n",
    "        max_new_tokens= 5\n",
    "    )\n",
    "\n",
    "    # out2 = whisper.generate(\n",
    "    #     input_features=feats,\n",
    "    #     attention_mask=masks,\n",
    "    #     return_dict_in_generate=True,\n",
    "    #     output_scores=True,\n",
    "    # )\n",
    "    \n",
    "# print((out1.scores[-1] != out2.scores[-1]).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66234358",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_scores']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do sample: False\n",
      "Temperature: 1.0\n",
      "Num beams: 1\n"
     ]
    }
   ],
   "source": [
    "# Check what generation strategy was used\n",
    "print(f\"Do sample: {whisper.config.do_sample if hasattr(whisper.config, 'do_sample') else 'N/A'}\")\n",
    "print(f\"Temperature: {whisper.config.temperature if hasattr(whisper.config, 'temperature') else 'N/A'}\")\n",
    "print(f\"Num beams: {whisper.config.num_beams if hasattr(whisper.config, 'num_beams') else 'N/A'}\")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# Force greedy decoding (argmax) to match your analysis\n",
    "out1_greedy = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=False,  # Force greedy/argmax\n",
    "    num_beams=1,      # No beam search\n",
    "    return_timestamps=False\n",
    "    # temperature=1.0,  # No temperature scaling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a3c9bc45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|>', '<|notimestamps|>']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode([50257, 50362])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7b59052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50257, 50362,   402,   312,  1681,    11, 26999,   282,    11, 32956,\n",
       "            11,   290, 16176,  4703,    11, 16356,  9367,    11, 45840,   516,\n",
       "            11, 32956,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256],\n",
       "        [50257, 50362, 17169,    13,   383, 12317, 12536,   389,  1598,    13,\n",
       "           383, 14383,   318, 37287,   290, 11832,   287, 31919,  2288,    13,\n",
       "          1318,   389,   645, 25397, 14383, 14568,    13,  1318,   338,   645,\n",
       "           493,   430,   393,  3131,    12,   258,   457, 12009, 28494,   576,\n",
       "         11844,   265,   341,    13,   383,  7976,  2436, 26676,   318,  4622,\n",
       "          1233,  1631,    13,   383, 26999,   282, 42093,    11, 22154,   260,\n",
       "           292,    11,   599, 20042,    11,   290,  1364, 21919,   389,  3487,\n",
       "            13,   317,  1105,    12, 17805, 16912,  2829,  3075,   301,   318,\n",
       "          1944,   287,    13, 50256],\n",
       "        [50257, 50362, 35909,    11, 16356,  9367,   286,   262, 32956,   290,\n",
       "         16176,  4703,  1231,   290,   351, 45840,   516,  6273,    13, 21234,\n",
       "         12955,    11,  1364,  2793, 15094,  5250, 32692,  2356,    13, 34420,\n",
       "            11,  4844,    11,  6373,    13, 16356,  9367,   286,   262, 32956,\n",
       "           290, 16176,  4703,   373,  6157,  1231,   290,   351, 45840,   516,\n",
       "          6273,    13,  7472,   286,  1802,  3939,  6392,   364,   286, 12460,\n",
       "          3824,    52,   373, 17169, 45840,  3481,    13, 42222,  6273,   373,\n",
       "           635,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256],\n",
       "        [50257, 50362,   262, 18536, 16825,   286,   262,   826, 21919,    13,\n",
       "          1318,   318,   645,  7409,  1313,  6570, 37172,   393, 17173,   495,\n",
       "         13165,    13,   383, 43600,   318,  3487,    13,  1318,   389,  3294,\n",
       "         12312, 13370,  4712,   287,   262, 13621,   418, 17225,  1868,    13,\n",
       "          1318,   318,  2370,   286, 25397,  3355,  6546,  3101,   287,   262,\n",
       "           264, 17225,  1868,  7633,    11,  2939,  1271,  8644,    11,   351,\n",
       "         15909,  3735,   965, 27225,   290,  8112,   351,   257, 12312, 13370,\n",
       "         14452,    13,  2312,  6373,   389,  6414,    13, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256],\n",
       "        [50257, 50362,   290,  6512,    12,  4906,  7460,    11,  1390, 19481,\n",
       "           884,   355,  8169,   945,  9449,   290, 10726, 22094,    13,  1375,\n",
       "           468,   587,   319,  3294,  3315,   842, 12117,   290,  4477,   351,\n",
       "         11267,   538, 17459,  7460,    13,  1114, 22338,    11,   706,  1774,\n",
       "          7981,  8281,   373,  6492,    11,   262,  5827,   373,  3181,   284,\n",
       "           262,   886, 17500, 11081, 18389,    13,  1375,   373,  4624,   287,\n",
       "           262,  1364, 25653,  2292,   290,   373,  1813,    13, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "         50256, 50256, 50256, 50256]], device='mps:0')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1_greedy.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b488d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefix length: 2\n",
      "\n",
      "Token choices:\n",
      "Raw ASR: scan\n",
      "GPT-2  : scan\n",
      "Fusion : scan\n",
      "Actual next token in sequence:  scan\n",
      "\n",
      "Full sequences:\n",
      "Context: <|startoftranscript|><|notimestamps|> Gidney, adrenal, abdomen, and pelvis, CT\n",
      "With raw ASR: <|startoftranscript|><|notimestamps|> Gidney, adrenal, abdomen, and pelvis, CT scan\n",
      "With fusion: <|startoftranscript|><|notimestamps|> Gidney, adrenal, abdomen, and pelvis, CT scan\n",
      "Actual sequence: <|startoftranscript|><|notimestamps|> Gidney, adrenal, abdomen, and pelvis, CT scan\n",
      "\n",
      "Note: Step 14 was pure ASR during generation (no LogitsProcessor used)\n"
     ]
    }
   ],
   "source": [
    "prefix_len = out1_greedy.sequences.shape[1] - len(out1_greedy.scores)\n",
    "print(f\"Prefix length: {prefix_len}\")\n",
    "\n",
    "t = 14\n",
    "input_ids_at_t = out1_greedy.sequences[:, :prefix_len + t].clone()\n",
    "scores_at_t = out1_greedy.scores[t]\n",
    "\n",
    "in_scope_ids = input_ids_at_t[:, 2:]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gpt2_scores = gpt2(\n",
    "        input_ids=in_scope_ids.to(DEVICE),\n",
    "    ).logits\n",
    "\n",
    "g_lp = torch.log_softmax(gpt2_scores[:, -1, :], dim=-1)\n",
    "w_lp = torch.log_softmax(scores_at_t, dim=-1)\n",
    "\n",
    "# Apply fusion (using alpha=0.3 as in your code)\n",
    "alpha = 0.1\n",
    "fused = scores_at_t.clone()\n",
    "fused[:, :EOS_ID] += alpha * g_lp[:, :EOS_ID]\n",
    "\n",
    "# Get next tokens with different strategies\n",
    "next_token_fused = fused.argmax(dim=-1)\n",
    "next_token_raw = scores_at_t.argmax(dim=-1)\n",
    "next_token_gpt2 = g_lp.argmax(dim=-1)\n",
    "\n",
    "# Decode for comparison\n",
    "print(\"\\nToken choices:\")\n",
    "print(f\"Raw ASR:{processor.decode(next_token_raw[0].item())}\")\n",
    "print(f\"GPT-2  :{processor.decode(next_token_gpt2[0].item())}\")\n",
    "print(f\"Fusion :{processor.decode(next_token_fused[0].item())}\")\n",
    "\n",
    "# Show the actual sequences\n",
    "actual_next = out1_greedy.sequences[:, prefix_len + t]\n",
    "print(f\"Actual next token in sequence: {processor.decode(actual_next[0].item())}\")\n",
    "\n",
    "# Compare full sequences\n",
    "inputs_with_raw = torch.cat([input_ids_at_t, next_token_raw.unsqueeze(1)], dim=-1)\n",
    "inputs_with_fused = torch.cat([input_ids_at_t, next_token_fused.unsqueeze(1)], dim=-1)\n",
    "inputs_actual = out1_greedy.sequences[:, :prefix_len + t + 1]\n",
    "\n",
    "print(\"\\nFull sequences:\")\n",
    "print(f\"Context: {processor.batch_decode(input_ids_at_t)[0]}\")\n",
    "print(f\"With raw ASR: {processor.batch_decode(inputs_with_raw)[0]}\")\n",
    "print(f\"With fusion: {processor.batch_decode(inputs_with_fused)[0]}\")\n",
    "print(f\"Actual sequence: {processor.batch_decode(inputs_actual)[0]}\")\n",
    "\n",
    "print(f\"\\nNote: Step {t} was pure ASR during generation (no LogitsProcessor used)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d261e677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|notimestamps|>'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.decode(50362)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "caef3cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50257, 50362, 50257, 50362, 50256]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.encode('<|startoftranscript|><|notimestamps|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prefix_len = out1_greedy.sequences.shape[1] - len(out1_greedy.scores)\n",
    "print(f\"Prefix length: {prefix_len}\")\n",
    "\n",
    "t = 10\n",
    "input_ids_at_t = out1_greedy.sequences[:, :prefix_len + t].clone()\n",
    "scores_at_t = out1_greedy.scores[t]\n",
    "\n",
    "# print(f\"Analyzing step {t}:\")\n",
    "# print(f\"Input shape: {input_ids_at_t.shape}\")\n",
    "# print(f\"This input was used to generate token at position {prefix_len + t}\")\n",
    "\n",
    "# Apply GPT-2 to the same input\n",
    "oob_mask = input_ids_at_t >= EOS_ID  # 50256\n",
    "filtered_ids = input_ids_at_t.masked_fill(oob_mask, PAD_ID)  # 50257\n",
    "attention_mask = (filtered_ids != PAD_ID).long()\n",
    "filtered_ids\n",
    "# with torch.inference_mode():\n",
    "#     gpt2_scores = gpt2(\n",
    "#         input_ids=filtered_ids.to(DEVICE),\n",
    "#         attention_mask=attention_mask.to(DEVICE)\n",
    "#     ).logits\n",
    "\n",
    "# # Get log probabilities\n",
    "# g_lp = torch.log_softmax(gpt2_scores[:, -1, :], dim=-1)\n",
    "# w_lp = torch.log_softmax(scores_at_t, dim=-1)\n",
    "\n",
    "# # Apply fusion (using alpha=0.3 as in your code)\n",
    "# alpha = 0.1\n",
    "# fused = scores_at_t.clone()\n",
    "# fused[:, :EOS_ID] += alpha * g_lp[:, :EOS_ID]\n",
    "\n",
    "# # Get next tokens with different strategies\n",
    "# next_token_fused = fused.argmax(dim=-1)\n",
    "# next_token_raw = scores_at_t.argmax(dim=-1)\n",
    "# next_token_gpt2 = g_lp.argmax(dim=-1)\n",
    "\n",
    "# # Decode for comparison\n",
    "# print(\"\\nToken choices:\")\n",
    "# print(f\"Raw ASR:{processor.decode(next_token_raw[0].item())}\")\n",
    "# print(f\"GPT-2  :{processor.decode(next_token_gpt2[0].item())}\")\n",
    "# print(f\"Fusion :{processor.decode(next_token_fused[0].item())}\")\n",
    "\n",
    "# # Show the actual sequences\n",
    "# actual_next = out1_greedy.sequences[:, prefix_len + t]\n",
    "# print(f\"Actual next token in sequence: {processor.decode(actual_next[0].item())}\")\n",
    "\n",
    "# # Compare full sequences\n",
    "# inputs_with_raw = torch.cat([input_ids_at_t, next_token_raw.unsqueeze(1)], dim=-1)\n",
    "# inputs_with_fused = torch.cat([input_ids_at_t, next_token_fused.unsqueeze(1)], dim=-1)\n",
    "# inputs_actual = out1_greedy.sequences[:, :prefix_len + t + 1]\n",
    "\n",
    "# print(\"\\nFull sequences:\")\n",
    "# print(f\"Context: {processor.batch_decode(input_ids_at_t)[0]}\")\n",
    "# print(f\"With raw ASR: {processor.batch_decode(inputs_with_raw)[0]}\")\n",
    "# print(f\"With fusion: {processor.batch_decode(inputs_with_fused)[0]}\")\n",
    "# print(f\"Actual sequence: {processor.batch_decode(inputs_actual)[0]}\")\n",
    "\n",
    "# print(f\"\\nNote: Step {t} was pure ASR during generation (no LogitsProcessor used)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69053e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_greedy.sequences\n",
    "oob_mask = input_ids_at_t >= EOS_ID  # 50256\n",
    "filtered_ids = input_ids_at_t.masked_fill(oob_mask, PAD_ID)  # 50257\n",
    "attention_mask = (filtered_ids != PAD_ID).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what generation strategy was used\n",
    "print(f\"Do sample: {whisper.config.do_sample if hasattr(whisper.config, 'do_sample') else 'N/A'}\")\n",
    "print(f\"Temperature: {whisper.config.temperature if hasattr(whisper.config, 'temperature') else 'N/A'}\")\n",
    "print(f\"Num beams: {whisper.config.num_beams if hasattr(whisper.config, 'num_beams') else 'N/A'}\")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# Force greedy decoding (argmax) to match your analysis\n",
    "out1_greedy = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=False,  # Force greedy/argmax\n",
    "    num_beams=1,      # No beam search\n",
    ")\n",
    "\n",
    "# First, determine the prefix length\n",
    "prefix_len = out1_greedy.sequences.shape[1] - len(out1_greedy.scores)\n",
    "print(f\"Prefix length: {prefix_len}\")\n",
    "\n",
    "# Choose which step to analyze\n",
    "t = 10\n",
    "\n",
    "# Get the correct input sequence that was used to generate scores[t]\n",
    "input_ids_at_t = out1_greedy.sequences[:, :prefix_len + t].clone()\n",
    "scores_at_t = out1_greedy.scores[t]\n",
    "\n",
    "print(f\"Analyzing step {t}:\")\n",
    "print(f\"Input shape: {input_ids_at_t.shape}\")\n",
    "print(f\"This input was used to generate token at position {prefix_len + t}\")\n",
    "\n",
    "# NEW: Properly handle special tokens for GPT-2\n",
    "# Extract only valid GPT-2 tokens (< EOS_ID) for each sequence\n",
    "batch_size = input_ids_at_t.shape[0]\n",
    "gpt2_inputs = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Get only tokens that are valid for GPT-2 (< 50256)\n",
    "    valid_mask = input_ids_at_t[i] < EOS_ID\n",
    "    valid_tokens = input_ids_at_t[i][valid_mask]\n",
    "    gpt2_inputs.append(valid_tokens)\n",
    "\n",
    "# Pad sequences to same length for batching\n",
    "max_len = max(len(seq) for seq in gpt2_inputs)\n",
    "filtered_ids = torch.full((batch_size, max_len), PAD_ID, device=DEVICE)\n",
    "attention_mask = torch.zeros((batch_size, max_len), device=DEVICE)\n",
    "\n",
    "for i, seq in enumerate(gpt2_inputs):\n",
    "    filtered_ids[i, :len(seq)] = seq\n",
    "    attention_mask[i, :len(seq)] = 1\n",
    "\n",
    "print(f\"GPT-2 input (first sequence): {processor.decode(gpt2_inputs[0])}\")\n",
    "print(f\"Valid token IDs: {gpt2_inputs[0].tolist()}\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gpt2_scores = gpt2(\n",
    "        input_ids=filtered_ids.to(DEVICE),\n",
    "        attention_mask=attention_mask.to(DEVICE)\n",
    "    ).logits\n",
    "\n",
    "# Get log probabilities\n",
    "g_lp = torch.log_softmax(gpt2_scores[:, -1, :], dim=-1)\n",
    "w_lp = torch.log_softmax(scores_at_t, dim=-1)\n",
    "\n",
    "# Apply fusion\n",
    "alpha = 0.1\n",
    "fused = scores_at_t.clone()\n",
    "fused[:, :EOS_ID] += alpha * g_lp[:, :EOS_ID]\n",
    "\n",
    "# Get next tokens with different strategie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get next tokens with different strategies\n",
    "next_token_fused = fused.argmax(dim=-1)\n",
    "next_token_raw = scores_at_t.argmax(dim=-1)\n",
    "next_token_gpt2 = g_lp.argmax(dim=-1)\n",
    "\n",
    "# Decode for comparison\n",
    "print(\"\\nToken choices:\")\n",
    "print(f\"Raw ASR:{processor.decode(next_token_raw[0].item())}\")\n",
    "print(f\"GPT-2  :{processor.decode(next_token_gpt2[0].item())}\")\n",
    "print(f\"Fusion :{processor.decode(next_token_fused[0].item())}\")\n",
    "\n",
    "# Show the actual sequences\n",
    "actual_next = out1_greedy.sequences[:, prefix_len + t]\n",
    "print(f\"Actual next token in sequence: {processor.decode(actual_next[0].item())}\")\n",
    "\n",
    "# Compare full sequences\n",
    "inputs_with_raw = torch.cat([input_ids_at_t, next_token_raw.unsqueeze(1)], dim=-1)\n",
    "inputs_with_fused = torch.cat([input_ids_at_t, next_token_fused.unsqueeze(1)], dim=-1)\n",
    "inputs_actual = out1_greedy.sequences[:, :prefix_len + t + 1]\n",
    "\n",
    "print(\"\\nFull sequences:\")\n",
    "print(f\"Context: {processor.batch_decode(input_ids_at_t)[0]}\")\n",
    "print(f\"With raw ASR: {processor.batch_decode(inputs_with_raw)[0]}\")\n",
    "print(f\"With fusion: {processor.batch_decode(inputs_with_fused)[0]}\")\n",
    "print(f\"Actual sequence: {processor.batch_decode(inputs_actual)[0]}\")\n",
    "\n",
    "print(f\"\\nNote: Step {t} was pure ASR during generation (no LogitsProcessor used)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_token_id = next_token_gpt2[0].item()\n",
    "print(f\"GPT-2 predicted token ID: {gpt2_token_id}\")\n",
    "\n",
    "# Try different decode methods\n",
    "print(f\"Decode with processor: '{processor.decode(gpt2_token_id)}'\")\n",
    "print(f\"Decode with tokenizer: '{processor.tokenizer.decode([gpt2_token_id])}'\")\n",
    "print(f\"Token string: '{processor.tokenizer.convert_ids_to_tokens([gpt2_token_id])[0]}'\")\n",
    "\n",
    "# Check if it's a space or special character\n",
    "if gpt2_token_id < 50257:\n",
    "    token = processor.tokenizer.convert_ids_to_tokens([gpt2_token_id])[0]\n",
    "    print(f\"Token repr: {repr(token)}\")  # This will show \\n, \\t, spaces etc\n",
    "    print(f\"Token bytes: {token.encode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd81a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e89853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what GPT-2 actually received as input\n",
    "print(f\"Input to GPT-2: {processor.decode(filtered_ids[0])}\")\n",
    "print(f\"Raw token IDs: {filtered_ids[0].tolist()}\")\n",
    "\n",
    "# Let's manually test GPT-2 with clear medical context\n",
    "test_sequence = gpt2_tok.encode(\"The patient had pain in the abdomen and pel\", return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    test_output = gpt2(test_sequence).logits[0, -1, :]\n",
    "    test_probs = torch.softmax(test_output, dim=0)\n",
    "    top5 = test_probs.topk(5)\n",
    "    \n",
    "print(\"\\nGPT-2 predictions for 'abdomen and pel':\")\n",
    "for prob, idx in zip(top5.values, top5.indices):\n",
    "    if idx < 50257:\n",
    "        print(f\"  {processor.decode(idx)}: {prob:.3f}\")\n",
    "\n",
    "# Also check if the model weights look reasonable\n",
    "print(f\"\\nGPT-2 weight stats:\")\n",
    "print(f\"Mean: {gpt2.lm_head.weight.mean().item():.4f}\")\n",
    "print(f\"Std: {gpt2.lm_head.weight.std().item():.4f}\")\n",
    "\n",
    "# Test a few medical terms\n",
    "medical_tests = [\n",
    "    \"The patient's hep\",  # -> hepatic/hepatitis\n",
    "    \"The cardiac cath\",   # -> catheterization  \n",
    "    \"Diagnosed with pneum\" # -> pneumonia\n",
    "]\n",
    "\n",
    "for test in medical_tests:\n",
    "    tokens = processor.tokenizer.encode(test, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = gpt2(tokens).logits[0, -1, :]\n",
    "    next_token = processor.decode(out.argmax().item())\n",
    "    print(f\"\\n'{test}' -> '{next_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, pad_id, eos_id, alpha=0.25, warmup=3):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "        self.pad_id = pad_id # should be 50257 \n",
    "        self.eos_id = eos_id # should be 50256\n",
    "        self.alpha = alpha \n",
    "        self.warmup = warmup\n",
    "        self.step = 0 \n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "\n",
    "        if self.step < self.warmup: \n",
    "            self.step+=1 \n",
    "            return scores\n",
    "        self.step+=1 \n",
    "\n",
    "        oov_mask = input_ids >= self.eos_id # gpt2 and whispers EOS token\n",
    "        padded_input_ids = input_ids.masked_fill(oov_mask, self.pad_id) # PAD_ID\n",
    "        attention_mask = (padded_input_ids != self.pad_id).long()\n",
    "        \n",
    "        lm_logits = self.lm(\n",
    "            input_ids=padded_input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).logits[:,-1,:] # just want next token logits\n",
    "\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "        \n",
    "        fused = scores.clone()\n",
    "        fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        # optional normalization step\n",
    "        # fused -= torch.logsumexp(fused, dim=-1, keepdim=True)\n",
    "        return fused\n",
    "\n",
    "alpha=0.3\n",
    "warmup = 1\n",
    "fusion_proc = ShallowFusion(gpt2, PAD_ID, EOS_ID, alpha=alpha, warmup=warmup)\n",
    "\n",
    "out = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    logits_processor=[fusion_proc],\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "\n",
    "prefix_len = out.sequences.shape[1] - len(out.scores)  # Should be 2\n",
    "pure_asr_step0 = out.scores[0]  # This is pure ASR (no fusion)\n",
    "\n",
    "input_for_step0 = out.sequences[:, :prefix_len]\n",
    "\n",
    "oov = input_for_step0 >= EOS_ID\n",
    "gpt2_inp = input_for_step0.masked_fill(oov, PAD_ID)\n",
    "attn = (gpt2_inp != PAD_ID).long()\n",
    "\n",
    "lm_logits = gpt2(gpt2_inp, attention_mask=attn).logits[:, -1]\n",
    "lm_logp = torch.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "fused_manual = pure_asr_step0.clone()\n",
    "fused_manual[:, :EOS_ID] += alpha * lm_logp[:, :EOS_ID]\n",
    "\n",
    "# Now run AGAIN with warmup=0 to get actual fused scores at step 0\n",
    "fusion_proc2 = ShallowFusion(gpt2, PAD_ID, EOS_ID, alpha=alpha, warmup=0)\n",
    "out2 = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    logits_processor=[fusion_proc2],\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "\n",
    "# Compare!\n",
    "fused_actual = out2.scores[0]\n",
    "diff = (fused_actual - fused_manual).abs()\n",
    "valid_mask = ~torch.isnan(diff)\n",
    "print(f\"Max diff: {diff[valid_mask].max().item():.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "(out3.scores[t] != fused).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba41e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a986a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 5\n",
    "input_ids_at_t = out1.sequences[:,:t].clone()\n",
    "scores_at_t = out1.scores[t-1]\n",
    "texts = processor.batch_decode(\n",
    "    input_ids_at_t,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "# texts = [t.strip() for t in texts] # this doesnt seem to make a difference\n",
    "gpt2_inputs = gpt2_tok(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,           # pads to longest in batch\n",
    "    truncation=False,       # adjust as you like\n",
    ").input_ids.to(DEVICE)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gpt2_scores = gpt2(\n",
    "        input_ids = gpt2_inputs,\n",
    "        attention_mask = torch.ones_like(gpt2_inputs).to(DEVICE)\n",
    "    ).logits[:,-1]\n",
    "\n",
    "g_lp = torch.log_softmax(gpt2_scores, dim=-1)\n",
    "w_lp = torch.log_softmax(scores_at_t, dim=-1)\n",
    "\n",
    "fused = w_lp.clone()\n",
    "fused[:, :g_lp.size(1)] += 0 * g_lp\n",
    "next_token = fused.argmax(dim=-1).unsqueeze(1)\n",
    "next_token_raw = scores_at_t.argmax(dim=-1).unsqueeze(1)\n",
    "\n",
    "inputs_raw = torch.cat([input_ids_at_t, next_token_raw], dim=-1)\n",
    "inputs_fused = torch.cat([input_ids_at_t, next_token], dim=-1)\n",
    "# inputs_fused -= torch.logsumexp(inputs_fused, dim=-1, keepdim=True)\n",
    "\n",
    "a = processor.batch_decode(input_ids_at_t)\n",
    "b = processor.batch_decode(inputs_fused)\n",
    "c = processor.batch_decode(inputs_raw)\n",
    "d = processor.batch_decode(out1.sequences[:,:t+1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147a785",
   "metadata": {},
   "source": [
    "### ------------ TESTING END ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================\n",
    "# #  One-cell evaluation (uses your original jiwer transform)\n",
    "# #  -------------------------------------------------------------\n",
    "# #  Metrics per model:\n",
    "# #    • Global WER                --> same as your old script\n",
    "# #    • Medical-Term Recall (MTR) --> fraction of terms perfectly present\n",
    "# #    • Medical-Term-only WER     --> WER on tokens that belong to terms\n",
    "# #\n",
    "# #  Expects a DataFrame `results_df` with columns:\n",
    "# #        reference, vanilla, fused, medical_terms\n",
    "# #  where medical_terms is list[str]  (or a string repr like \"['a','b']\").\n",
    "# # =============================================================\n",
    "\n",
    "# import re, ast, itertools, pandas as pd\n",
    "# from jiwer import (\n",
    "#     Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces,\n",
    "#     Strip, ReduceToListOfListOfWords, wer\n",
    "# )\n",
    "# from unidecode import unidecode\n",
    "\n",
    "\n",
    "# # ---------- helper to handle both str & list[str] -------------------\n",
    "# def _map(func, x):\n",
    "#     return [func(t) for t in x] if isinstance(x, list) else func(x)\n",
    "\n",
    "# def remove_diacritics(x):\n",
    "#     return _map(unidecode, x)\n",
    "\n",
    "# def split_hyphens_and_slashes(x):\n",
    "#     return _map(lambda t: re.sub(r\"[-–—/]\", \" \", t), x)\n",
    "\n",
    "# def normalize_nums(x):\n",
    "#     return _map(lambda t: re.sub(r\"(\\d)[-–—-](\\d)\", r\"\\1-\\2\", t), x)\n",
    "\n",
    "# # ---------- your original jiwer transform ---------------------------\n",
    "# transform = Compose([\n",
    "#     ToLowerCase(),\n",
    "#     remove_diacritics,\n",
    "#     split_hyphens_and_slashes,\n",
    "#     normalize_nums,\n",
    "#     RemovePunctuation(),\n",
    "#     RemoveMultipleSpaces(),\n",
    "#     Strip(),\n",
    "#     ReduceToListOfListOfWords(),   # -> [[\"word\", ...], ...]\n",
    "# ])\n",
    "\n",
    "# def compute_wer(ref, hyp):\n",
    "#     return wer(\n",
    "#         ref, hyp,\n",
    "#         reference_transform=transform,\n",
    "#         hypothesis_transform=transform,\n",
    "#     )\n",
    "\n",
    "# # ---------- lightweight normaliser for term metrics -----------------\n",
    "# _punc_rx   = re.compile(r\"[^\\w\\s]\")\n",
    "# _range_rx  = re.compile(r\"(\\d)[-–—-](\\d)\")\n",
    "# _split_rx  = re.compile(r\"[-–—/]\")\n",
    "\n",
    "# def _normalise(text: str) -> str:\n",
    "#     text = unidecode(text.lower())\n",
    "#     text = _range_rx.sub(r\"\\1-\\2\", text)\n",
    "#     text = _split_rx.sub(\" \", text)\n",
    "#     text = _punc_rx.sub(\" \", text)\n",
    "#     return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# def _term_recall(row, hyp_text):\n",
    "#     hyp_norm = _normalise(hyp_text)\n",
    "#     hits = sum(1 for t in row[\"medical_terms\"] if _normalise(t) in hyp_norm)\n",
    "#     return hits / len(row[\"medical_terms\"])\n",
    "\n",
    "# def _extract_term_tokens(row, text):\n",
    "#     tokens = _normalise(text).split()\n",
    "#     keep   = [False] * len(tokens)\n",
    "#     for term in row[\"medical_terms\"]:\n",
    "#         ttoks = _normalise(term).split()\n",
    "#         for i in range(len(tokens) - len(ttoks) + 1):\n",
    "#             if tokens[i:i+len(ttoks)] == ttoks:\n",
    "#                 for j in range(i, i+len(ttoks)):\n",
    "#                     keep[j] = True\n",
    "#     return \" \".join(tok for tok, flag in zip(tokens, keep) if flag)\n",
    "\n",
    "# # ---------- main evaluation routine ---------------------------------\n",
    "# def evaluate(df: pd.DataFrame) -> None:\n",
    "#     # ensure list[str] in medical_terms\n",
    "#     if isinstance(df[\"medical_terms\"].iloc[0], str):\n",
    "#         df[\"medical_terms\"] = df[\"medical_terms\"].apply(ast.literal_eval)\n",
    "\n",
    "#     # Global WER (your existing metric)\n",
    "#     df[\"wer_vanilla\"] = df.apply(\n",
    "#         lambda r: compute_wer(r[\"reference\"], r[\"vanilla\"]), axis=1)\n",
    "#     df[\"wer_fused\"]   = df.apply(\n",
    "#         lambda r: compute_wer(r[\"reference\"], r[\"fused\"]), axis=1)\n",
    "\n",
    "#     # Medical-Term Recall\n",
    "#     df[\"mtr_vanilla\"] = df.apply(\n",
    "#         lambda r: _term_recall(r, r[\"vanilla\"]), axis=1)\n",
    "#     df[\"mtr_fused\"]   = df.apply(\n",
    "#         lambda r: _term_recall(r, r[\"fused\"]), axis=1)\n",
    "\n",
    "#     # Medical-Term-only WER\n",
    "#     df[\"mtwer_vanilla\"] = df.apply(\n",
    "#         lambda r: wer(\n",
    "#             _extract_term_tokens(r, r[\"reference\"]),\n",
    "#             _extract_term_tokens(r, r[\"vanilla\"]),\n",
    "#             reference_transform=transform,\n",
    "#             hypothesis_transform=transform), axis=1)\n",
    "#     df[\"mtwer_fused\"]   = df.apply(\n",
    "#         lambda r: wer(\n",
    "#             _extract_term_tokens(r, r[\"reference\"]),\n",
    "#             _extract_term_tokens(r, r[\"fused\"]),\n",
    "#             reference_transform=transform,\n",
    "#             hypothesis_transform=transform), axis=1)\n",
    "\n",
    "#     # -------- summary printout --------------------------------------\n",
    "#     print(\"\\n=== Global WER ===\")\n",
    "#     print(f\"  vanilla : {df['wer_vanilla'].mean():.4f}\")\n",
    "#     print(f\"  fused   : {df['wer_fused'].mean():.4f}\")\n",
    "\n",
    "#     print(\"\\n=== Medical-Term Recall ===\")\n",
    "#     print(f\"  vanilla : {df['mtr_vanilla'].mean():.4f}\")\n",
    "#     print(f\"  fused   : {df['mtr_fused'].mean():.4f}\")\n",
    "\n",
    "#     print(\"\\n=== Medical-Term-only WER ===\")\n",
    "#     print(f\"  vanilla : {df['mtwer_vanilla'].mean():.4f}\")\n",
    "#     print(f\"  fused   : {df['mtwer_fused'].mean():.4f}\")\n",
    "\n",
    "# # ---------- run on your DataFrame -----------------------------------\n",
    "\n",
    "# import pandas as pd \n",
    "\n",
    "# results_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"vanilla\":[i.strip() for i in vanilla], \n",
    "#         \"fused\":[i.strip() for i in fused], \n",
    "#         \"reference\":ds['text'],\n",
    "#         \"medical_terms\":ds['medical_terms']\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# evaluate(results_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LogitsProcessor, LogitsProcessorList\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ShallowFusion(LogitsProcessor):\n",
    "#     def __init__(self, lm, shared_vocab, eos, alpha=0.3, warmup_steps=3):\n",
    "#         super().__init__()\n",
    "#         self.lm = lm.eval().requires_grad_(False)\n",
    "#         self.V = shared_vocab\n",
    "#         self.eos = eos\n",
    "#         self.alpha = alpha\n",
    "#         self.warmup = warmup_steps\n",
    "#         self.step = 0\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         print('printing input_ids.size(), scores.size(), step, input_ids, dec_ids')\n",
    "#         print(input_ids.size(), scores.size(), self.step, input_ids, processor.batch_decode(input_ids))\n",
    "#         self.step+=1 \n",
    "\n",
    "#         return scores\n",
    "    \n",
    "# fusion_proc = ShallowFusion(\n",
    "#     lm=gpt2,\n",
    "#     shared_vocab=gpt2.config.vocab_size,\n",
    "#     eos=EOS_ID,\n",
    "#     alpha=0.3\n",
    "# )\n",
    "\n",
    "# batch = next(iter(loader))\n",
    "# feats = batch['input_features'].to(DEVICE)\n",
    "# masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     out1 = whisper.generate(\n",
    "#         input_features=feats,\n",
    "#         attention_mask=masks,\n",
    "#         logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "#         return_dict_in_generate=True,\n",
    "#         output_scores=True,\n",
    "#         num_beams=2,\n",
    "#     )\n",
    "\n",
    "#     # out2 = whisper.generate(\n",
    "#     #     input_features=feats,\n",
    "#     #     attention_mask=masks,\n",
    "#     #     return_dict_in_generate=True,\n",
    "#     #     output_scores=True,\n",
    "#     #     num_beams=2\n",
    "#     # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14230833",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# Generate to get sequences\n",
    "with torch.no_grad():\n",
    "    out = whisper.generate(\n",
    "        input_features=feats,\n",
    "        attention_mask=masks,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=5\n",
    "    )\n",
    "# Compare step 0\n",
    "decoder_ids = out.sequences[:,0:-1]  # Just the start token\n",
    "with torch.no_grad():\n",
    "    direct_logits = whisper(feats, decoder_input_ids=decoder_ids).logits[:, -1, :].to(DEVICE)\n",
    "    direct_lp = torch.log_softmax(direct_logits, dim=-1)\n",
    "\n",
    "gen_lp = out.scores[-1].to(DEVICE)\n",
    "\n",
    "print(gen_lp)\n",
    "print(direct_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d3e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd359960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter truly out of bounds vocab >=EOS\n",
    "oob_mask = decoder_ids > EOS_ID # create mask for gpt2 OOV tokens emitted by whisper\n",
    " # replace with gpt2 pad token\n",
    "filtered = decoder_ids.masked_fill(oob_mask, gpt2_tok.pad_token_id)\n",
    "attention_mask = (filtered != gpt2_tok.pad_token_id).long()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_new = gpt2(input_ids=filtered, attention_mask=attention_mask).logits[:,-1, :]\n",
    "\n",
    "# because we dont want gpt2 to impact or determine termination just ASR model\n",
    "logits_new[:,:EOS_ID-1].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
