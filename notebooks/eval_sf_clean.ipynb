{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7588133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SHALLOW_FUSION_EVAL', 'SF_EVAL', '.models']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, librosa, numpy as np, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(42)\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "print(str(CACHE_DIR).split('/')[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3be2e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    GPT2LMHeadModel, AutoTokenizer\n",
    ")\n",
    "\n",
    "SR = 16_000\n",
    "BATCH_SIZE = 5\n",
    "WHISPER_ID = \"openai/whisper-small.en\"\n",
    "GPT2_ID = \"cwestnedge/gpt2-small-pubmed\"\n",
    "GPT2_ID = \"openai-community/gpt2\"\n",
    "\n",
    "MANIFEST = \"../data/output/manifest.jsonl\"\n",
    "AUDIO_DIR = \"../data/output\"  \n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8f72bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder start token ID: 50257\n",
      "Decoder start token: <|startoftranscript|>\n",
      "\n",
      "Complete prefix: [50257, 50362]\n",
      "Decoded: '<|startoftranscript|><|notimestamps|>'\n",
      "Total prefix length: 2\n"
     ]
    }
   ],
   "source": [
    "# fast tokenizers will show token mismatch between models and will be auto loaded when we run on colab A100 set flag to false to avoid annoyingness\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "EOS_ID = gpt2_tok.eos_token_id # 50256 (unchanged)\n",
    "\n",
    "print(\"Decoder start token ID:\", whisper.generation_config.decoder_start_token_id)\n",
    "print(\"Decoder start token:\", processor.decode([whisper.generation_config.decoder_start_token_id]))\n",
    "\n",
    "PREFIX_TOK_IDS = [whisper.generation_config.decoder_start_token_id]\n",
    "for position, token_id in whisper.generation_config.forced_decoder_ids:\n",
    "    PREFIX_TOK_IDS.append(token_id)\n",
    "\n",
    "print(f\"\\nComplete prefix: {PREFIX_TOK_IDS}\")\n",
    "print(f\"Decoded: '{processor.decode(PREFIX_TOK_IDS)}'\")\n",
    "print(f\"Total prefix length: {len(PREFIX_TOK_IDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c5dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gpt2_tok.get_vocab())):\n",
    "    a = processor.tokenizer.decode([i])\n",
    "    b = gpt2_tok.decode([i])\n",
    "    if a != b:\n",
    "        print(f\"Token mismatch at index {i}\\nwhisper token: [{a}]\\n   gpt2 token: [{b}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be085a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 358/358 [00:07<00:00, 45.72 examples/s] \n",
      "Map: 100%|██████████| 358/358 [00:07<00:00, 46.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(manifest_path: str, batch_size: int, num_proc: int = 4) -> Dataset:\n",
    "    with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "\n",
    "    def add_audio(batch):\n",
    "        batch[\"audio\"] = [\n",
    "            librosa.load(f\"{AUDIO_DIR}/{fname}\", sr=SR, mono=True)[0].astype(np.float32)\n",
    "            for fname in batch[\"file\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return ds.map(add_audio, batched=True, batch_size=batch_size, num_proc=num_proc)\n",
    "\n",
    "def encode_audio(batch):\n",
    "    # batch[\"audio\"] is List[np.ndarray], each at its natural length\n",
    "    feats = processor.feature_extractor(\n",
    "        batch[\"audio\"], # for whatever reason processor doesnt support PT tensors so numpy array or list for now.\n",
    "        sampling_rate=SR,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=processor.feature_extractor.n_samples,  # n_samples = chunk_length * sampling_rate\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    #  input_features : Tensor (B, T_max, 80)\n",
    "    #  attention_mask : Tensor (B, T_max)\n",
    "    batch[\"input_features\"] = feats.input_features\n",
    "    batch[\"attention_mask\"] = feats.attention_mask\n",
    "    return batch\n",
    "\n",
    "ds = build_dataset(MANIFEST, batch_size=BATCH_SIZE) #.select(range(20)) #.select(range(10))\n",
    "\n",
    "# choosing NOT to overwrite ds with removed fields so we can eval on text field later,\n",
    "# could also create a collator and pass fields we care about through, but seems like \n",
    "# too much extra code tbh, indices will still match if we dont shuffle\n",
    "ds_processed = ds.map(\n",
    "    encode_audio, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    batched=True,\n",
    "    remove_columns=list(ds.features.keys())\n",
    "    )\n",
    "\n",
    "ds_processed.set_format(type=\"torch\", columns=[\"input_features\",\"attention_mask\"])\n",
    "loader = DataLoader(ds_processed, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0fc3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "\n",
    "class NoOpLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    A simple logits processor that performs no modifications to the logits.\n",
    "    \"\"\"\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Return the scores unchanged\n",
    "\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        return w_lp\n",
    "    \n",
    "\n",
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, eos_id, prefix_tokens, alpha=0.25, warmup=3):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "        self.eos_id = eos_id\n",
    "        self.prefix_tokens = prefix_tokens  # <|startoftranscript|><|notimestamps|>\n",
    "        self.alpha = alpha \n",
    "        self.warmup = warmup\n",
    "        self.step = 0 \n",
    "        self.use_mask = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        if self.step < self.warmup: \n",
    "            self.step += 1 \n",
    "            return w_lp\n",
    "\n",
    "        prefix_len = len(self.prefix_tokens)\n",
    "        prefix_ids = torch.tensor(self.prefix_tokens, device=input_ids.device)\n",
    "\n",
    "        \n",
    "        # make sure that the actual prefix matches expectation \n",
    "        if self.step == self.warmup:  # (only need to check once)\n",
    "            prefix_matches = (input_ids[0, :prefix_len] == prefix_ids).all()\n",
    "            if not prefix_matches:\n",
    "                print(f\"WARNING: prefix mismatch:\\nexpected {self.prefix_tokens}, got {input_ids[0, :prefix_len].tolist()}\")\n",
    "        \n",
    "        # make sure we dont have special tokens emitted AFTER decoder prefix\n",
    "        has_special_after_prefix = (input_ids[:, prefix_len:] > self.eos_id).any()\n",
    "        if has_special_after_prefix:\n",
    "            print(f\"WARNING: special tokens found after prefix at step {self.step}\")\n",
    "        \n",
    "        clean_ids = input_ids[:, prefix_len:]\n",
    "        lm_logits = self.lm(input_ids=clean_ids).logits[:, -1, :]\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "        # FUSION STEP \n",
    "        # P_fused(y|x) = log P_ASR(y|x) + [HAS_EOS_MASK] * alpha * log P_LM(y)\n",
    "        fused = w_lp.clone()\n",
    "        if self.use_mask: \n",
    "            # this condition is to help exclude terminated sequences from getting revived by our lm\n",
    "            has_eos = (input_ids == self.eos_id).any(dim=1)\n",
    "            fusion_mask = (~has_eos).float().unsqueeze(-1) \n",
    "            fused[:, :self.eos_id] += fusion_mask*self.alpha * lm_lp[:, :self.eos_id]\n",
    "        else: \n",
    "            fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        \n",
    "        # print(input_ids)\n",
    "        # optional normalization step\n",
    "        # fused -= torch.logsumexp(fused, dim=-1, keepdim=True)\n",
    "        self.step += 1\n",
    "        return fused\n",
    "  \n",
    "fusion_proc = ShallowFusion(\n",
    "    lm=gpt2,\n",
    "    eos_id=EOS_ID,\n",
    "    prefix_tokens=PREFIX_TOK_IDS,\n",
    "    alpha=0.25,\n",
    "    warmup=2\n",
    ")\n",
    "fusion_proc.use_mask = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "61d2dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 72/72 [01:44<00:00,  1.45s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "from tqdm import tqdm \n",
    "\n",
    "fused = []\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        fused_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "            num_beams=1,\n",
    "            return_timestamps=False,\n",
    "            return_token_timestamps=False,\n",
    "            # do_sample=False,\n",
    "            # length_penalty=1.1,\n",
    "            # repetition_penalty=1.1,\n",
    "            # max_length=100,  # Safety limit\n",
    "        )\n",
    "\n",
    "    decoded = processor.batch_decode(\n",
    "        fused_ids, \n",
    "        skip_special_tokens=True, \n",
    "        # output_word_offsets=True\n",
    "        )\n",
    "    \n",
    "    fused.extend(decoded)\n",
    "    fusion_proc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f466239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 72/72 [01:19<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "noop_processor = NoOpLogitsProcessor()\n",
    "vanilla = []\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        vanilla_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([noop_processor]),\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            return_timestamps=False,\n",
    "            return_token_timestamps=False\n",
    "            # do_sample=False,\n",
    "            # length_penalty=1.1,\n",
    "            # repetition_penalty=1.1,\n",
    "            # max_length=100,  # Safety limit\n",
    "        )\n",
    "\n",
    "    decoded = processor.batch_decode(\n",
    "        vanilla_ids, \n",
    "        skip_special_tokens=True, \n",
    "        # output_word_offsets=True\n",
    "        )\n",
    "    \n",
    "    vanilla.extend(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f06c8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"base\":[i.strip() for i in vanilla], \n",
    "        \"fused\":[i.strip() for i in fused], \n",
    "        \"reference\":ds['text'],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "09d68078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base (macro) WER (punct-insensitive): 0.06720552802472587\n",
      "Fused (macro) WER (punct-insensitive): 0.0729187381686821\n",
      "\n",
      "Base (micro) WER (punct-insensitive): 0.06901782328403488\n",
      "Fused (micro) WER (punct-insensitive): 0.07527493363670838\n"
     ]
    }
   ],
   "source": [
    "from jiwer import (\n",
    "    Compose,\n",
    "    ToLowerCase,\n",
    "    RemovePunctuation,\n",
    "    RemoveMultipleSpaces,\n",
    "    Strip,\n",
    "    ReduceToListOfListOfWords,\n",
    "    wer\n",
    ")\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "# helper to handle both str and list[str]\n",
    "def _map(func, x):\n",
    "    return [func(t) for t in x] if isinstance(x, list) else func(x)\n",
    "\n",
    "def remove_diacritics(x):\n",
    "    return _map(unidecode, x)\n",
    "\n",
    "def split_hyphens_and_slashes(x):\n",
    "    # replace any dash or slash with a space so we never glue words together\n",
    "    return _map(lambda t: re.sub(r\"[-–—/]\", \" \", t), x)\n",
    "\n",
    "def normalize_nums(x):\n",
    "    # unify 12–16 → 12-16\n",
    "    return _map(lambda t: re.sub(r\"(\\d)[-–—-](\\d)\", r\"\\1-\\2\", t), x)\n",
    "\n",
    "def normalize_measurements(x):\n",
    "    \"\"\"Normalize all measurement units to their abbreviated forms\"\"\"\n",
    "    def normalize_single(text):\n",
    "        # Create a mapping of full forms to abbreviated forms\n",
    "        # Handle both singular and plural, case-insensitive\n",
    "        replacements = [\n",
    "            # Length measurements\n",
    "            (r'\\b(millimeters?|millimetres?)\\b', 'mm'),\n",
    "            (r'\\b(centimeters?|centimetres?)\\b', 'cm'),\n",
    "            (r'\\b(meters?|metres?)\\b', 'm'),\n",
    "            (r'\\b(kilometers?|kilometres?)\\b', 'km'),\n",
    "            \n",
    "            # Weight measurements\n",
    "            (r'\\b(milligrams?)\\b', 'mg'),\n",
    "            (r'\\b(grams?)\\b', 'g'),\n",
    "            (r'\\b(kilograms?)\\b', 'kg'),\n",
    "            \n",
    "            # Volume measurements\n",
    "            (r'\\b(milliliters?|millilitres?)\\b', 'ml'),\n",
    "            (r'\\b(liters?|litres?)\\b', 'l'),\n",
    "            \n",
    "            # Pressure measurements\n",
    "            (r'\\b(millimeters? of mercury)\\b', 'mmhg'),\n",
    "            \n",
    "            # Other medical measurements\n",
    "            (r'\\b(seconds?)\\b', 's'),\n",
    "            (r'\\b(minutes?)\\b', 'min'),\n",
    "            (r'\\b(hours?)\\b', 'hr'),\n",
    "            (r'\\b(days?)\\b', 'd'),\n",
    "            (r'\\b(weeks?)\\b', 'wk'),\n",
    "            (r'\\b(months?)\\b', 'mo'),\n",
    "            (r'\\b(years?)\\b', 'yr'),\n",
    "            \n",
    "            # Percentage (optional - depends on your preference)\n",
    "            (r'\\b(percent)\\b', '%'),\n",
    "        ]\n",
    "        \n",
    "        result = text\n",
    "        for pattern, replacement in replacements:\n",
    "            result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return _map(normalize_single, x)\n",
    "\n",
    "def normalize_numbers(x):\n",
    "    \"\"\"Normalize number representations\"\"\"\n",
    "    def normalize_single(text):\n",
    "        # Convert spelled-out numbers to digits (for common medical ones)\n",
    "        number_map = {\n",
    "            'zero': '0',\n",
    "            'one': '1', 'two': '2', 'three': '3', 'four': '4', 'five': '5',\n",
    "            'six': '6', 'seven': '7', 'eight': '8', 'nine': '9', 'ten': '10',\n",
    "            'eleven': '11', 'twelve': '12', 'thirteen': '13', 'fourteen': '14',\n",
    "            'fifteen': '15', 'sixteen': '16', 'seventeen': '17', 'eighteen': '18',\n",
    "            'nineteen': '19', 'twenty': '20'\n",
    "        }\n",
    "        \n",
    "        result = text\n",
    "        for word, digit in number_map.items():\n",
    "            result = re.sub(r'\\b' + word + r'\\b', digit, result, flags=re.IGNORECASE)\n",
    "        \n",
    "        # Also normalize 'negative' to minus sign for medical contexts\n",
    "        result = re.sub(r'\\bnegative\\s+(\\d)', r'-\\1', result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    return _map(normalize_single, x)\n",
    "\n",
    "transform = Compose([\n",
    "    ToLowerCase(),\n",
    "    remove_diacritics,\n",
    "    normalize_measurements,        # ← Add measurement normalization early\n",
    "    normalize_numbers,             # ← Add number normalization\n",
    "    # normalize_medical_terms,       # ← Optional: medical term normalization\n",
    "    split_hyphens_and_slashes,    \n",
    "    normalize_nums,\n",
    "    RemovePunctuation(),           \n",
    "    RemoveMultipleSpaces(),\n",
    "    Strip(),\n",
    "    ReduceToListOfListOfWords(),   \n",
    "])\n",
    "\n",
    "def compute_wer(ref, hyp):\n",
    "    return wer(\n",
    "        ref, hyp,\n",
    "        reference_transform=transform,\n",
    "        hypothesis_transform=transform,\n",
    "    )\n",
    "\n",
    "# rename & score\n",
    "results_df = results_df.rename(columns={\"gt\": \"reference\"})\n",
    "results_df[\"wer_base\"]  = results_df.apply(\n",
    "    lambda r: compute_wer(r[\"reference\"], r[\"base\"]), axis=1\n",
    ")\n",
    "results_df[\"wer_fused\"] = results_df.apply(\n",
    "    lambda r: compute_wer(r[\"reference\"], r[\"fused\"]), axis=1\n",
    ")\n",
    "\n",
    "print(\"Base (macro) WER (punct-insensitive):\", results_df[\"wer_base\"].mean())\n",
    "print(\"Fused (macro) WER (punct-insensitive):\", results_df[\"wer_fused\"].mean())\n",
    "\n",
    "micro_base  = wer(results_df.reference.to_list(), results_df.base.to_list(),  reference_transform=transform, hypothesis_transform=transform)\n",
    "micro_fused = wer(results_df.reference.to_list(), results_df.fused.to_list(), reference_transform=transform, hypothesis_transform=transform)\n",
    "\n",
    "print()\n",
    "print(\"Base (micro) WER (punct-insensitive):\", micro_base)\n",
    "print(\"Fused (micro) WER (punct-insensitive):\", micro_fused)\n",
    "\n",
    "# Optional: Show which examples changed most with normalization\n",
    "results_df[\"improvement\"] = results_df[\"wer_base\"] - results_df[\"wer_fused\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c1ac6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['diff'] = results_df.wer_base - results_df.wer_fused\n",
    "top_diffs = results_df.sort_values(by='diff', ascending=False)\n",
    "top_diffs = top_diffs[top_diffs['diff']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "32014088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "error_mask = (results_df['diff'] < 0)\n",
    "success_mask = (results_df['diff'] > 0)\n",
    "\n",
    "print(len(results_df[error_mask]))\n",
    "print(len(results_df[success_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87f577d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GT:    Two view chest radiographs demonstrate mild bibasilar atelectasis without cardiomegaly or effusion.\n",
      "Base:  Two view chest radiographs demonstrate mild bi-basilar atelectasis without cardiomegaly or effusion.\n",
      "Fused: Two view-chest radiographs demonstrate mild bibasilar atelectasis without cardiomegaly or effusion.\n",
      "base err - fused err: 0.16666666666666666\n",
      "\n",
      "GT:    Single view chest radiograph shows clear lungs without focal airspace consolidation, pleural effusion, or pneumothorax.\n",
      "Base:  Single View Chest Radiograph shows clear lungs without focal airspace consolidation, plural effusion or pneumothorax.\n",
      "Fused: Single View Chest Radiograph shows clear lungs without focal airspace consolidation, pleural effusion, or pneumothorax.\n",
      "base err - fused err: 0.06666666666666667\n",
      "\n",
      "GT:    Head computed tomography shows no calvarial fracture and patent basal cisterns.\n",
      "Base:  Head-computed tomography shows no calvariel fracture and patent basal cisterns.\n",
      "Fused: Head-computed tomography shows no calvarial fracture and patent basal cisterns.\n",
      "base err - fused err: 0.09090909090909091\n",
      "\n",
      "GT:    Head computed tomography shows loss of the gray white differentiation and mild sulcal effacement in the left middle cerebral artery territory concerning for acute infarct.\n",
      "Base:  Edcomputed tomography shows loss of the gray-white differentiation and mild sulcal effacement in the left-middle cerebral artery territory. Concerning for acute infarct.\n",
      "Fused: Edcomputed tomography shows loss of the gray-white differentiation and mild sulcal effacement in the left-middle cerebral artery territory.\n",
      "base err - fused err: -0.15999999999999998\n",
      "\n",
      "GT:    Computed tomography abdomen and pelvis shows bilateral nonobstructive renal calculi measuring up to four millimeters.\n",
      "Base:  computed tomography abdomen and pelvis shows bilateral nonobstructive renal calculi measuring up to 4 mm.\n",
      "Fused: computed tomography abdomen and pelvis shows bilateral non-obstructive renal calculi measuring up to 4 mm.\n",
      "base err - fused err: -0.13333333333333333\n",
      "\n",
      "GT:    Magnetic resonance imaging brain and internal auditory canals shows no vestibular schwannoma and normal seventh and eighth cranial nerves.\n",
      "Base:  Magnetic resonance imaging brain and internal auditory canals shows no vestibular schwannoma and normal seventh and eighth cranial nerves.\n",
      "Fused: Magnetic resonance imaging brain and internal auditory canals shows no vestibular schwannoma and normal 7th and 8th cranial nerves.\n",
      "base err - fused err: -0.10526315789473684\n",
      "\n",
      "GT:    Magnetic resonance imaging brain with contrast demonstrates multiple periventricular and juxtacortical T two hyperintense plaques consistent with demyelinating disease.\n",
      "Base:  Magnetic resonance imaging brain with contrast demonstrates multiple periventricular and jups to cortical T2 hyperintense plaques, consistent with demyelinating disease.\n",
      "Fused: Magnetic resonance imaging brain with contrast demonstrates multiple periventricular and juxtacortical T2-hyperintense plaques, consistent with demyelinating disease.\n",
      "base err - fused err: 0.10526315789473684\n",
      "\n",
      "GT:    Magnetic resonance imaging thoracic spine demonstrates no cord signal abnormality and no significant canal stenosis.\n",
      "Base:  Magnetic resonance imaging thoracic spine demonstrates no chord signal abnormality and no significant canal stenosis.\n",
      "Fused: Magnetic resonance imaging thoracic spine demonstrates no cord signal abnormality and no significant canal stenosis.\n",
      "base err - fused err: 0.06666666666666667\n",
      "\n",
      "GT:    Magnetic resonance imaging cervical spine without contrast shows multilevel spondylosis with severe right neural foraminal narrowing at C five to C six.\n",
      "Base:  Magnetic resonance imaging cervical spine, without contrast, shows multilevel spondylosis with severe right neural forraminal narrowing at C5 to C6.\n",
      "Fused: Magnetic resonance imaging cervical spine, without contrast, shows multilevel spondylosis with severe right neural foraminal narrowing at C5 to C6.\n",
      "base err - fused err: 0.04545454545454544\n",
      "\n",
      "GT:    Magnetic resonance imaging pelvis without and with contrast shows a six centimeter uterine intramural fibroid with homogeneous enhancement.\n",
      "Base:  Magnetic Resonance Imaging Pelvis Without and With Contrast shows a 6-centimeter uterine intramural fibroid with homogeneous enhancement.\n",
      "Fused: Magnetic Resonance Imaging Pelvis with Out-and-With Contrast shows a 6-centimeter uterine intramural fibroid with homogeneous enhancement.\n",
      "base err - fused err: -0.1111111111111111\n",
      "\n",
      "GT:    Magnetic resonance imaging hip demonstrates cam type femoroacetabular impingement morphology with anterosuperior labral tear.\n",
      "Base:  Magnetic Resonance Imaging HIP demonstrates CAM type femoroacetabular impingement morphology with anterosuperior labri-tare.\n",
      "Fused: Magnetic Resonance Imaging-HIP demonstrates CAM-type femoroacetabular impingement morphology with anterosuperior labral tear.\n",
      "base err - fused err: 0.14285714285714285\n",
      "\n",
      "GT:    Magnetic resonance imaging ankle shows osteochondral lesion of the lateral talar dome measuring seven millimeters.\n",
      "Base:  Magnetic Resonance Imaging Ankle shows osteochondral lesion of the lateral Taylor Dome measuring 7 mm.\n",
      "Fused: Magnetic Resonance Imaging Ankle shows osteochondral lesion of the lateral talar dome measuring 7 mm.\n",
      "base err - fused err: 0.06666666666666667\n",
      "\n",
      "GT:    Magnetic resonance imaging shoulder shows tendinosis of the subscapularis tendon and moderate acromioclavicular joint osteoarthritis.\n",
      "Base:  magnetic resonance imaging shoulder-shows tendinosis of the subscapularis tendon and moderate acromioclavicular joint osteoarthritis.\n",
      "Fused: magnetic resonance imaging, shoulder-shoe tendinosis of the subscapularis tendon, and moderate acromioclavicular joint osteoarthritis.\n",
      "base err - fused err: -0.06666666666666667\n",
      "\n",
      "GT:    Right upper quadrant ultrasound shows gallbladder wall thickening and pericholecystic fluid with patent common bile duct.\n",
      "Base:  Right upper quadrant ultrasound shows gall bladder wall thickening and pericolocystic fluid with patent common bile duct.\n",
      "Fused: Right upper quadrant ultrasound shows gallbladder wall thickening and pericolicisstic fluid with patent common bile duct.\n",
      "base err - fused err: 0.125\n",
      "\n",
      "GT:    Right upper quadrant ultrasound demonstrates gallstones with posterior acoustic shadowing and a positive sonographic Murphy sign consistent with acute cholecystitis.\n",
      "Base:  Right Upper Quadrant ultrasound demonstrates gallstones with posterior acoustic shadowing and a positive sonographic Murphy sign consistent with acute colosceitis.\n",
      "Fused: Right Upper Quadrant ultrasound demonstrates gallstones with posterior acoustic shadowing and a positive sonographic Murphy sign consistent with acute cholecystitis.\n",
      "base err - fused err: 0.05\n",
      "\n",
      "GT:    Abdominal aortic ultrasound demonstrates a fusiform infrarenal abdominal aortic aneurysm measuring three point four centimeters without mural thrombus.\n",
      "Base:  abdominal aortic ultrasound demonstrates effusiform inferenal abdominal aortic aneurysm measuring 3.4 centimeters without mural frombus.\n",
      "Fused: abdominal aortic ultrasound demonstrates a fusiform infarrenal abdominal aortic aneurysm, measuring 3.4 cm without mural thrombus.\n",
      "base err - fused err: 0.16666666666666669\n",
      "\n",
      "GT:    Digital screening mammogram demonstrates scattered areas of fibroglandular density without suspicious mass, calcifications, or architectural distortion.\n",
      "Base:  Digital screening mammogram demonstrates scattered areas of fibroglandular density without suspicious mass, calcifications, or architectural distortion.\n",
      "Fused: Digital screening mammogram demonstrates scattered areas of fibro-glendular density, without suspicious mass, calcifications, or architectural distortion.\n",
      "base err - fused err: -0.125\n",
      "\n",
      "GT:    Plain radiographs of the left knee show moderate tricompartmental osteoarthritis with joint space narrowing and osteophyte formation.\n",
      "Base:  Plain radiographs of the left knee show moderate tri-compartmental osteoarthritis with joint space narrowing and osteophyte formation.\n",
      "Fused: Plain radiographs of the left knee show moderate tricompartmental osteoarthritis with joint space narrowing and osteophyte formation.\n",
      "base err - fused err: 0.11764705882352941\n",
      "\n",
      "GT:    Diagnostic mammogram with targeted ultrasound shows a spiculated mass in the upper outer quadrant of the left breast measuring two point one centimeters, suspicious for malignancy.\n",
      "Base:  The diagnostic mammogram with targeted ultrasound shows a speculated mass in the upper outer quadrant of the left breast measuring 2.1 cm, suspicious for malignancy.\n",
      "Fused: The diagnostic mammogram with targeted ultrasound shows a spiculated mass in the upper-outer quadrant of the left breast measuring 2.1 cm, suspicious for malignancy.\n",
      "base err - fused err: 0.038461538461538464\n",
      "\n",
      "GT:    Plain radiographs of the hand show a comminuted intraarticular distal radius fracture with dorsal angulation.\n",
      "Base:  Plain radiographs of the hand show a comminuted intraarticular distal radius fracture with dorsal angulation.\n",
      "Fused: Plain radiographs of the hand show a comminuted intra-articular distal radius fracture with dorsal angulation.\n",
      "base err - fused err: -0.13333333333333333\n",
      "\n",
      "GT:    Plain radiographs of the lumbar spine show grade one anterolisthesis of L four on L five with facet arthropathy.\n",
      "Base:  Plain radiographs of the lumbar spine show grade 1 anterior lys thesis of L4 on L5 with facet arthropathy.\n",
      "Fused: Plain radiographs of the lumbar spine show great one-anterolisthesis of L4 on L5 with facet arthropathy.\n",
      "base err - fused err: 0.10526315789473684\n",
      "\n",
      "GT:    Computed tomography maxillofacial shows acute left zygomaticomaxillary complex fracture with mild orbital emphysema but intact globe.\n",
      "Base:  The computed tomography maxillofacial shows acute left zygomaticomaxillary complex fracture with mild orbital emphysema but intact globe.\n",
      "Fused: The computed tomography maxillofacial shows a cute left zygomaticomaxillary complex fracture with mild orbital emphysema but intact globe.\n",
      "base err - fused err: -0.125\n",
      "\n",
      "GT:    Computed tomography abdomen and pelvis shows a heterogeneously enhancing hepatic lesion measuring three point two centimeters suspicious for hepatocellular carcinoma on a background of cirrhosis.\n",
      "Base:  Computed tomography abdomen and pelvis shows a heterogeneously enhancing hepatic lesion measuring 3.2 centimeters, suspicious for hepatocellular parsinoma, on a background of cirrhosis.\n",
      "Fused: Computed tomography abdomen and pelvis shows a heterogeneously enhancing hepatic lesion, measuring 3.2 cm, suspicious for hepatocellular carcinoma, on a background of cirrhosis.\n",
      "base err - fused err: 0.04000000000000001\n",
      "\n",
      "GT:    Computed tomography urogram demonstrates a filling defect in the right renal pelvis suspicious for urothelial carcinoma.\n",
      "Base:  The computed tomography urogram demonstrates a filling defect in the right renal pelvis, suspicious for urovelial carcinoma.\n",
      "Fused: The computed tomography urogram demonstrates a filling defect in the right renal pelvis, suspicious for urovilliocarcinoma.\n",
      "base err - fused err: -0.0625\n",
      "\n",
      "GT:    Magnetic resonance imaging brain demonstrates mild pachymeningeal enhancement consistent with intracranial hypotension in the appropriate context.\n",
      "Base:  Magnetic resonance imaging, BRAIN, demonstrates mild papumenangeal enhancement consistent with intracranial hypotension in the appropriate context.\n",
      "Fused: Magnetic resonance imaging, brain, demonstrates mild papule meningeal enhancement, consistent with intracranial hypotension in the appropriate context.\n",
      "base err - fused err: -0.0625\n",
      "\n",
      "GT:    Nuclear medicine hepatobiliary iminodiacetic acid scan demonstrates nonvisualization of the gallbladder at one hour consistent with acute cholecystitis.\n",
      "Base:  Nuclear medicine hepatobiliary iminodiacetic acid scan demonstrates non-visualization of the gallbladder at one hour, consistent with acute cholecystitis.\n",
      "Fused: Nuclear medicine hepatobiliary iminodiasetic acid scan demonstrates non-visualization of the gallbladder at one hour, consistent with acute cholecystitis.\n",
      "base err - fused err: -0.05555555555555555\n",
      "\n",
      "GT:    Ultrasound abdomen demonstrates no free intraperitoneal fluid and normal caliber common bile duct.\n",
      "Base:  Ultrasound Aptenin demonstrates no free intraperitoneal fluid and normal caliber common bile duct.\n",
      "Fused: Ultrasound abdomen demonstrates no free intraperitoneal fluid and normal caliber common bile duct.\n",
      "base err - fused err: 0.07692307692307693\n",
      "\n",
      "GT:    Magnetic resonance imaging orbit with contrast demonstrates enhancement and enlargement of the left optic nerve sheath consistent with optic perineuritis.\n",
      "Base:  Magnetic resonance imaging orbit, with contrast, demonstrates enhancement and enlargement of the left optic nerve sheath consistent with optic perineritis.\n",
      "Fused: Magnetic resonance imaging orbit, with contrast, demonstrates enhancement and enlargement of the left optic nerve sheath consistent with optic perineuritis.\n",
      "base err - fused err: 0.05\n",
      "\n",
      "GT:    Magnetic resonance imaging orbit shows proptosis and enlargement of the extraocular muscles sparing the tendinous insertions consistent with thyroid associated orbitopathy.\n",
      "Base:  Magnetic resonance imaging orbit shows propptosis and enlargement of the extraocular muscles sparing the tendinous insertions, consistent with thyroid-associated orbitopathy.\n",
      "Fused: Magnetic resonance imaging orbit shows proptosis and enlargement of the extraocular muscles, sparing the tendinous insertions, consistent with thyroid-associated orbitopathy.\n",
      "base err - fused err: 0.047619047619047616\n",
      "\n",
      "GT:    Computed tomography chest shows bronchial wall thickening and tree in bud nodularity in the right middle lobe suggestive of endobronchial infection.\n",
      "Base:  computed tomography chest-shays bronchial wall thickening and tree and bud nodularity in the right middle lobe suggestive of endobronchial infection.\n",
      "Fused: computed tomography chest-shoes bronchial wall thickening and tree-in-bud nodularity in the right middle lobe suggestive of endobronchial infection.\n",
      "base err - fused err: 0.047619047619047616\n",
      "\n",
      "GT:    Plain radiographs of the abdomen demonstrate a nonspecific bowel gas pattern without evidence of obstruction or free air.\n",
      "Base:  Plain radiographs of the abdomen demonstrate a nonspecific bowel gas pattern without evidence of obstruction or free air.\n",
      "Fused: Plain radiographs of the abdomen demonstrate a non-specific bowel gas pattern without evidence of obstruction or free air.\n",
      "base err - fused err: -0.1111111111111111\n",
      "\n",
      "GT:    Magnetic resonance imaging cardiac with contrast demonstrates mildly reduced left ventricular ejection fraction of forty five percent with mid myocardial late gadolinium enhancement in the inferolateral wall consistent with nonischemic cardiomyopathy.\n",
      "Base:  Magnetic resonance imaging cardiac with contrast demonstrates mildly reduced left ventricular ejection fraction of 45% with mid myocardial late gadolinium enhancement in the infralateral wall consistent with non ischemic cardiomyotathy.\n",
      "Fused: Magnetic resonance imaging cardiac with contrast demonstrates mildly reduced left ventricular ejection fraction of 45% with mid-myocardial late gadolinium enhancement in the infralateral wall, consistent with non-ischemic cardiomyopathy.\n",
      "base err - fused err: 0.033333333333333354\n",
      "\n",
      "GT:    Plain radiographs of the wrist show scapholunate interval widening suggestive of scapholunate ligament injury.\n",
      "Base:  Plain radiographs of the wrist show scaffolunate interval widening suggestive of scaffolunate ligament injury.\n",
      "Fused: Plain radiographs of the wrist show scapholunate interval widening suggestive of scapholunate ligament injury.\n",
      "base err - fused err: 0.14285714285714285\n",
      "\n",
      "GT:    Computed tomography chest demonstrates a loculated left pleural effusion with enhancing pleural rind consistent with empyema.\n",
      "Base:  A computed tomography chest demonstrates a loculated left pleural effusion with enhancing pleural rind, consistent with empyema.\n",
      "Fused: A computed tomography chest demonstrates a lachulated left pleural effusion with enhancing pleural rind, consistent with empyema.\n",
      "base err - fused err: -0.0625\n",
      "\n",
      "GT:    Magnetic resonance imaging spine whole shows diffuse marrow replacement compatible with metastatic disease, correlate with clinical history.\n",
      "Base:  Magnetic Resonance Imaging Spine, whole shows diffuse narrow replacement, compatible with metastatic disease. Correlate with clinical history.\n",
      "Fused: Magnetic Resonance Imaging Spine, whole-shoes diffuse-narrow replacement, compatible with metastatic disease. Correlate with clinical history.\n",
      "base err - fused err: -0.058823529411764705\n",
      "\n",
      "GT:    Magnetic resonance imaging brain demonstrates pachygyria polymicrogyria pattern in the right frontal lobe, correlate with seizures.\n",
      "Base:  Magnetic resonance imaging brain demonstrates pachigeria polymicrogeria pattern in the right frontal lobe, correlate with seizures.\n",
      "Fused: Magnetic resonance imaging brain demonstrates pachygyria-polymicrogyria pattern in the right frontal lobe, correlate with seizures.\n",
      "base err - fused err: 0.125\n",
      "\n",
      "GT:    Magnetic resonance imaging sacroiliac joints demonstrates subchondral bone marrow edema and erosions consistent with active sacroiliitis.\n",
      "Base:  Magnetic resonance imaging sacroiliac joints demonstrates subchondral bone marrow edema and erosions consistent with active sacroiliitis.\n",
      "Fused: Magnetic resonance imaging sacroiliac joints demonstrate subchondral bone marrow edema and erosions consistent with active sacroiliitis.\n",
      "base err - fused err: -0.0625\n",
      "\n",
      "GT:    Computed tomography kidneys ureters bladder demonstrates a seven millimeter calculus at the left ureteropelvic junction with mild hydronephrosis.\n",
      "Base:  Computed tomography kidneys' ureters' bladder demonstrates a 7mm calculus at the left ureter-pelvic junction with mild hydroinophrosis.\n",
      "Fused: Computed tomography kidneys, ureters, bladder, demonstrates a 7-millimeter calculus at the left ureter-pelvic junction with mild hydroinophrosis.\n",
      "base err - fused err: 0.11111111111111113\n",
      "\n",
      "GT:    Ultrasound soft tissue shows a complex fluid collection in the left thigh measuring six by three by two centimeters consistent with abscess.\n",
      "Base:  Ultrasound soft tissue shows a complex fluid collection in the left thigh measuring 6 by 3 by 2 centimeters consistent with abscess.\n",
      "Fused: Ultrasound soft tissue shows a complex fluid collection in the left thigh, measuring 6 x 3 x 2 cm, consistent with abscess.\n",
      "base err - fused err: -0.09090909090909091\n",
      "\n",
      "GT:    Magnetic resonance imaging ankle demonstrates a high grade tear of the anterior talofibular ligament with surrounding soft tissue edema.\n",
      "Base:  Magnetic Resonance Imaging Ankle demonstrates a high-grade tear of the anterior telefibular ligament with surrounding soft tissue edema.\n",
      "Fused: Magnetic Resonance Imaging Ankle demonstrates a high-grade tear of the anterior talofibular ligament with surrounding soft tissue edema.\n",
      "base err - fused err: 0.05263157894736842\n",
      "\n",
      "GT:    Magnetic resonance imaging wrist demonstrates avascular necrosis of the lunate consistent with Kienbock disease stage two.\n",
      "Base:  Magnetic Resonance Imaging Wrist demonstrates avascular necrosis of the lunate, consistent with Kienbach disease, stage two.\n",
      "Fused: Magnetic Resonance Imaging Wrist demonstrates avascular necrosis of the lunate, consistent with Kienbock disease, stage 2.\n",
      "base err - fused err: 0.0625\n",
      "\n",
      "GT:    Magnetic resonance imaging elbow shows osteochondritis dissecans of the capitellum with unstable osteochondral fragment.\n",
      "Base:  Magnetic Resonance Imaging Elbow shows osteochondritis desecchins of the capitellum with unstable osteochondral fragment.\n",
      "Fused: Magnetic Resonance Imaging Elbow shows osteochondritis dissecans of the capitellum with unstable osteochondral fragment.\n",
      "base err - fused err: 0.07142857142857142\n",
      "\n",
      "GT:    Computed tomography chest shows traction bronchiectasis and subpleural reticulations consistent with a usual interstitial pneumonia pattern of fibrosis.\n",
      "Base:  The computed tomography chest shows traction bronchianctasis and subplural reticulations consistent with the usual interstitial pneumonia pattern of fibrosis.\n",
      "Fused: The computed tomography chest shows traction bronchial decis and subplural reticulations consistent with the usual interstitial pneumonia pattern of fibrosis.\n",
      "base err - fused err: -0.05555555555555558\n",
      "\n",
      "GT:    Computed tomography abdomen demonstrates focal fat deposition adjacent to the falciform ligament, a benign variant.\n",
      "Base:  Computed tomography abdomen demonstrates folkly fat deposition adjacent to the falsiform ligament, a benign variant.\n",
      "Fused: Computed tomography abdomen demonstrates focal fat deposition adjacent to the falsiform ligament, a benign variant.\n",
      "base err - fused err: 0.06666666666666667\n",
      "\n",
      "GT:    Ultrasound pelvis demonstrates endometrial polypoid lesion with focal vascular stalk sign.\n",
      "Base:  Ultrasound Pelvis demonstrates endometrial polypoid lesion with focal vascular stock sign.\n",
      "Fused: Ultrasound pelvis demonstrates endometrial polypoid lesion with focal vascular stalksin.\n",
      "base err - fused err: -0.09090909090909091\n",
      "\n",
      "GT:    Plain radiographs of the chest show multiple lines and tubes in expected positions including a right chest tube with tip at the apex.\n",
      "Base:  Plain radiographs of the chest show multiple lines in tubes in expected positions, including a right chest tube with tip at the apex.\n",
      "Fused: Plain radiographs of the chest show multiple lines in tubes and expected positions, including a right chest tube with tip at the apex.\n",
      "base err - fused err: -0.043478260869565216\n",
      "\n",
      "GT:    Chest radiograph demonstrates clear lung fields bilaterally with no focal consolidation or pleural effusion.\n",
      "Base:  The chest radiograph demonstrates clear lung fields bilaterally, with no focal consolidation or plural effusion.\n",
      "Fused: The chest radiograph demonstrates clear lung fields bilaterally, with no focal consolidation or pleural effusion.\n",
      "base err - fused err: 0.07142857142857142\n",
      "\n",
      "GT:    MRI of the lumbar spine reveals mild degenerative disc disease at L4-L5 with minimal neural foraminal narrowing.\n",
      "Base:  MRI of the lumbar spine reveals mild degenerative disc disease at L4, L5 with minimal neural foraminal narrowing.\n",
      "Fused: MRI of the lumbar spine reveals mild degenerative disc disease at L4, L5, with minimal neural pherominal narrowing.\n",
      "base err - fused err: -0.05555555555555555\n",
      "\n",
      "GT:    Abdominal ultrasound shows cholelithiasis without evidence of acute cholecystitis.\n",
      "Base:  abdominal ultrasound shows cholella fiasis without evidence of acute polycystitis.\n",
      "Fused: abdominal ultrasound shows chollelophiasis without evidence of acute polycystitis.\n",
      "base err - fused err: 0.1111111111111111\n",
      "\n",
      "GT:    Lateral cervical spine radiograph shows loss of normal lordosis with multilevel degenerative changes.\n",
      "Base:  Lateral cervical spine radiograph shows loss of normal lordosis with multi-level degenerative changes.\n",
      "Fused: Lateral cervical spine radiograph shows loss of normal lordosis with multilevel degenerative changes.\n",
      "base err - fused err: 0.15384615384615385\n",
      "\n",
      "GT:    CT urogram demonstrates normal bilateral renal collecting systems without hydronephrosis.\n",
      "Base:  CT urogram demonstrates normal bilateral renal collecting systems without hydronephrosis.\n",
      "Fused: CT urogram demonstrates normal bilateral renal collecting systems without hydro-nephrosis.\n",
      "base err - fused err: -0.2\n",
      "\n",
      "GT:    Transvaginal ultrasound shows an anteverted uterus with normal endometrial thickness.\n",
      "Base:  Transvaginal ultrasound shows an anti-verted uterus with normal and demetrile thickness.\n",
      "Fused: Transvaginal ultrasound shows an anteverted uterus with normal endometrial thickness.\n",
      "base err - fused err: 0.4\n",
      "\n",
      "GT:    Portable chest radiograph shows proper positioning of the endotracheal tube and central venous catheter.\n",
      "Base:  Portable chest radiograph shows proper positioning of the endotracheal tube and centrovenous catheter.\n",
      "Fused: Portable chest radiograph shows proper positioning of the endotracheal tube and central venous catheter.\n",
      "base err - fused err: 0.14285714285714285\n",
      "\n",
      "GT:    Screening mammography shows scattered fibroglandular densities, BI-RADS category 2.\n",
      "Base:  Screening mammography shows scattered fibroglandular densities BI RADS category 2.\n",
      "Fused: Screening mammography shows scattered fibro-glangellar densities, BI-RADS category 2.\n",
      "base err - fused err: -0.2\n",
      "\n",
      "GT:    MRI of the prostate shows peripheral zone changes suspicious for malignancy, PI-RADS 4.\n",
      "Base:  MRI of the prostate shows peripheral zone changes suspicious for malignancy, PI-RADS-IV.\n",
      "Fused: MRI of the prostate shows peripheral zone changes suspicious for malignancy. PI-RADS-4\n",
      "base err - fused err: 0.07142857142857142\n",
      "\n",
      "GT:    Chest radiograph demonstrates bilateral hilar lymphadenopathy.\n",
      "Base:  chest radiograph demonstrates bilateral hylerlymphedinopathy.\n",
      "Fused: chest radiograph demonstrates bilateral hyalur lymphadenopathy.\n",
      "base err - fused err: 0.16666666666666666\n",
      "\n",
      "GT:    Head CT angiography shows a 3 millimeter anterior communicating artery aneurysm.\n",
      "Base:  Head CT angiography shows a 3mm anterior communicating artery aneurysm.\n",
      "Fused: Head CT angiography shows a 3-mm anterior communicating artery aneurysm.\n",
      "base err - fused err: 0.18181818181818182\n",
      "\n",
      "GT:    Abdominal ultrasound reveals splenomegaly measuring 15 centimeters in craniocaudal dimension.\n",
      "Base:  Abdominal ultrasound reveals splintomegaly measuring 15 centimeters in craniocottle dimension.\n",
      "Fused: Abdominal ultrasound reveals splintomegaly measuring 15 cm in craniocaudal dimension.\n",
      "base err - fused err: 0.1\n",
      "\n",
      "GT:    CT of the orbits reveals bilateral proptosis with increased retrobulbar fat.\n",
      "Base:  CT of the orbits reveals bilateral propptosis with increased retrobulbar fat.\n",
      "Fused: CT of the orbits reveals bilateral proptosis with increased retrobulbar fat.\n",
      "base err - fused err: 0.09090909090909091\n",
      "\n",
      "GT:    Cardiac MRI demonstrates hypertrophic cardiomyopathy with asymmetric septal hypertrophy.\n",
      "Base:  Cardiac MRI demonstrates hypertrophic cardiomyopathy with asymmetric septal hyperternity.\n",
      "Fused: Cardiac MRI demonstrates hypertrophic cardiomyopathy with asymmetric septal hypertrophy.\n",
      "base err - fused err: 0.1111111111111111\n",
      "\n",
      "GT:    Radiograph of the thoracolumbar spine reveals severe kyphosis with Cobb angle of 65 degrees.\n",
      "Base:  Radiograph of the thoracolumbar spine reveals severe kyphosis with cob angle of 65 degrees.\n",
      "Fused: Radiograph of the thoracolumbar spine reveals severe kyphosis with Cobb angle of 65 degrees.\n",
      "base err - fused err: 0.07142857142857142\n",
      "\n",
      "GT:    Chest radiograph demonstrates complete white-out of the left hemithorax.\n",
      "Base:  Chest radiograph demonstrates complete whiteout of the left hemithorax.\n",
      "Fused: Chest radiograph demonstrates complete white-out of the left hemithorax.\n",
      "base err - fused err: 0.2\n",
      "\n",
      "GT:    Ultrasound elastography of the liver shows increased stiffness consistent with cirrhosis.\n",
      "Base:  Ultrasound elastigraphy of the liver shows increased stiffness consistent with cirrhosis.\n",
      "Fused: Ultrasound elastography of the liver shows increased stiffness consistent with cirrhosis.\n",
      "base err - fused err: 0.09090909090909091\n",
      "\n",
      "GT:    CT urography reveals duplicated collecting system on the left with hydronephrosis.\n",
      "Base:  CT urography reveals duplicated collecting system on the left with hydronaphrosis.\n",
      "Fused: CT urography reveals duplicated collecting system on the left with hydro-naphrosis.\n",
      "base err - fused err: -0.09090909090909091\n",
      "\n",
      "GT:    CT angiography of the lower extremities shows occlusion of the right popliteal artery.\n",
      "Base:  CT angiography of the lowered extremities shows occlusion of the right popliteal artery.\n",
      "Fused: CT angiography of the lower extremities shows occlusion of the right popliteal artery.\n",
      "base err - fused err: 0.07692307692307693\n",
      "\n",
      "GT:    CT of the neck shows a large retropharyngeal abscess with mass effect.\n",
      "Base:  CT of the neck shows a large retrofarengeal adcess. Give mass effect.\n",
      "Fused: CT of the neck shows a large retrofaryngeal adhesus.\n",
      "base err - fused err: -0.16666666666666669\n",
      "\n",
      "GT:    Hepatobiliary scintigraphy demonstrates non-visualization of the gallbladder at 4 hours.\n",
      "Base:  Hepatobiliary syntigraphy demonstrates non-visualization of the gallbladder at four hours.\n",
      "Fused: Hepatobiliary scintigraphy demonstrates non-visualization of the gallbladder at four hours.\n",
      "base err - fused err: 0.09090909090909091\n",
      "\n",
      "GT:    Chest CT shows paraseptal emphysema predominantly in the upper lobes.\n",
      "Base:  Just see T shows pariceptal enthazema predominantly in the upper lobes.\n",
      "Fused: Just see T shows paraceptal emphysema, predominantly in the upper lobes.\n",
      "base err - fused err: 0.09999999999999998\n"
     ]
    }
   ],
   "source": [
    "print_str = '''\n",
    "GT:    {}\n",
    "Base:  {}\n",
    "Fused: {}\n",
    "base err - fused err: {}'''\n",
    "\n",
    "t = results_df[results_df['diff']!=0].copy()\n",
    "for idx, row in t.iterrows():\n",
    "    row_str = print_str.format(\n",
    "        row['reference'], \n",
    "        row['base'], \n",
    "        row['fused'],\n",
    "        row['wer_base'] - row['wer_fused']\n",
    "    )\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8691a39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Permutation test on corpus (micro) ΔWER ===\n",
      "Utterances used: 358\n",
      "Baseline WER: 6.90%\n",
      "Fused    WER: 6.45%\n",
      "ΔWER (baseline − fused): 0.46 points (~6.6% relative)\n",
      "Permutation p (two-sided): 0.0878\n",
      "Permutation p (one-sided, fusion better): 0.0444\n"
     ]
    }
   ],
   "source": [
    "def ref_len_after_transform(s: str) -> int:\n",
    "    out = transform(str(s))\n",
    "    toks = out[0] if isinstance(out, list) and out and isinstance(out[0], list) else (out if isinstance(out, list) else [])\n",
    "    return len(toks)\n",
    "\n",
    "N = results_df[\"reference\"].map(ref_len_after_transform).to_numpy(float)\n",
    "mask = N > 0\n",
    "N = N[mask]\n",
    "\n",
    "# per-utterance WERs (already computed with your compute_wer)\n",
    "WER_b = results_df.loc[mask, \"wer_base\"].to_numpy(float)\n",
    "WER_f = results_df.loc[mask, \"wer_fused\"].to_numpy(float)\n",
    "\n",
    "# convert to edit counts, compute observed corpus diff WER\n",
    "Eb, Ef = WER_b * N, WER_f * N\n",
    "Nsum = N.sum()\n",
    "micro_base  = Eb.sum() / Nsum\n",
    "micro_fused = Ef.sum() / Nsum\n",
    "delta_obs   = micro_base - micro_fused\n",
    "\n",
    "# permutation test (swap baseline↔fused within each utterance)\n",
    "T = 10_000\n",
    "rng = np.random.default_rng(0)\n",
    "two_ge = one_ge = 0\n",
    "for _ in range(T):\n",
    "    swap = rng.random(N.size) < 0.5\n",
    "    Eb_s = np.where(swap, Ef, Eb)\n",
    "    Ef_s = np.where(swap, Eb, Ef)\n",
    "    delta_t = (Eb_s.sum()/Nsum) - (Ef_s.sum()/Nsum)\n",
    "    if abs(delta_t) >= abs(delta_obs): two_ge += 1\n",
    "    if delta_t >= delta_obs:          one_ge += 1\n",
    "\n",
    "p_two = (two_ge + 1) / (T + 1)\n",
    "p_one = (one_ge + 1) / (T + 1)  # “fusion better?” one-sided\n",
    "\n",
    "print(\"=== Permutation test on corpus (micro) ΔWER ===\")\n",
    "print(f\"Utterances used: {N.size}\")\n",
    "print(f\"Baseline WER: {micro_base*100:.2f}%\")\n",
    "print(f\"Fused    WER: {micro_fused*100:.2f}%\")\n",
    "print(f\"ΔWER (baseline − fused): {delta_obs*100:.2f} points (~{(delta_obs/micro_base)*100:.1f}% relative)\")\n",
    "print(f\"Permutation p (two-sided): {p_two:.4f}\")\n",
    "print(f\"Permutation p (one-sided, fusion better): {p_one:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01c7fa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sign-flip permutation on mean per-clip ΔWER (macro) ===\n",
      "Clips used: 358\n",
      "Macro WER: baseline=6.72%  fused=6.16%  Δ=0.56 points\n",
      "Permutation p (two-sided): 0.0274\n",
      "Permutation p (one-sided, fusion better): 0.0139\n"
     ]
    }
   ],
   "source": [
    "d = WER_b - WER_f # per-clip differences (baseline − fused)\n",
    "macro_base  = WER_b.mean()\n",
    "macro_fused = WER_f.mean()\n",
    "macro_delta = macro_base - macro_fused  # observed mean difference\n",
    "T = 10_000\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "two_ge = 0; one_ge = 0\n",
    "for _ in range(T):\n",
    "    signs = np.where(rng.random(d.size) < 0.5, 1.0, -1.0)  # flip signs at random\n",
    "    t_star = float(np.mean(d * signs)) # permuted mean diff\n",
    "    if abs(t_star) >= abs(macro_delta): two_ge += 1\n",
    "    if t_star >= macro_delta: one_ge += 1\n",
    "\n",
    "p_two_macro = (two_ge + 1) / (T + 1)\n",
    "p_one_macro = (one_ge + 1) / (T + 1)\n",
    "\n",
    "print(\"=== Sign-flip permutation on mean per-clip ΔWER (macro) ===\")\n",
    "print(f\"Clips used: {d.size}\")\n",
    "print(f\"Macro WER: baseline={macro_base*100:.2f}%  fused={macro_fused*100:.2f}%  Δ={macro_delta*100:.2f} points\")\n",
    "print(f\"Permutation p (two-sided): {p_two_macro:.4f}\")\n",
    "print(f\"Permutation p (one-sided, fusion better): {p_one_macro:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9a145",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_small = [\n",
    "    {\"alpha\":0.03, 'base_macro': 0.08310380288544161, 'fused_macro': 0.07944915207681158, 'base': 0.07650273224043716, 'fused': 0.07377049180327869},\n",
    "    {\"alpha\":0.06, 'base_macro': 0.08310380288544161, 'fused_macro': 0.07568975358057098, 'base': 0.07650273224043716, 'fused': 0.0703551912568306},\n",
    "    {\"alpha\":0.09, 'base_macro': 0.08310380288544161, 'fused_macro': 0.07310977422040585, 'base': 0.07650273224043716, 'fused': 0.06830601092896176},\n",
    "    {\"alpha\":0.12, 'base_macro': 0.08310380288544161, 'fused_macro': 0.07412772685600555, 'base': 0.07650273224043716, 'fused': 0.07240437158469945},\n",
    "    {\"alpha\":0.15, 'base_macro': 0.08310380288544161, 'fused_macro': 0.07408447426084983, 'base': 0.07650273224043716, 'fused': 0.07240437158469945},\n",
    "    {\"alpha\":0.18, 'base_macro': 0.08310380288544161, 'fused_macro': 0.07526094484908513, 'base': 0.07650273224043716, 'fused': 0.07377049180327869},\n",
    "    {\"alpha\":0.21, 'base_macro': 0.08310380288544161, 'fused_macro': 0.0730615209425001, 'base': 0.07650273224043716, 'fused': 0.07240437158469945},\n",
    "    {\"alpha\":0.24, 'base_macro': 0.08310380288544161, 'fused_macro': 0.08214022982543406, 'base': 0.07650273224043716, 'fused': 0.08333333333333333},\n",
    "    {\"alpha\":0.27, 'base_macro': 0.08310380288544161, 'fused_macro': 0.08427926725858914, 'base': 0.07650273224043716, 'fused': 0.08469945355191257},\n",
    "    {\"alpha\":0.30, 'base_macro': 0.08310380288544161, 'fused_macro': 0.08712007024645095, 'base': 0.07650273224043716, 'fused': 0.08674863387978142},\n",
    "    ]\n",
    "\n",
    "out_medium = [\n",
    "    {\"alpha\":0.03, 'base_macro': 0.061775464546995826, 'fused_macro': 0.057041815842758886, 'base':0.056010928961748634,  'fused': 0.05259562841530055},\n",
    "    {\"alpha\":0.06, 'base_macro': 0.061775464546995826, 'fused_macro': 0.0561141585428561, 'base':0.056010928961748634, 'fused': 0.05259562841530055},\n",
    "    {\"alpha\":0.09, 'base_macro': 0.061775464546995826, 'fused_macro': 0.056157411138011815, 'base':0.056010928961748634, 'fused': 0.05259562841530055},\n",
    "    {\"alpha\":0.12, 'base_macro': 0.061775464546995826, 'fused_macro': 0.05263905642770664, 'base':0.056010928961748634, 'fused': 0.04918032786885246},\n",
    "    {\"alpha\":0.15, 'base_macro': 0.061775464546995826, 'fused_macro': 0.05394624597019029, 'base':0.056010928961748634, 'fused': 0.05259562841530055},\n",
    "    {\"alpha\":0.18, 'base_macro': 0.061775464546995826, 'fused_macro': 0.05468154008783735, 'base':0.056010928961748634, 'fused': 0.05327868852459016},\n",
    "    {\"alpha\":0.21, 'base_macro': 0.061775464546995826, 'fused_macro': 0.05361499225658364, 'base':0.056010928961748634, 'fused': 0.05327868852459016},\n",
    "    {\"alpha\":0.24, 'base_macro': 0.061775464546995826, 'fused_macro': 0.0543502863742307, 'base':0.056010928961748634, 'fused': 0.05396174863387978},\n",
    "    {\"alpha\":0.27, 'base_macro': 0.061775464546995826, 'fused_macro': 0.05541980509080823, 'base':0.056010928961748634, 'fused': 0.0546448087431694},\n",
    "    {\"alpha\":0.30, 'base_macro': 0.061775464546995826, 'fused_macro': 0.05755596390215886, 'base':0.056010928961748634, 'fused': 0.05669398907103825},\n",
    "    ]\n",
    "\n",
    "\n",
    "def make_lambda_table_wide(err_df: pd.DataFrame,\n",
    "                           caption_prefix=\"WER vs. λ\"):\n",
    "    df = err_df.copy().sort_values(\"alpha\").reset_index(drop=True)\n",
    "\n",
    "    # Derive a single baseline (and sanity-check it's constant)\n",
    "    base_vals = df[\"base\"].values\n",
    "    baseline = float(base_vals[0])\n",
    "    if not np.allclose(base_vals, baseline, rtol=0, atol=1e-9):\n",
    "        baseline = float(base_vals.mean())\n",
    "        caption = f\"{caption_prefix} Baseline WER ≈ {baseline:.4f} (mean across rows).\"\n",
    "    else:\n",
    "        caption = f\"{caption_prefix} Baseline WER = {baseline:.4f}.\"\n",
    "\n",
    "    # Relative reduction (%) with sign\n",
    "    rel_pct = ((df[\"base\"] - df[\"fused\"]) / df[\"base\"]) * 100.0\n",
    "    rel_str = rel_pct.map(lambda x: f\"{x:+.1f}\")\n",
    "\n",
    "    # Format λ headers\n",
    "    lambdas = df[\"alpha\"].map(lambda x: f\"{x:.2f}\").tolist()\n",
    "\n",
    "    # Build wide table\n",
    "    disp = pd.DataFrame(\n",
    "        [\n",
    "            df[\"fused\"].map(lambda x: f\"{x:.3f}\").tolist(),\n",
    "            rel_str.tolist()\n",
    "        ],\n",
    "        index=[\"**Fused WER**\", \"**Relative Δ (%)**\"],\n",
    "        columns=lambdas\n",
    "    )\n",
    "    disp.index.name = \"λ (Fusion Weight)\"\n",
    "\n",
    "    md = disp.to_markdown(tablefmt=\"github\")\n",
    "    return caption, md\n",
    "\n",
    "# Example usage:\n",
    "err_df = pd.DataFrame(out_small)\n",
    "caption, table_md = make_lambda_table_wide(err_df)\n",
    "print(f\"**Table. {caption}**\\n\")\n",
    "print(table_md)\n",
    "print(\"\\n> *Note: Variability across λ values likely reflects the small synthetic evaluation set.*\")\n",
    "# λ (Fusion Weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6f2316",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32e310",
   "metadata": {},
   "source": [
    "# BONEYARD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332dfbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, librosa, numpy as np, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "print(str(CACHE_DIR).split('/')[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3161254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    GPT2LMHeadModel, AutoTokenizer\n",
    ")\n",
    "\n",
    "SR = 16_000\n",
    "BATCH_SIZE = 5\n",
    "WHISPER_ID = \"openai/whisper-small.en\"\n",
    "GPT2_ID = \"cwestnedge/gpt2-small-pubmed\"\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\").resolve()\n",
    "MANIFEST = \"../data/output/manifest.jsonl\"\n",
    "AUDIO_DIR = \"../data/output\"  \n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# fast tokenizers will show token mismatch between models and will be auto loaded when we run on colab A100 set flag to false to avoid annoyingness\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "EOS_ID = gpt2_tok.eos_token_id # 50256 (unchanged)\n",
    "\n",
    "print(\"Decoder start token ID:\", whisper.generation_config.decoder_start_token_id)\n",
    "print(\"Decoder start token:\", processor.decode([whisper.generation_config.decoder_start_token_id]))\n",
    "\n",
    "PREFIX_TOK_IDS = [whisper.generation_config.decoder_start_token_id]\n",
    "for position, token_id in whisper.generation_config.forced_decoder_ids:\n",
    "    PREFIX_TOK_IDS.append(token_id)\n",
    "\n",
    "print(f\"\\nComplete prefix: {PREFIX_TOK_IDS}\")\n",
    "print(f\"Decoded: '{processor.decode(PREFIX_TOK_IDS)}'\")\n",
    "print(f\"Total prefix length: {len(PREFIX_TOK_IDS)}\")\n",
    "\n",
    "def build_dataset(manifest_path: str, batch_size: int, num_proc: int = 4) -> Dataset:\n",
    "    with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "\n",
    "    def add_audio(batch):\n",
    "        batch[\"audio\"] = [\n",
    "            librosa.load(f\"{AUDIO_DIR}/{fname}\", sr=SR, mono=True)[0].astype(np.float32)\n",
    "            for fname in batch[\"file\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return ds.map(add_audio, batched=True, batch_size=batch_size, num_proc=num_proc)\n",
    "\n",
    "def encode_audio(batch):\n",
    "    # batch[\"audio\"] is List[np.ndarray], each at its natural length\n",
    "    feats = processor.feature_extractor(\n",
    "        batch[\"audio\"], # for whatever reason processor doesnt support PT tensors so numpy array or list for now.\n",
    "        sampling_rate=SR,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=processor.feature_extractor.n_samples,  # n_samples = chunk_length * sampling_rate\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    #  input_features : Tensor (B, T_max, 80)\n",
    "    #  attention_mask : Tensor (B, T_max)\n",
    "    batch[\"input_features\"] = feats.input_features\n",
    "    batch[\"attention_mask\"] = feats.attention_mask\n",
    "    return batch\n",
    "\n",
    "ds = build_dataset(MANIFEST, batch_size=BATCH_SIZE).select([21,22,23,24,25])\n",
    "\n",
    "# choosing NOT to overwrite ds with removed fields so we can eval on text field later,\n",
    "# could also create a collator and pass fields we care about through, but seems like \n",
    "# too much extra code tbh, indices will still match if we dont shuffle\n",
    "ds_processed = ds.map(\n",
    "    encode_audio, \n",
    "    batch_size=2, \n",
    "    batched=True,\n",
    "    remove_columns=list(ds.features.keys())\n",
    "    )\n",
    "\n",
    "ds_processed.set_format(type=\"torch\", columns=[\"input_features\",\"attention_mask\"])\n",
    "loader = DataLoader(ds_processed, batch_size=2, shuffle=False)\n",
    "\n",
    "for i in range(len(gpt2_tok.get_vocab())):\n",
    "    a = processor.tokenizer.decode([i])\n",
    "    b = gpt2_tok.decode([i])\n",
    "    if a != b:\n",
    "        print(f\"Token mismatch at index {i}\\nwhisper token: {a}\\n   gpt2 token: {b} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0635234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Decoder start token ID:\", whisper.config.decoder_start_token_id)\n",
    "# print(\"BOS token ID:\", processor.tokenizer.bos_token_id)\n",
    "# print(\"Suppress tokens:\", whisper.config.suppress_tokens if hasattr(whisper.config, 'suppress_tokens') else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09978102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor, LogitsProcessorList\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "class HelloWorldLP(LogitsProcessor):\n",
    "    def __init__(self, alpha=0.0, warmup=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.step  = 0\n",
    "        self.warmup = warmup\n",
    "\n",
    "    def reset(self): self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        if self.step < self.warmup: \n",
    "            self.step+=1 \n",
    "            return w_lp\n",
    "        self.step+=1\n",
    "\n",
    "        return scores\n",
    "\n",
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, eos_id, alpha=0.25, warmup=3):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "        self.eos_id = eos_id # should be 50256\n",
    "        self.alpha = alpha \n",
    "        self.warmup = warmup\n",
    "        self.step = 0 \n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        if self.step < self.warmup: \n",
    "            self.step += 1 \n",
    "            return scores  # Return RAW SCORES, not w_lp!\n",
    "        \n",
    "        self.step += 1 \n",
    "\n",
    "        whisper_start_ids = [50257, 50362]\n",
    "        lm_input_ids = input_ids[:, len(whisper_start_ids):]\n",
    "\n",
    "        lm_logits = self.lm(\n",
    "            input_ids=lm_input_ids,\n",
    "        ).logits[:, -1, :]  # just want next token logits\n",
    "\n",
    "        # Convert to log probs\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "        # Create fusion mask: shape [batch_size, 1]\n",
    "        # 1.0 for sequences that DON'T want EOS, 0.0 for those that do\n",
    "        \n",
    "        # Apply fusion\n",
    "        fused = w_lp.clone()\n",
    "        fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        \n",
    "        return fused  # RETU\n",
    "\n",
    "class FixedShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, eos_id, prefix_tokens, alpha=0.25, warmup=2):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False)\n",
    "        self.eos_id = eos_id\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.alpha = alpha\n",
    "        self.warmup = warmup\n",
    "        self.step = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        \n",
    "        if self.step < self.warmup:\n",
    "            self.step += 1\n",
    "            return w_lp\n",
    "        \n",
    "        prefix_len = len(self.prefix_tokens)\n",
    "        \n",
    "        if input_ids.shape[1] <= prefix_len:\n",
    "            self.step += 1\n",
    "            return w_lp\n",
    "        \n",
    "        # CRITICAL FIX: Remove EOS tokens from sequence\n",
    "        clean_ids = input_ids[:, prefix_len:]\n",
    "        \n",
    "        # Remove EOS tokens - truncate at first EOS\n",
    "        for i in range(clean_ids.shape[0]):\n",
    "            seq = clean_ids[i]\n",
    "            eos_mask = seq == self.eos_id\n",
    "            if eos_mask.any():\n",
    "                first_eos = eos_mask.nonzero(as_tuple=True)[0][0].item()\n",
    "                clean_ids = clean_ids[:, :first_eos]\n",
    "                break\n",
    "        \n",
    "        if clean_ids.shape[1] == 0:\n",
    "            self.step += 1\n",
    "            return w_lp\n",
    "        \n",
    "        # Now GPT-2 gets clean input without EOS\n",
    "        lm_logits = self.lm(input_ids=clean_ids).logits[:, -1, :]\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "        \n",
    "        # Simple fusion\n",
    "        fused = w_lp.clone()\n",
    "        fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        \n",
    "        self.step += 1\n",
    "        return fused\n",
    "    \n",
    "    \n",
    "hw_proc = HelloWorldLP(warmup=2)\n",
    "\n",
    "fusion_proc = ShallowFusion(\n",
    "    lm=gpt2,\n",
    "    eos_id=50256,\n",
    "    alpha=0.15,\n",
    "    warmup=2\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    out1 = whisper.generate(\n",
    "        input_features=feats,\n",
    "        attention_mask=masks,\n",
    "        logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        return_timestamps=False,\n",
    "        num_beams=2,\n",
    "        # max_new_tokens= 10\n",
    "    )\n",
    "    fusion_proc.reset()\n",
    "    # out2 = whisper.generate(\n",
    "    #     input_features=feats,\n",
    "    #     attention_mask=masks,\n",
    "    #     return_dict_in_generate=True,\n",
    "    #     output_scores=True,\n",
    "    # )\n",
    "    \n",
    "# print((out1.scores[-1] != out2.scores[-1]).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [torch.tensor([[50257, 50362,  6930,   329],\n",
    "        [50257, 50362,  6930,    13],\n",
    "        [50257, 50362,   383,  7976],\n",
    "        [50257, 50362,   262,  7976],\n",
    "        [50257, 50362,  6952,   345],\n",
    "        [50257, 50362,  1318,   318],\n",
    "        [50257, 50362,  6952,   345],\n",
    "        [50257, 50362,  6952,   921],\n",
    "        [50257, 50362,   329,   262],\n",
    "        [50257, 50362,   329,   345]], device='mps:0'),\n",
    "torch.tensor([[50257, 50362,  6930,   329,  4964],\n",
    "        [50257, 50362,  6930,   329,   262],\n",
    "        [50257, 50362,   383,  7976,  2436],\n",
    "        [50257, 50362,   262,  7976,  2436],\n",
    "        [50257, 50362,  1318,   318,   257],\n",
    "        [50257, 50362,  6952,   345,    13],\n",
    "        [50257, 50362,  6952,   345,    13],\n",
    "        [50257, 50362,  6952,   345,   329],\n",
    "        [50257, 50362,   329,   345,    13],\n",
    "        [50257, 50362,   329,   262,  1306]], device='mps:0')]\n",
    "\n",
    "\n",
    "prefix_len = len(PREFIX_TOK_IDS)\n",
    "gpt2_ids = sequences[0][:,prefix_len:]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    lm_scores = gpt2(\n",
    "        input_ids = gpt2_ids\n",
    "    ).logits[:,-1,:]\n",
    "\n",
    "\n",
    "lm_lp = torch.log_softmax(lm_scores, dim=-1)\n",
    "lm_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154595f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # clean_ids = input_ids[:, prefix_len:]\n",
    "    # lm_logits = self.lm(input_ids=clean_ids).logits[:, -1, :]\n",
    "    # lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "    # # FUSION STEP \n",
    "    # # P_fused(y|x) = log P_ASR(y|x) + [HAS_EOS_MASK] * alpha * log P_LM(y)\n",
    "    # fused = w_lp.clone()\n",
    "    # if self.use_mask: \n",
    "    #     # this condition is to help exclude terminated sequences from getting revived by our lm\n",
    "    #     has_eos = (input_ids == self.eos_id).any(dim=1)\n",
    "    #     fusion_mask = (~has_eos).float().unsqueeze(-1) \n",
    "    #     fused[:, :self.eos_id] += fusion_mask*self.alpha * lm_lp[:, :self.eos_id]\n",
    "    # else: \n",
    "    #     fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "    \n",
    "    # print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66234358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what generation strategy was used\n",
    "print(f\"Do sample: {whisper.config.do_sample if hasattr(whisper.config, 'do_sample') else 'N/A'}\")\n",
    "print(f\"Temperature: {whisper.config.temperature if hasattr(whisper.config, 'temperature') else 'N/A'}\")\n",
    "print(f\"Num beams: {whisper.config.num_beams if hasattr(whisper.config, 'num_beams') else 'N/A'}\")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# Force greedy decoding (argmax) to match your analysis\n",
    "out1_greedy = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=False,  # Force greedy/argmax\n",
    "    num_beams=1,      # No beam search\n",
    "    return_timestamps=False\n",
    "    # temperature=1.0,  # No temperature scaling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b488d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_len = out1_greedy.sequences.shape[1] - len(out1_greedy.scores)\n",
    "print(f\"Prefix length: {prefix_len}\")\n",
    "\n",
    "t = 30\n",
    "input_ids_at_t = out1_greedy.sequences[:, :prefix_len + t].clone()\n",
    "scores_at_t = out1_greedy.scores[t]\n",
    "\n",
    "in_scope_ids = input_ids_at_t[:, 2:]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gpt2_scores = gpt2(\n",
    "        input_ids=in_scope_ids.to(DEVICE),\n",
    "    ).logits[:, -1, :]\n",
    "\n",
    "g_lp = torch.log_softmax(gpt2_scores, dim=-1)\n",
    "w_lp = torch.log_softmax(scores_at_t, dim=-1)\n",
    "\n",
    "# Apply fusion (using alpha=0.3 as in your code)\n",
    "alpha = 0.3\n",
    "\n",
    "fused = scores_at_t.clone()\n",
    "fused[:, :EOS_ID] +=  alpha * g_lp[:, :EOS_ID]\n",
    "\n",
    "# Get next tokens with different strategies\n",
    "next_token_fused = fused.argmax(dim=-1)\n",
    "next_token_raw = scores_at_t.argmax(dim=-1)\n",
    "next_token_gpt2 = g_lp.argmax(dim=-1)\n",
    "\n",
    "# Decode for comparison\n",
    "print(\"\\nToken choices:\")\n",
    "print(f\"Raw ASR:{processor.decode(next_token_raw[0].item())}\")\n",
    "print(f\"GPT-2  :{processor.decode(next_token_gpt2[0].item())}\")\n",
    "print(f\"Fusion :{processor.decode(next_token_fused[0].item())}\")\n",
    "\n",
    "# Show the actual sequences\n",
    "actual_next = out1_greedy.sequences[:, prefix_len + t]\n",
    "print(f\"Actual next token in sequence: {processor.decode(actual_next[0].item())}\")\n",
    "\n",
    "# Compare full sequences\n",
    "inputs_with_raw = torch.cat([input_ids_at_t, next_token_raw.unsqueeze(1)], dim=-1)\n",
    "inputs_with_fused = torch.cat([input_ids_at_t, next_token_fused.unsqueeze(1)], dim=-1)\n",
    "inputs_actual = out1_greedy.sequences[:, :prefix_len + t + 1]\n",
    "\n",
    "print(\"\\nFull sequences:\")\n",
    "print(f\"Context: {processor.batch_decode(input_ids_at_t)[0]}\")\n",
    "print(f\"With raw ASR: {processor.batch_decode(inputs_with_raw)[0]}\")\n",
    "print(f\"With fusion: {processor.batch_decode(inputs_with_fused)[0]}\")\n",
    "print(f\"Actual sequence: {processor.batch_decode(inputs_actual)[0]}\")\n",
    "\n",
    "print(f\"\\nNote: Step {t} was pure ASR during generation (no LogitsProcessor used)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROPER FUSION TESTING - During Active Generation (Not After EOS)\n",
    "\n",
    "def test_fusion_during_generation():\n",
    "    \"\"\"Test fusion at the right time - during active generation\"\"\"\n",
    "    \n",
    "    print(\"=== TESTING FUSION DURING ACTIVE GENERATION ===\")\n",
    "    \n",
    "    # Get a fresh generation\n",
    "    batch = next(iter(loader))\n",
    "    feats = batch['input_features'][:1].to(DEVICE)\n",
    "    masks = batch['attention_mask'][:1].to(DEVICE)\n",
    "    \n",
    "    # Generate with return_dict to get intermediate scores\n",
    "    with torch.no_grad():\n",
    "        result = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=20\n",
    "        )\n",
    "    \n",
    "    prefix_len = result.sequences.shape[1] - len(result.scores)\n",
    "    print(f\"Prefix length: {prefix_len}\")\n",
    "    print(f\"Total generation steps: {len(result.scores)}\")\n",
    "    \n",
    "    # Test fusion at different steps DURING generation\n",
    "    test_steps = [2, 5, 8, 10] if len(result.scores) > 10 else list(range(min(len(result.scores), 5)))\n",
    "    \n",
    "    for step in test_steps:\n",
    "        if step >= len(result.scores):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- TESTING STEP {step} (Active Generation) ---\")\n",
    "        \n",
    "        # Get the context at this step\n",
    "        input_ids_at_step = result.sequences[:, :prefix_len + step]\n",
    "        scores_at_step = result.scores[step]\n",
    "        \n",
    "        # Extract clean context for GPT-2\n",
    "        clean_context = input_ids_at_step[:, prefix_len:]\n",
    "        \n",
    "        print(f\"Context: '{processor.decode(clean_context[0])}'\")\n",
    "        print(f\"Contains EOS: {(clean_context == EOS_ID).any().item()}\")\n",
    "        \n",
    "        # Only test if no EOS in context (active generation)\n",
    "        if not (clean_context == EOS_ID).any():\n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                gpt2_logits = gpt2(clean_context).logits[:, -1, :]\n",
    "            \n",
    "            w_lp = torch.log_softmax(scores_at_step, dim=-1)\n",
    "            g_lp = torch.log_softmax(gpt2_logits, dim=-1)\n",
    "            \n",
    "            # Test different alpha values\n",
    "            alphas = [0.2, 0.5, 0.8]\n",
    "            \n",
    "            whisper_pred = w_lp.argmax(dim=-1)[0].item()\n",
    "            gpt2_pred = g_lp.argmax(dim=-1)[0].item()\n",
    "            \n",
    "            print(f\"  Whisper wants: '{processor.decode(whisper_pred)}'\")\n",
    "            print(f\"  GPT-2 wants: '{processor.decode(gpt2_pred)}'\")\n",
    "            \n",
    "            for alpha in alphas:\n",
    "                fused_scores = w_lp.clone()\n",
    "                fused_scores[:, :EOS_ID] += alpha * g_lp[:, :EOS_ID]\n",
    "                fused_pred = fused_scores.argmax(dim=-1)[0].item()\n",
    "                \n",
    "                if fused_pred != whisper_pred:\n",
    "                    print(f\"  α={alpha}: '{processor.decode(fused_pred)}' 🔄 CHANGED!\")\n",
    "                else:\n",
    "                    print(f\"  α={alpha}: '{processor.decode(fused_pred)}' ➡️ Same\")\n",
    "        else:\n",
    "            print(f\"  Skipping - sequence already ended\")\n",
    "\n",
    "def compare_fusion_effectiveness():\n",
    "    \"\"\"Compare vanilla vs fusion with proper alpha\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"=== COMPARING VANILLA VS FUSION (Proper Alpha) ===\")\n",
    "    \n",
    "    batch = next(iter(loader))\n",
    "    feats = batch['input_features'][:1].to(DEVICE)\n",
    "    masks = batch['attention_mask'][:1].to(DEVICE)\n",
    "    \n",
    "    # Vanilla generation\n",
    "    with torch.no_grad():\n",
    "        vanilla_result = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=30,\n",
    "            return_timestamps=False\n",
    "        )\n",
    "    \n",
    "    # Fusion with higher alpha\n",
    "    fusion_proc = ShallowFusion(\n",
    "        lm=gpt2,\n",
    "        eos_id=EOS_ID,\n",
    "        prefix_tokens=PREFIX_TOK_IDS,\n",
    "        alpha=0.7,  # Higher alpha to see effect\n",
    "        warmup=2\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fusion_result = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=30,\n",
    "            return_timestamps=False\n",
    "        )\n",
    "    \n",
    "    # Compare results\n",
    "    vanilla_text = processor.decode(vanilla_result[0], skip_special_tokens=True)\n",
    "    fusion_text = processor.decode(fusion_result[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Reference: {ds['text'][0]}\")\n",
    "    print(f\"Vanilla:   {vanilla_text}\")\n",
    "    print(f\"Fusion α=0.7: {fusion_text}\")\n",
    "    print(f\"Different: {vanilla_text != fusion_text}\")\n",
    "    \n",
    "    if vanilla_text != fusion_text:\n",
    "        print(\"✅ SUCCESS: Fusion with proper alpha shows differences!\")\n",
    "        \n",
    "        # Show character-level diff\n",
    "        from difflib import unified_diff\n",
    "        diff = list(unified_diff(\n",
    "            vanilla_text.split(), \n",
    "            fusion_text.split(),\n",
    "            fromfile='vanilla',\n",
    "            tofile='fusion',\n",
    "            lineterm=''\n",
    "        ))\n",
    "        if diff:\n",
    "            print(\"\\nWord-level differences:\")\n",
    "            for line in diff:\n",
    "                print(f\"  {line}\")\n",
    "    else:\n",
    "        print(\"⚠️ Still identical - try even higher alpha\")\n",
    "    \n",
    "    return vanilla_text, fusion_text\n",
    "\n",
    "# RUN THE PROPER TESTS\n",
    "test_fusion_during_generation()\n",
    "compare_fusion_effectiveness()\n",
    "\n",
    "print(\"\\n🎯 KEY INSIGHTS:\")\n",
    "print(\"1. Test fusion DURING generation, not after EOS\")\n",
    "print(\"2. GPT-2 predicting 'The' after EOS is normal - it doesn't understand stopping context\")\n",
    "print(\"3. Your fusion correctly chooses to end rather than continue inappropriately\")\n",
    "print(\"4. Use higher alpha (0.5-0.8) to see LM influence during active generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.encode('<|startoftranscript|><|notimestamps|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69053e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_greedy.sequences\n",
    "oob_mask = input_ids_at_t >= EOS_ID  # 50256\n",
    "filtered_ids = input_ids_at_t.masked_fill(oob_mask, PAD_ID)  # 50257\n",
    "attention_mask = (filtered_ids != PAD_ID).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what generation strategy was used\n",
    "print(f\"Do sample: {whisper.config.do_sample if hasattr(whisper.config, 'do_sample') else 'N/A'}\")\n",
    "print(f\"Temperature: {whisper.config.temperature if hasattr(whisper.config, 'temperature') else 'N/A'}\")\n",
    "print(f\"Num beams: {whisper.config.num_beams if hasattr(whisper.config, 'num_beams') else 'N/A'}\")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# Force greedy decoding (argmax) to match your analysis\n",
    "out1_greedy = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=False,  # Force greedy/argmax\n",
    "    num_beams=1,      # No beam search\n",
    ")\n",
    "\n",
    "# First, determine the prefix length\n",
    "prefix_len = out1_greedy.sequences.shape[1] - len(out1_greedy.scores)\n",
    "print(f\"Prefix length: {prefix_len}\")\n",
    "\n",
    "# Choose which step to analyze\n",
    "t = 10\n",
    "\n",
    "# Get the correct input sequence that was used to generate scores[t]\n",
    "input_ids_at_t = out1_greedy.sequences[:, :prefix_len + t].clone()\n",
    "scores_at_t = out1_greedy.scores[t]\n",
    "\n",
    "print(f\"Analyzing step {t}:\")\n",
    "print(f\"Input shape: {input_ids_at_t.shape}\")\n",
    "print(f\"This input was used to generate token at position {prefix_len + t}\")\n",
    "\n",
    "# NEW: Properly handle special tokens for GPT-2\n",
    "# Extract only valid GPT-2 tokens (< EOS_ID) for each sequence\n",
    "batch_size = input_ids_at_t.shape[0]\n",
    "gpt2_inputs = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Get only tokens that are valid for GPT-2 (< 50256)\n",
    "    valid_mask = input_ids_at_t[i] < EOS_ID\n",
    "    valid_tokens = input_ids_at_t[i][valid_mask]\n",
    "    gpt2_inputs.append(valid_tokens)\n",
    "\n",
    "# Pad sequences to same length for batching\n",
    "max_len = max(len(seq) for seq in gpt2_inputs)\n",
    "filtered_ids = torch.full((batch_size, max_len), PAD_ID, device=DEVICE)\n",
    "attention_mask = torch.zeros((batch_size, max_len), device=DEVICE)\n",
    "\n",
    "for i, seq in enumerate(gpt2_inputs):\n",
    "    filtered_ids[i, :len(seq)] = seq\n",
    "    attention_mask[i, :len(seq)] = 1\n",
    "\n",
    "print(f\"GPT-2 input (first sequence): {processor.decode(gpt2_inputs[0])}\")\n",
    "print(f\"Valid token IDs: {gpt2_inputs[0].tolist()}\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gpt2_scores = gpt2(\n",
    "        input_ids=filtered_ids.to(DEVICE),\n",
    "        attention_mask=attention_mask.to(DEVICE)\n",
    "    ).logits\n",
    "\n",
    "# Get log probabilities\n",
    "g_lp = torch.log_softmax(gpt2_scores[:, -1, :], dim=-1)\n",
    "w_lp = torch.log_softmax(scores_at_t, dim=-1)\n",
    "\n",
    "# Apply fusion\n",
    "alpha = 0.1\n",
    "fused = scores_at_t.clone()\n",
    "fused[:, :EOS_ID] += alpha * g_lp[:, :EOS_ID]\n",
    "\n",
    "# Get next tokens with different strategie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get next tokens with different strategies\n",
    "next_token_fused = fused.argmax(dim=-1)\n",
    "next_token_raw = scores_at_t.argmax(dim=-1)\n",
    "next_token_gpt2 = g_lp.argmax(dim=-1)\n",
    "\n",
    "# Decode for comparison\n",
    "print(\"\\nToken choices:\")\n",
    "print(f\"Raw ASR:{processor.decode(next_token_raw[0].item())}\")\n",
    "print(f\"GPT-2  :{processor.decode(next_token_gpt2[0].item())}\")\n",
    "print(f\"Fusion :{processor.decode(next_token_fused[0].item())}\")\n",
    "\n",
    "# Show the actual sequences\n",
    "actual_next = out1_greedy.sequences[:, prefix_len + t]\n",
    "print(f\"Actual next token in sequence: {processor.decode(actual_next[0].item())}\")\n",
    "\n",
    "# Compare full sequences\n",
    "inputs_with_raw = torch.cat([input_ids_at_t, next_token_raw.unsqueeze(1)], dim=-1)\n",
    "inputs_with_fused = torch.cat([input_ids_at_t, next_token_fused.unsqueeze(1)], dim=-1)\n",
    "inputs_actual = out1_greedy.sequences[:, :prefix_len + t + 1]\n",
    "\n",
    "print(\"\\nFull sequences:\")\n",
    "print(f\"Context: {processor.batch_decode(input_ids_at_t)[0]}\")\n",
    "print(f\"With raw ASR: {processor.batch_decode(inputs_with_raw)[0]}\")\n",
    "print(f\"With fusion: {processor.batch_decode(inputs_with_fused)[0]}\")\n",
    "print(f\"Actual sequence: {processor.batch_decode(inputs_actual)[0]}\")\n",
    "\n",
    "print(f\"\\nNote: Step {t} was pure ASR during generation (no LogitsProcessor used)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_token_id = next_token_gpt2[0].item()\n",
    "print(f\"GPT-2 predicted token ID: {gpt2_token_id}\")\n",
    "\n",
    "# Try different decode methods\n",
    "print(f\"Decode with processor: '{processor.decode(gpt2_token_id)}'\")\n",
    "print(f\"Decode with tokenizer: '{processor.tokenizer.decode([gpt2_token_id])}'\")\n",
    "print(f\"Token string: '{processor.tokenizer.convert_ids_to_tokens([gpt2_token_id])[0]}'\")\n",
    "\n",
    "# Check if it's a space or special character\n",
    "if gpt2_token_id < 50257:\n",
    "    token = processor.tokenizer.convert_ids_to_tokens([gpt2_token_id])[0]\n",
    "    print(f\"Token repr: {repr(token)}\")  # This will show \\n, \\t, spaces etc\n",
    "    print(f\"Token bytes: {token.encode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd81a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e89853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what GPT-2 actually received as input\n",
    "print(f\"Input to GPT-2: {processor.decode(filtered_ids[0])}\")\n",
    "print(f\"Raw token IDs: {filtered_ids[0].tolist()}\")\n",
    "\n",
    "# Let's manually test GPT-2 with clear medical context\n",
    "test_sequence = gpt2_tok.encode(\"The patient had pain in the abdomen and pel\", return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    test_output = gpt2(test_sequence).logits[0, -1, :]\n",
    "    test_probs = torch.softmax(test_output, dim=0)\n",
    "    top5 = test_probs.topk(5)\n",
    "    \n",
    "print(\"\\nGPT-2 predictions for 'abdomen and pel':\")\n",
    "for prob, idx in zip(top5.values, top5.indices):\n",
    "    if idx < 50257:\n",
    "        print(f\"  {processor.decode(idx)}: {prob:.3f}\")\n",
    "\n",
    "# Also check if the model weights look reasonable\n",
    "print(f\"\\nGPT-2 weight stats:\")\n",
    "print(f\"Mean: {gpt2.lm_head.weight.mean().item():.4f}\")\n",
    "print(f\"Std: {gpt2.lm_head.weight.std().item():.4f}\")\n",
    "\n",
    "# Test a few medical terms\n",
    "medical_tests = [\n",
    "    \"The patient's hep\",  # -> hepatic/hepatitis\n",
    "    \"The cardiac cath\",   # -> catheterization  \n",
    "    \"Diagnosed with pneum\" # -> pneumonia\n",
    "]\n",
    "\n",
    "for test in medical_tests:\n",
    "    tokens = processor.tokenizer.encode(test, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = gpt2(tokens).logits[0, -1, :]\n",
    "    next_token = processor.decode(out.argmax().item())\n",
    "    print(f\"\\n'{test}' -> '{next_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, pad_id, eos_id, alpha=0.25, warmup=3):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "        self.pad_id = pad_id # should be 50257 \n",
    "        self.eos_id = eos_id # should be 50256\n",
    "        self.alpha = alpha \n",
    "        self.warmup = warmup\n",
    "        self.step = 0 \n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "\n",
    "        if self.step < self.warmup: \n",
    "            self.step+=1 \n",
    "            return scores\n",
    "        self.step+=1 \n",
    "\n",
    "        oov_mask = input_ids >= self.eos_id # gpt2 and whispers EOS token\n",
    "        padded_input_ids = input_ids.masked_fill(oov_mask, self.pad_id) # PAD_ID\n",
    "        attention_mask = (padded_input_ids != self.pad_id).long()\n",
    "        \n",
    "        lm_logits = self.lm(\n",
    "            input_ids=padded_input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).logits[:,-1,:] # just want next token logits\n",
    "\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "        \n",
    "        fused = scores.clone()\n",
    "        fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        # optional normalization step\n",
    "        # fused -= torch.logsumexp(fused, dim=-1, keepdim=True)\n",
    "        return fused\n",
    "\n",
    "alpha=0.3\n",
    "warmup = 1\n",
    "fusion_proc = ShallowFusion(gpt2, PAD_ID, EOS_ID, alpha=alpha, warmup=warmup)\n",
    "\n",
    "out = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    logits_processor=[fusion_proc],\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "\n",
    "prefix_len = out.sequences.shape[1] - len(out.scores)  # Should be 2\n",
    "pure_asr_step0 = out.scores[0]  # This is pure ASR (no fusion)\n",
    "\n",
    "input_for_step0 = out.sequences[:, :prefix_len]\n",
    "\n",
    "oov = input_for_step0 >= EOS_ID\n",
    "gpt2_inp = input_for_step0.masked_fill(oov, PAD_ID)\n",
    "attn = (gpt2_inp != PAD_ID).long()\n",
    "\n",
    "lm_logits = gpt2(gpt2_inp, attention_mask=attn).logits[:, -1]\n",
    "lm_logp = torch.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "fused_manual = pure_asr_step0.clone()\n",
    "fused_manual[:, :EOS_ID] += alpha * lm_logp[:, :EOS_ID]\n",
    "\n",
    "# Now run AGAIN with warmup=0 to get actual fused scores at step 0\n",
    "fusion_proc2 = ShallowFusion(gpt2, PAD_ID, EOS_ID, alpha=alpha, warmup=0)\n",
    "out2 = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    logits_processor=[fusion_proc2],\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "\n",
    "# Compare!\n",
    "fused_actual = out2.scores[0]\n",
    "diff = (fused_actual - fused_manual).abs()\n",
    "valid_mask = ~torch.isnan(diff)\n",
    "print(f\"Max diff: {diff[valid_mask].max().item():.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "(out3.scores[t] != fused).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba41e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a986a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 5\n",
    "input_ids_at_t = out1.sequences[:,:t].clone()\n",
    "scores_at_t = out1.scores[t-1]\n",
    "texts = processor.batch_decode(\n",
    "    input_ids_at_t,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "# texts = [t.strip() for t in texts] # this doesnt seem to make a difference\n",
    "gpt2_inputs = gpt2_tok(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,           # pads to longest in batch\n",
    "    truncation=False,       # adjust as you like\n",
    ").input_ids.to(DEVICE)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gpt2_scores = gpt2(\n",
    "        input_ids = gpt2_inputs,\n",
    "        attention_mask = torch.ones_like(gpt2_inputs).to(DEVICE)\n",
    "    ).logits[:,-1]\n",
    "\n",
    "g_lp = torch.log_softmax(gpt2_scores, dim=-1)\n",
    "w_lp = torch.log_softmax(scores_at_t, dim=-1)\n",
    "\n",
    "fused = w_lp.clone()\n",
    "fused[:, :g_lp.size(1)] += 0 * g_lp\n",
    "next_token = fused.argmax(dim=-1).unsqueeze(1)\n",
    "next_token_raw = scores_at_t.argmax(dim=-1).unsqueeze(1)\n",
    "\n",
    "inputs_raw = torch.cat([input_ids_at_t, next_token_raw], dim=-1)\n",
    "inputs_fused = torch.cat([input_ids_at_t, next_token], dim=-1)\n",
    "# inputs_fused -= torch.logsumexp(inputs_fused, dim=-1, keepdim=True)\n",
    "\n",
    "a = processor.batch_decode(input_ids_at_t)\n",
    "b = processor.batch_decode(inputs_fused)\n",
    "c = processor.batch_decode(inputs_raw)\n",
    "d = processor.batch_decode(out1.sequences[:,:t+1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147a785",
   "metadata": {},
   "source": [
    "### ------------ TESTING END ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================\n",
    "# #  One-cell evaluation (uses your original jiwer transform)\n",
    "# #  -------------------------------------------------------------\n",
    "# #  Metrics per model:\n",
    "# #    • Global WER                --> same as your old script\n",
    "# #    • Medical-Term Recall (MTR) --> fraction of terms perfectly present\n",
    "# #    • Medical-Term-only WER     --> WER on tokens that belong to terms\n",
    "# #\n",
    "# #  Expects a DataFrame `results_df` with columns:\n",
    "# #        reference, vanilla, fused, medical_terms\n",
    "# #  where medical_terms is list[str]  (or a string repr like \"['a','b']\").\n",
    "# # =============================================================\n",
    "\n",
    "# import re, ast, itertools, pandas as pd\n",
    "# from jiwer import (\n",
    "#     Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces,\n",
    "#     Strip, ReduceToListOfListOfWords, wer\n",
    "# )\n",
    "# from unidecode import unidecode\n",
    "\n",
    "\n",
    "# # ---------- helper to handle both str & list[str] -------------------\n",
    "# def _map(func, x):\n",
    "#     return [func(t) for t in x] if isinstance(x, list) else func(x)\n",
    "\n",
    "# def remove_diacritics(x):\n",
    "#     return _map(unidecode, x)\n",
    "\n",
    "# def split_hyphens_and_slashes(x):\n",
    "#     return _map(lambda t: re.sub(r\"[-–—/]\", \" \", t), x)\n",
    "\n",
    "# def normalize_nums(x):\n",
    "#     return _map(lambda t: re.sub(r\"(\\d)[-–—-](\\d)\", r\"\\1-\\2\", t), x)\n",
    "\n",
    "# # ---------- your original jiwer transform ---------------------------\n",
    "# transform = Compose([\n",
    "#     ToLowerCase(),\n",
    "#     remove_diacritics,\n",
    "#     split_hyphens_and_slashes,\n",
    "#     normalize_nums,\n",
    "#     RemovePunctuation(),\n",
    "#     RemoveMultipleSpaces(),\n",
    "#     Strip(),\n",
    "#     ReduceToListOfListOfWords(),   # -> [[\"word\", ...], ...]\n",
    "# ])\n",
    "\n",
    "# def compute_wer(ref, hyp):\n",
    "#     return wer(\n",
    "#         ref, hyp,\n",
    "#         reference_transform=transform,\n",
    "#         hypothesis_transform=transform,\n",
    "#     )\n",
    "\n",
    "# # ---------- lightweight normaliser for term metrics -----------------\n",
    "# _punc_rx   = re.compile(r\"[^\\w\\s]\")\n",
    "# _range_rx  = re.compile(r\"(\\d)[-–—-](\\d)\")\n",
    "# _split_rx  = re.compile(r\"[-–—/]\")\n",
    "\n",
    "# def _normalise(text: str) -> str:\n",
    "#     text = unidecode(text.lower())\n",
    "#     text = _range_rx.sub(r\"\\1-\\2\", text)\n",
    "#     text = _split_rx.sub(\" \", text)\n",
    "#     text = _punc_rx.sub(\" \", text)\n",
    "#     return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# def _term_recall(row, hyp_text):\n",
    "#     hyp_norm = _normalise(hyp_text)\n",
    "#     hits = sum(1 for t in row[\"medical_terms\"] if _normalise(t) in hyp_norm)\n",
    "#     return hits / len(row[\"medical_terms\"])\n",
    "\n",
    "# def _extract_term_tokens(row, text):\n",
    "#     tokens = _normalise(text).split()\n",
    "#     keep   = [False] * len(tokens)\n",
    "#     for term in row[\"medical_terms\"]:\n",
    "#         ttoks = _normalise(term).split()\n",
    "#         for i in range(len(tokens) - len(ttoks) + 1):\n",
    "#             if tokens[i:i+len(ttoks)] == ttoks:\n",
    "#                 for j in range(i, i+len(ttoks)):\n",
    "#                     keep[j] = True\n",
    "#     return \" \".join(tok for tok, flag in zip(tokens, keep) if flag)\n",
    "\n",
    "# # ---------- main evaluation routine ---------------------------------\n",
    "# def evaluate(df: pd.DataFrame) -> None:\n",
    "#     # ensure list[str] in medical_terms\n",
    "#     if isinstance(df[\"medical_terms\"].iloc[0], str):\n",
    "#         df[\"medical_terms\"] = df[\"medical_terms\"].apply(ast.literal_eval)\n",
    "\n",
    "#     # Global WER (your existing metric)\n",
    "#     df[\"wer_vanilla\"] = df.apply(\n",
    "#         lambda r: compute_wer(r[\"reference\"], r[\"vanilla\"]), axis=1)\n",
    "#     df[\"wer_fused\"]   = df.apply(\n",
    "#         lambda r: compute_wer(r[\"reference\"], r[\"fused\"]), axis=1)\n",
    "\n",
    "#     # Medical-Term Recall\n",
    "#     df[\"mtr_vanilla\"] = df.apply(\n",
    "#         lambda r: _term_recall(r, r[\"vanilla\"]), axis=1)\n",
    "#     df[\"mtr_fused\"]   = df.apply(\n",
    "#         lambda r: _term_recall(r, r[\"fused\"]), axis=1)\n",
    "\n",
    "#     # Medical-Term-only WER\n",
    "#     df[\"mtwer_vanilla\"] = df.apply(\n",
    "#         lambda r: wer(\n",
    "#             _extract_term_tokens(r, r[\"reference\"]),\n",
    "#             _extract_term_tokens(r, r[\"vanilla\"]),\n",
    "#             reference_transform=transform,\n",
    "#             hypothesis_transform=transform), axis=1)\n",
    "#     df[\"mtwer_fused\"]   = df.apply(\n",
    "#         lambda r: wer(\n",
    "#             _extract_term_tokens(r, r[\"reference\"]),\n",
    "#             _extract_term_tokens(r, r[\"fused\"]),\n",
    "#             reference_transform=transform,\n",
    "#             hypothesis_transform=transform), axis=1)\n",
    "\n",
    "#     # -------- summary printout --------------------------------------\n",
    "#     print(\"\\n=== Global WER ===\")\n",
    "#     print(f\"  vanilla : {df['wer_vanilla'].mean():.4f}\")\n",
    "#     print(f\"  fused   : {df['wer_fused'].mean():.4f}\")\n",
    "\n",
    "#     print(\"\\n=== Medical-Term Recall ===\")\n",
    "#     print(f\"  vanilla : {df['mtr_vanilla'].mean():.4f}\")\n",
    "#     print(f\"  fused   : {df['mtr_fused'].mean():.4f}\")\n",
    "\n",
    "#     print(\"\\n=== Medical-Term-only WER ===\")\n",
    "#     print(f\"  vanilla : {df['mtwer_vanilla'].mean():.4f}\")\n",
    "#     print(f\"  fused   : {df['mtwer_fused'].mean():.4f}\")\n",
    "\n",
    "# # ---------- run on your DataFrame -----------------------------------\n",
    "\n",
    "# import pandas as pd \n",
    "\n",
    "# results_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"vanilla\":[i.strip() for i in vanilla], \n",
    "#         \"fused\":[i.strip() for i in fused], \n",
    "#         \"reference\":ds['text'],\n",
    "#         \"medical_terms\":ds['medical_terms']\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# evaluate(results_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LogitsProcessor, LogitsProcessorList\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ShallowFusion(LogitsProcessor):\n",
    "#     def __init__(self, lm, shared_vocab, eos, alpha=0.3, warmup_steps=3):\n",
    "#         super().__init__()\n",
    "#         self.lm = lm.eval().requires_grad_(False)\n",
    "#         self.V = shared_vocab\n",
    "#         self.eos = eos\n",
    "#         self.alpha = alpha\n",
    "#         self.warmup = warmup_steps\n",
    "#         self.step = 0\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         print('printing input_ids.size(), scores.size(), step, input_ids, dec_ids')\n",
    "#         print(input_ids.size(), scores.size(), self.step, input_ids, processor.batch_decode(input_ids))\n",
    "#         self.step+=1 \n",
    "\n",
    "#         return scores\n",
    "    \n",
    "# fusion_proc = ShallowFusion(\n",
    "#     lm=gpt2,\n",
    "#     shared_vocab=gpt2.config.vocab_size,\n",
    "#     eos=EOS_ID,\n",
    "#     alpha=0.3\n",
    "# )\n",
    "\n",
    "# batch = next(iter(loader))\n",
    "# feats = batch['input_features'].to(DEVICE)\n",
    "# masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     out1 = whisper.generate(\n",
    "#         input_features=feats,\n",
    "#         attention_mask=masks,\n",
    "#         logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "#         return_dict_in_generate=True,\n",
    "#         output_scores=True,\n",
    "#         num_beams=2,\n",
    "#     )\n",
    "\n",
    "#     # out2 = whisper.generate(\n",
    "#     #     input_features=feats,\n",
    "#     #     attention_mask=masks,\n",
    "#     #     return_dict_in_generate=True,\n",
    "#     #     output_scores=True,\n",
    "#     #     num_beams=2\n",
    "#     # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14230833",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# Generate to get sequences\n",
    "with torch.no_grad():\n",
    "    out = whisper.generate(\n",
    "        input_features=feats,\n",
    "        attention_mask=masks,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=5\n",
    "    )\n",
    "# Compare step 0\n",
    "decoder_ids = out.sequences[:,0:-1]  # Just the start token\n",
    "with torch.no_grad():\n",
    "    direct_logits = whisper(feats, decoder_input_ids=decoder_ids).logits[:, -1, :].to(DEVICE)\n",
    "    direct_lp = torch.log_softmax(direct_logits, dim=-1)\n",
    "\n",
    "gen_lp = out.scores[-1].to(DEVICE)\n",
    "\n",
    "print(gen_lp)\n",
    "print(direct_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d3e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd359960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter truly out of bounds vocab >=EOS\n",
    "oob_mask = decoder_ids > EOS_ID # create mask for gpt2 OOV tokens emitted by whisper\n",
    " # replace with gpt2 pad token\n",
    "filtered = decoder_ids.masked_fill(oob_mask, gpt2_tok.pad_token_id)\n",
    "attention_mask = (filtered != gpt2_tok.pad_token_id).long()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_new = gpt2(input_ids=filtered, attention_mask=attention_mask).logits[:,-1, :]\n",
    "\n",
    "# because we dont want gpt2 to impact or determine termination just ASR model\n",
    "logits_new[:,:EOS_ID-1].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
