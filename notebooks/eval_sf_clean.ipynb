{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7588133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SF_EVAL', '.models', 'hfcache']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, librosa, numpy as np, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\" / \"hfcache\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "print(str(CACHE_DIR).split('/')[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3be2e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    GPT2LMHeadModel, AutoTokenizer\n",
    ")\n",
    "\n",
    "SR = 16_000\n",
    "BATCH_SIZE = 5\n",
    "WHISPER_ID = \"openai/whisper-tiny.en\"\n",
    "GPT2_ID = \"cwestnedge/gpt2-medium-pubmed\"\n",
    "\n",
    "MANIFEST = \"../data/output/manifest.jsonl\"\n",
    "AUDIO_DIR = \"../data/output\"  \n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f72bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "processor = WhisperProcessor.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(GPT2_ID)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "if gpt2_tok.pad_token is None:\n",
    "    gpt2_tok.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "    gpt2.resize_token_embeddings(len(gpt2_tok))\n",
    "\n",
    "PAD_ID = gpt2_tok.pad_token_id # e.g. 50257\n",
    "EOS_ID = gpt2_tok.eos_token_id # 50256 (unchanged)\n",
    "SHARED_VOCAB = EOS_ID + 1\n",
    "\n",
    "print(PAD_ID, EOS_ID, SHARED_VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be085a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 30/30 [00:00<00:00, 37.71 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 46.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(manifest_path: str, batch_size: int, num_proc: int = 4) -> Dataset:\n",
    "    with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "\n",
    "    def add_audio(batch):\n",
    "        batch[\"audio\"] = [\n",
    "            librosa.load(f\"{AUDIO_DIR}/{fname}\", sr=SR, mono=True)[0].astype(np.float32)\n",
    "            for fname in batch[\"file\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return ds.map(add_audio, batched=True, batch_size=batch_size, num_proc=num_proc)\n",
    "\n",
    "def encode_audio(batch):\n",
    "    # batch[\"audio\"] is List[np.ndarray], each at its natural length\n",
    "    feats = processor.feature_extractor(\n",
    "        batch[\"audio\"], # for whatever reason processor doesnt support PT tensors so numpy array or list for now.\n",
    "        sampling_rate=SR,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=processor.feature_extractor.n_samples,  # n_samples = chunk_length * sampling_rate\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    #  input_features : Tensor (B, T_max, 80)\n",
    "    #  attention_mask : Tensor (B, T_max)\n",
    "    batch[\"input_features\"] = feats.input_features\n",
    "    batch[\"attention_mask\"] = feats.attention_mask\n",
    "    return batch\n",
    "\n",
    "ds = build_dataset(MANIFEST, batch_size=BATCH_SIZE)\n",
    "\n",
    "# choosing NOT to overwrite ds with removed fields so we can eval on text field later,\n",
    "# could also create a collator and pass fields we care about through, but seems like \n",
    "# too much extra code tbh, indices will still match if we dont shuffle\n",
    "ds_processed = ds.map(\n",
    "    encode_audio, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    batched=True,\n",
    "    remove_columns=['uuid', 'file', 'category', 'index', 'text', 'audio']\n",
    "    )\n",
    "\n",
    "ds_processed.set_format(type=\"torch\", columns=[\"input_features\",\"attention_mask\"])\n",
    "loader = DataLoader(ds_processed, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "\n",
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, shared_vocab, pad_id, alpha=0.3, warmup_steps=3, temperature=0.05):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False)\n",
    "        self.V = shared_vocab\n",
    "        self.pad_id = pad_id\n",
    "        self.alpha = alpha\n",
    "        self.warmup = warmup_steps\n",
    "        self.temp = temperature\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "\n",
    "        if self.step < self.warmup: \n",
    "            self.step+=1 \n",
    "            return scores\n",
    "        self.step+=1 \n",
    "\n",
    "        oob_mask = input_ids >= self.V\n",
    "        filtered_ids = input_ids.masked_fill(oob_mask, self.pad_id)\n",
    "        attn_mask = (filtered_ids != self.pad_id).long()\n",
    "\n",
    "        lm_logits = self.lm(\n",
    "            input_ids=filtered_ids, \n",
    "            attention_mask=attn_mask\n",
    "        ).logits[:,-1,:] # only want logits for next token\n",
    "\n",
    "        # lm_logits_shared = lm_logits[:, : self.V] # (B, 50257)\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)[:, : self.V] # (B, 50257)\n",
    "        w_lp = scores # whisper emits log probs already so no need to do log_softmax\n",
    "\n",
    "        fused_slice = w_lp[:, : self.V] + self.alpha * lm_lp\n",
    "        fused_lp = w_lp.clone()\n",
    "        fused_lp[:, : self.V] = fused_slice\n",
    "\n",
    "        # two things whisper emits log probs already\n",
    "        fused_lp -= torch.logsumexp(fused_lp, dim=-1, keepdim=True)\n",
    "        \n",
    "        # print(\"Normalized fused tensor range :\", fused_lp.min().item(), fused_lp.max().item())\n",
    "        # print(\"Whisper logits range          :\", scores.min().item(), scores.max().item())\n",
    "        # print(\"ELM logits range              :\", lm_logits.min().item(), lm_logits.max().item())\n",
    "        # print(\"ELM log_prob range            :\", lm_lp.min().item(), lm_lp.max().item())\n",
    "        # print()\n",
    "        return fused_lp #/ self.temp\n",
    "    \n",
    "fusion_proc = ShallowFusion(\n",
    "    lm=gpt2,\n",
    "    shared_vocab=SHARED_VOCAB, # now 50 258 after adding <|pad|>\n",
    "    pad_id=PAD_ID,# <— 50257\n",
    "    alpha=0.35,\n",
    "    warmup_steps=3,\n",
    "    temperature = 0.05\n",
    ")\n",
    "\n",
    "# sanity check\n",
    "with torch.no_grad():\n",
    "    dummy_ids = torch.tensor([[50256, 50257, 200, 345]], device=DEVICE)\n",
    "    dummy_lp  = torch.log_softmax(torch.randn(1, whisper.config.vocab_size, device=DEVICE), dim=-1)\n",
    "    out = fusion_proc(dummy_ids, dummy_lp)\n",
    "    print(\"∑exp =\", torch.exp(out).sum().item())      # 1.0 ± 1e‑4\n",
    "    print(\"range\",  out.min().item(), out.max().item())  # ≈ (‑20 … 0) after /temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 6/6 [00:23<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∑exp = 1.000000238418579\n",
      "range -28.52570915222168 -4.04352331161499\n"
     ]
    }
   ],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "from tqdm import tqdm \n",
    "\n",
    "fused = []\n",
    "refs = []\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        fused_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "            num_beams=3,\n",
    "            do_sample=False\n",
    "            # generation_config=gen_cfg,\n",
    "        )\n",
    "    decoded = processor.batch_decode(fused_ids, skip_special_tokens=True)\n",
    "    fused.extend(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ae516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∑exp = 1.000000238418579\n",
      "range -17.092540740966797 -5.121554374694824\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    dummy_ids = torch.tensor([[50256, 200, 345]], device=DEVICE)\n",
    "    dummy_lp  = torch.log_softmax(torch.randn(1, whisper.config.vocab_size, device=DEVICE), dim=-1)\n",
    "    out = fusion_proc(dummy_ids, dummy_lp)\n",
    "    print(\"∑exp =\", torch.exp(out).sum().item())      # 1.0 ± 1e‑4\n",
    "    print(\"range\",  out.min().item(), out.max().item())  # ≈ (‑20 … 0) after /temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f466239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 6/6 [00:07<00:00,  1.29s/it]\n"
     ]
    }
   ],
   "source": [
    "vanilla = []\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        vanilla_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            num_beams=1,\n",
    "            do_sample=False\n",
    "        )\n",
    "    decoded = processor.batch_decode(vanilla_ids, skip_special_tokens=True)\n",
    "    vanilla.extend(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c8610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GT:    The echocardiogram shows an ejection fraction of thirty-five percent with global hypokinesis.\n",
      "Base:  The echocardiogram shows an ejection fraction of 35% with global hypokinesis.\n",
      "Fused: The echocardiogram shows an ejection fraction of 35% with global hypokinesis.\n",
      "\n",
      "GT:    Post-operative pathology confirmed a stage two-A adenocarcinoma of the sigmoid colon.\n",
      "Base:  Postoperative pathology confirmed a stage 2a adenocarcinoma of the sigmoid colon.\n",
      "Fused: Post-operative pathology confirmed a stage 2a adenocarcinoma of the sigmoid colon.\n",
      "\n",
      "GT:    Her hemoglobin A-one-C has stabilized at seven point one percent after switching to semaglutide.\n",
      "Base:  Her hemoglobin A1c has stabilized at 7.1% after switching to semaglutide.\n",
      "Fused: Her hemoglobin A1c has stabilized at 7.1% after switching to semaglutide.\n",
      "\n",
      "GT:    Magnetic resonance imaging revealed a three-centimeter demyelinating plaque in the periventricular white matter.\n",
      "Base:  Magnetic resonance imaging revealed a 3 cm demyelinating plaque in the periventricular white matter.\n",
      "Fused: Magnetic resonance imaging revealed a 3-centimeter demyelinating plaque in the peri-ventricular white matter.\n",
      "\n",
      "GT:    We started ceftriaxone, two grams given intravenously every twenty-four hours, for suspected bacterial meningitis.\n",
      "Base:  We started ceftriaxone 2 grams given intravenously every 24 hours for suspected bacterial meningitis.\n",
      "Fused: We started ceftriaxone, 2 grams, given intravenously every 24 hours for suspected bacterial meningitis.\n",
      "\n",
      "GT:    His B-N-P is one thousand two hundred forty picograms per milliliter, consistent with decompensated heart failure.\n",
      "Base:  His BNP is 1240 picograms per milliliter, consistent with decompensated heart failure.\n",
      "Fused: His BNP is 1,240 picograms per milliliter, consistent with decompensated heart failure.\n",
      "\n",
      "GT:    She is allergic to fluoroquinolones and developed Stevens-Johnson syndrome after taking ciprofloxacin.\n",
      "Base:  She is allergic to fluoroquinolones and developed Stevens-Johnson syndrome after taking ciprofloxacin.\n",
      "Fused: She is allergic to fluoroquinolones and developed Stevens-Johnson syndrome after taking ciprofloxacin.\n",
      "\n",
      "GT:    Give zero point four milligrams of sublingual nitroglycerin as needed if chest pain is not relieved by rest.\n",
      "Base:  Give 0.4 milligrams of sublingual nitroglycerin as needed if chest pain is not relieved by rest.\n",
      "Fused: Give 0.4 milligrams of sublingual nitroglycerin as needed if chest pain is not relieved by rest.\n",
      "\n",
      "GT:    We documented a Mallampati class three airway before intubation.\n",
      "Base:  We documented a Malampati Class III airway before intubation.\n",
      "Fused: We documented a Malampati Class III airway before intubation.\n",
      "\n",
      "GT:    Colonoscopy identified a sessile serrated lesion in the transverse colon, removed with a cold snare.\n",
      "Base:  The colonoscopy identified a sessile serrated lesion in the transverse colon removed with a cold snare.\n",
      "Fused: The colonoscopy identified a sessile serrated lesion in the transverse colon, removed with a cold snare.\n",
      "\n",
      "GT:    The infant’s Apgar scores were eight and nine at one and five minutes, respectively.\n",
      "Base:  The Infants Apgar scores were 8 and 9 at 1 and 5 minutes, respectively.\n",
      "Fused: The Infant's Apgar scores were 8 and 9 at 1 and 5 minutes, respectively.\n",
      "\n",
      "GT:    Start methylprednisolone, one gram intravenously each day, for acute optic neuritis.\n",
      "Base:  Start methylprednisolone one gram intravenously each day for acute optic neuritis.\n",
      "Fused: Start methylprednisolone one gram intravenously each day for acute optic neuritis.\n",
      "\n",
      "GT:    Arterial blood gas shows a P-A-O-two of fifty-five millimeters of mercury on room air, indicating moderate hypoxemia.\n",
      "Base:  Arterial blood gas shows a PAO2 of 55 millimeters of mercury on room air, indicating moderate hypoxemia.\n",
      "Fused: Arterial blood gas shows a PAO2 of 55 mm of mercury on room air, indicating moderate hypoxemia.\n",
      "\n",
      "GT:    Current Procedural Terminology code nine three three zero six for transthoracic echocardiography was partially denied because modifiers were missing.\n",
      "Base:  The current procedural terminology code 93306 for trans-thoracic echocardiography was partially denied because modifiers were missing.\n",
      "Fused: The current procedural terminology code 9 3 3 0 6 for trans-thoracic echocardiography was partially denied because modifiers were missing.\n",
      "\n",
      "GT:    The member’s deductible was met, so the three-hundred-twenty-dollar coinsurance on H-C-P-C-S code J seventeen forty should be waived.\n",
      "Base:  The member's deductible was met, so the $320 coinsurance on HCPCS code J1740 should be waived.\n",
      "Fused: The members' deductible was met, so the $320 coinsurance on HCPCS code J1740 should be waived.\n",
      "\n",
      "GT:    The patient presents with intermittent angina and a positive troponin I of zero point three six nanograms per milliliter.\n",
      "Base:  The patient presents with intermittent angina and a positive troponin 1 of 0.36 ng per ml.\n",
      "Fused: The patient presents with intermittent angina and a positive troponin-1 of 0.36 nanograms per milliliter.\n",
      "\n",
      "GT:    International Classification of Diseases, tenth revision, code J forty-five point nine zero nine was flagged as unspecified asthma; a documentation update was requested.\n",
      "Base:  International Classification of Diseases 10th revision code J45.909 was flagged as unspecified asthma. A documentation update was requested.\n",
      "Fused: International Classification of Diseases, 10th revision, code J45.909, was flagged as unspecified asthma. A documentation update was requested.\n",
      "\n",
      "GT:    Doppler ultrasound detected a non-compressible femoral vein, consistent with deep venous thrombosis.\n",
      "Base:  Doppler ultrasound detected a non-compressible femoral vein consistent with deep venous thrombosis.\n",
      "Fused: Doppler ultrasound detected a non-compressible femoral vein, consistent with deep venous thrombosis.\n",
      "\n",
      "GT:    The pharmacy rejected the G-L-P-one authorization because the prior authorization expired on June thirtieth, twenty twenty-five.\n",
      "Base:  The pharmacy rejected the GLP-1 authorization because the prior authorization expired on June 30, 2025.\n",
      "Fused: The pharmacy rejected the GLP-1 authorization because the prior authorization expired on June 30, 2025.\n",
      "\n",
      "GT:    Diagnosis-related group three-thirty payment was reduced due to a coding discrepancy with a secondary diagnosis of hypokalemia.\n",
      "Base:  The diagnosis-related Group 330 payment was reduced due to a coding discrepancy with a secondary diagnosis of hypokalemia.\n",
      "Fused: The diagnosis-related group 330 payment was reduced due to a coding discrepancy with a secondary diagnosis of hypokalemia.\n",
      "\n",
      "GT:    The E-O-B shows a coordination-of-benefits adjustment after the Medicare crossover.\n",
      "Base:  The EOB shows a coordination of benefits adjustment after the Medicare crossover.\n",
      "Fused: The EOB shows a coordination of benefits adjustment after the Medicare crossover.\n",
      "\n",
      "GT:    We need operative notes to support Current Procedural Terminology code two nine eight eight one for arthroscopic medial meniscectomy.\n",
      "Base:  We need operative notes to support current procedural terminology code 29881 for arthroscopic medial meniscectomy.\n",
      "Fused: We need operative notes to support current procedural terminology code 2-9-8-8-1 for arthroscopic medial meniscectomy.\n",
      "\n",
      "GT:    Modifier twenty-five was omitted on the evaluation and management service, causing bundling with the injection.\n",
      "Base:  Modifier 25 was omitted on the Evaluation and Management Service, causing bundling with the injection.\n",
      "Fused: Modifier-25 was omitted on the evaluation and management service, causing bundling with the injection.\n",
      "\n",
      "GT:    Denial code C-O one ninety-seven cites missing pre-certification for the lumbar laminectomy.\n",
      "Base:  Denial code CO197 cites missing pre-certification for the lumbar laminectomy.\n",
      "Fused: Denial code CO-197, sites missing pre-certification for the lumbar laminectomy.\n",
      "\n",
      "GT:    The appeals team requests a radiology report to validate Current Procedural Terminology code seven four one seven seven for computed tomography of the abdomen and pelvis with contrast.\n",
      "Base:  The appeals team requests a radiology report to validate current procedural terminology code 74177 for computed tomography of the abdomen and pelvis with contrast.\n",
      "Fused: The appeals team requests a radiology report to validate current procedural terminology, code 74177, for computed tomography of the abdomen and pelvis with contrast.\n",
      "\n",
      "GT:    An out-of-network penalty applied because N-P-I one two one five nine eight three seven four six lacks a single-case agreement.\n",
      "Base:  an out-of-network penalty applied because NPI 1215983746 lacks a single case agreement.\n",
      "Fused: and out-of-network penalty, applied because NPI-1215983746 lacks a single-case agreement.\n",
      "\n",
      "GT:    The ambulance claim billed A zero four two eight, but mileage code A zero four two five was missing, triggering partial payment.\n",
      "Base:  The ambulance claimed build A0428, but mileage code A0425 was missing, triggering partial payment.\n",
      "Fused: The ambulance claimed build A0428, but mileage code A0425 was missing, triggering partial payment.\n",
      "\n",
      "GT:    The prosthetic hip device falls under H-C-P-C-S code L eight six nine nine, which is not covered without a K modifier.\n",
      "Base:  The prosthetic hip device falls under HCPCS code L8699, which is not covered without a K modifier.\n",
      "Fused: The Prosthetic H.I.P.P. device falls under H.C.P.C.S. code L-8-6-9-9, which is not covered without a K-modifier.\n",
      "\n",
      "GT:    The high-cost threshold was exceeded; specialty medication coded J three four nine zero requires National Drug Code submission.\n",
      "Base:  The high-cost threshold was exceeded. Specialty medication coded J3490 requires national drug code submission.\n",
      "Fused: The high-cost threshold was exceeded. Specialty medication-coated J3490 requires national drug code submission.\n",
      "\n",
      "GT:    Claim edits routed Current Procedural Terminology nine six three seven two to incidental when billed with nine nine two one four on the same date of service.\n",
      "Base:  Claim edits routed current procedural terminology 96372 to incidental when billed with 99214 on the same date of service.\n",
      "Fused: Claim Edits Routed, Current Procedural Terminology 96372 to Incidental, when Billed with 99214, on the same date of service.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "results_df = pd.DataFrame({'vanilla':vanilla, 'fused':fused, 'gt':ds['text']})\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    row_str = f\"GT:    {row['gt']}\\nBase:  {row['vanilla'].strip()}\\nFused: {row['fused'].strip()}\"\n",
    "    print(row_str)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32e310",
   "metadata": {},
   "source": [
    "# BONEYARD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LogitsProcessor, LogitsProcessorList\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ShallowFusion(LogitsProcessor):\n",
    "#     def __init__(self, lm, shared_vocab, eos, alpha=0.3, warmup_steps=3):\n",
    "#         super().__init__()\n",
    "#         self.lm = lm.eval().requires_grad_(False)\n",
    "#         self.V = shared_vocab\n",
    "#         self.eos = eos\n",
    "#         self.alpha = alpha\n",
    "#         self.warmup = warmup_steps\n",
    "#         self.step = 0\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         print('printing input_ids.size(), scores.size(), step, input_ids, dec_ids')\n",
    "#         print(input_ids.size(), scores.size(), self.step, input_ids, processor.batch_decode(input_ids))\n",
    "#         self.step+=1 \n",
    "\n",
    "#         return scores\n",
    "    \n",
    "# fusion_proc = ShallowFusion(\n",
    "#     lm=gpt2,\n",
    "#     shared_vocab=gpt2.config.vocab_size,\n",
    "#     eos=EOS_ID,\n",
    "#     alpha=0.3\n",
    "# )\n",
    "\n",
    "# batch = next(iter(loader))\n",
    "# feats = batch['input_features'].to(DEVICE)\n",
    "# masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     out1 = whisper.generate(\n",
    "#         input_features=feats,\n",
    "#         attention_mask=masks,\n",
    "#         logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "#         return_dict_in_generate=True,\n",
    "#         output_scores=True,\n",
    "#         num_beams=2,\n",
    "#     )\n",
    "\n",
    "#     # out2 = whisper.generate(\n",
    "#     #     input_features=feats,\n",
    "#     #     attention_mask=masks,\n",
    "#     #     return_dict_in_generate=True,\n",
    "#     #     output_scores=True,\n",
    "#     #     num_beams=2\n",
    "#     # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = [\n",
    "# torch.tensor([[50257, 50362],\n",
    "#         [50257, 50362],\n",
    "#         [50257, 50362]], device='mps:0'),\n",
    "# torch.tensor([[50257, 50362,   383],\n",
    "#         [50257, 50362,  2947],\n",
    "#         [50257, 50362,  2332]], device='mps:0'),\n",
    "# torch.tensor([[50257, 50362,   383,   304],\n",
    "#         [50257, 50362,  2947,    12],\n",
    "#         [50257, 50362,  2332, 16869]], device='mps:0'),\n",
    "# torch.tensor([[50257, 50362,   383,   304,   354],\n",
    "#         [50257, 50362,  2947,    12, 27173],\n",
    "#         [50257, 50362,  2332, 16869, 49835]], device='mps:0')\n",
    "# ]\n",
    "\n",
    "# idx = 3\n",
    "# with torch.no_grad():\n",
    "#         dec_ids = steps[idx]\n",
    "#         logits_a = whisper(feats, decoder_input_ids=dec_ids).logits[:, -1, :] \n",
    "#         out = whisper.generate(\n",
    "#                 input_features=feats,\n",
    "#                 attention_mask=masks,\n",
    "#                 num_beams=1,\n",
    "#                 do_sample=False,\n",
    "#                 return_dict_in_generate=True,\n",
    "#                 output_scores=True,\n",
    "#                 max_new_tokens=len(steps)\n",
    "#                 )\n",
    "#         logits_b = out.scores[idx]\n",
    "\n",
    "# print(logits_a)\n",
    "# print(logits_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd359960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oob_mask = dec_ids >= EOS_ID # create mask for gpt2 OOV tokens emitted by whisper\n",
    "# pad_token = gpt2_tok.eos_token_id # replace with gpt2 pad token\n",
    "# filtered = dec_ids.masked_fill(oob_mask, pad_token)\n",
    "# attention_mask = (filtered != pad_token).long()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     logits_new = gpt2(input_ids=filtered, attention_mask=attention_mask).logits[:,-1, :]\n",
    "\n",
    "# logits_new.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
