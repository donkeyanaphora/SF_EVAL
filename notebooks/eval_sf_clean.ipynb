{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7588133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SHALLOW_FUSION_EVAL', 'SF_EVAL', '.models']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, librosa, numpy as np, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(42)\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "print(str(CACHE_DIR).split('/')[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be2e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinswestnedge/Desktop/programming/SHALLOW_FUSION_EVAL/SF_EVAL/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    GPT2LMHeadModel, AutoTokenizer\n",
    ")\n",
    "\n",
    "SR = 16_000\n",
    "BATCH_SIZE = 5\n",
    "WHISPER_ID = \"openai/whisper-small.en\"\n",
    "GPT2_ID = \"cwestnedge/gpt2-small-pubmed\"\n",
    "\n",
    "MANIFEST = \"../data/output/manifest.jsonl\"\n",
    "AUDIO_DIR = \"../data/output\"  \n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f72bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder start token ID: 50257\n",
      "Decoder start token: <|startoftranscript|>\n",
      "\n",
      "Complete prefix: [50257, 50362]\n",
      "Decoded: '<|startoftranscript|><|notimestamps|>'\n",
      "Total prefix length: 2\n"
     ]
    }
   ],
   "source": [
    "# fast tokenizers will show token mismatch between models and will be auto loaded when we run on colab A100 set flag to false to avoid annoyingness\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "EOS_ID = gpt2_tok.eos_token_id # 50256 (unchanged)\n",
    "\n",
    "print(\"Decoder start token ID:\", whisper.generation_config.decoder_start_token_id)\n",
    "print(\"Decoder start token:\", processor.decode([whisper.generation_config.decoder_start_token_id]))\n",
    "\n",
    "PREFIX_TOK_IDS = [whisper.generation_config.decoder_start_token_id]\n",
    "for position, token_id in whisper.generation_config.forced_decoder_ids:\n",
    "    PREFIX_TOK_IDS.append(token_id)\n",
    "\n",
    "print(f\"\\nComplete prefix: {PREFIX_TOK_IDS}\")\n",
    "print(f\"Decoded: '{processor.decode(PREFIX_TOK_IDS)}'\")\n",
    "print(f\"Total prefix length: {len(PREFIX_TOK_IDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8c5dd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gpt2_tok.get_vocab())):\n",
    "    a = processor.tokenizer.decode([i])\n",
    "    b = gpt2_tok.decode([i])\n",
    "    if a != b:\n",
    "        print(f\"Token mismatch at index {i}\\nwhisper token: [{a}]\\n   gpt2 token: [{b}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be085a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 85/85 [00:03<00:00, 23.25 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:02<00:00, 38.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(manifest_path: str, batch_size: int, num_proc: int = 4) -> Dataset:\n",
    "    with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "\n",
    "    def add_audio(batch):\n",
    "        batch[\"audio\"] = [\n",
    "            librosa.load(f\"{AUDIO_DIR}/{fname}\", sr=SR, mono=True)[0].astype(np.float32)\n",
    "            for fname in batch[\"file\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return ds.map(add_audio, batched=True, batch_size=batch_size, num_proc=num_proc)\n",
    "\n",
    "def encode_audio(batch):\n",
    "    # batch[\"audio\"] is List[np.ndarray], each at its natural length\n",
    "    feats = processor.feature_extractor(\n",
    "        batch[\"audio\"], # for whatever reason processor doesnt support PT tensors so numpy array or list for now.\n",
    "        sampling_rate=SR,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=processor.feature_extractor.n_samples,  # n_samples = chunk_length * sampling_rate\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    #  input_features : Tensor (B, T_max, 80)\n",
    "    #  attention_mask : Tensor (B, T_max)\n",
    "    batch[\"input_features\"] = feats.input_features\n",
    "    batch[\"attention_mask\"] = feats.attention_mask\n",
    "    return batch\n",
    "\n",
    "ds = build_dataset(MANIFEST, batch_size=BATCH_SIZE) #.select(range(20)) #.select(range(10))\n",
    "\n",
    "# choosing NOT to overwrite ds with removed fields so we can eval on text field later,\n",
    "# could also create a collator and pass fields we care about through, but seems like \n",
    "# too much extra code tbh, indices will still match if we dont shuffle\n",
    "ds_processed = ds.map(\n",
    "    encode_audio, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    batched=True,\n",
    "    remove_columns=list(ds.features.keys())\n",
    "    )\n",
    "\n",
    "ds_processed.set_format(type=\"torch\", columns=[\"input_features\",\"attention_mask\"])\n",
    "loader = DataLoader(ds_processed, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19058265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0fc3a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "\n",
    "## this works\n",
    "# class ShallowFusion(LogitsProcessor):\n",
    "#     def __init__(self, lm, pad_id, eos_id, alpha=0.25, warmup=3):\n",
    "#         super().__init__()\n",
    "#         self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "#         self.pad_id = pad_id  # 50257 \n",
    "#         self.eos_id = eos_id  # 50256\n",
    "#         self.alpha = alpha \n",
    "#         self.warmup = warmup\n",
    "#         self.step = 0 \n",
    "    \n",
    "#     def reset(self):\n",
    "#         self.step = 0\n",
    "\n",
    "#     @torch.inference_mode()\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         if self.step < self.warmup: \n",
    "#             self.step += 1 \n",
    "#             return scores\n",
    "#         self.step += 1 \n",
    "\n",
    "#         # Find where valid tokens start (first token < eos_id)\n",
    "#         valid_mask = input_ids < self.eos_id\n",
    "        \n",
    "#         # Find the first valid token position for each sequence\n",
    "#         # This skips special tokens at the beginning\n",
    "#         first_valid = valid_mask.long().argmax(dim=1, keepdim=True)\n",
    "        \n",
    "#         # Create indices for gathering\n",
    "#         batch_size, seq_len = input_ids.shape\n",
    "#         batch_indices = torch.arange(batch_size, device=input_ids.device).unsqueeze(1)\n",
    "        \n",
    "#         # Gather only the valid portion of each sequence\n",
    "#         gather_indices = first_valid + torch.arange(seq_len - first_valid.max(), \n",
    "#                                                     device=input_ids.device).unsqueeze(0)\n",
    "#         gather_indices = gather_indices.clamp(max=seq_len-1)\n",
    "        \n",
    "#         # Extract subsequences starting from first valid token\n",
    "#         clean_ids = input_ids.gather(1, gather_indices)\n",
    "#         clean_mask = valid_mask.gather(1, gather_indices)\n",
    "        \n",
    "#         # Apply your original logic on clean sequences\n",
    "#         clean_ids[~clean_mask] = self.pad_id\n",
    "#         attention_mask = clean_mask.long()\n",
    "        \n",
    "#         lm_logits = self.lm(\n",
    "#             input_ids=clean_ids,\n",
    "#             attention_mask=attention_mask\n",
    "#         ).logits[:, -1, :]\n",
    "\n",
    "#         lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "        \n",
    "#         fused = scores.clone()\n",
    "#         fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        \n",
    "#         return fused\n",
    "\n",
    "# class ShallowFusion(LogitsProcessor):\n",
    "#     def __init__(self, lm, eos_id, prefix_tokens, alpha=0.25, warmup=3):\n",
    "#         super().__init__()\n",
    "#         self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "#         self.eos_id = eos_id\n",
    "#         self.prefix_tokens = prefix_tokens  # <|startoftranscript|><|notimestamps|>\n",
    "#         self.alpha = alpha \n",
    "#         self.warmup = warmup\n",
    "#         self.step = 0 \n",
    "    \n",
    "#     def reset(self):\n",
    "#         self.step = 0\n",
    "\n",
    "#     @torch.inference_mode()\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         w_lp = torch.log_softmax(scores, dim=-1)\n",
    "#         if self.step < self.warmup: \n",
    "#             self.step += 1 \n",
    "#             return w_lp\n",
    "\n",
    "#         prefix_len = len(self.prefix_tokens)\n",
    "#         prefix_ids = torch.tensor(self.prefix_tokens, device=input_ids.device)\n",
    "        \n",
    "#         # make sure that the actual prefix matches expectation \n",
    "#         if self.step == self.warmup:  # (only need to check once)\n",
    "#             prefix_matches = (input_ids[0, :prefix_len] == prefix_ids).all()\n",
    "#             if not prefix_matches:\n",
    "#                 print(f\"WARNING: prefix mismatch:\\nexpected {self.prefix_tokens}, got {input_ids[0, :prefix_len].tolist()}\")\n",
    "        \n",
    "#         # make sure we dont have special tokens emitted AFTER decoder prefix\n",
    "#         has_special_after_prefix = (input_ids[:, prefix_len:] > self.eos_id).any()\n",
    "#         if has_special_after_prefix:\n",
    "#             print(f\"WARNING: special tokens found after prefix at step {self.step}\")\n",
    "        \n",
    "#         clean_ids = input_ids[:, prefix_len:]\n",
    "#         lm_logits = self.lm(input_ids=clean_ids).logits[:, -1, :]\n",
    "#         lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "        \n",
    "#         # FUSION STEP \n",
    "#         # P_fused(y|x) = log P_ASR(y|x) + alpha × log P_LM(y)\n",
    "#         fused = w_lp.clone()\n",
    "#         fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        \n",
    "#         # optional normalization step\n",
    "#         # fused -= torch.logsumexp(fused, dim=-1, keepdim=True)\n",
    "#         self.step += 1\n",
    "#         return fused\n",
    "\n",
    "class NoOpLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    A simple logits processor that performs no modifications to the logits.\n",
    "    \"\"\"\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Return the scores unchanged\n",
    "\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        return w_lp\n",
    "    \n",
    "\n",
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, eos_id, prefix_tokens, alpha=0.25, warmup=3):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "        self.eos_id = eos_id\n",
    "        self.prefix_tokens = prefix_tokens  # <|startoftranscript|><|notimestamps|>\n",
    "        self.alpha = alpha \n",
    "        self.warmup = warmup\n",
    "        self.step = 0 \n",
    "        self.use_mask = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        if self.step < self.warmup: \n",
    "            self.step += 1 \n",
    "            return w_lp\n",
    "\n",
    "        prefix_len = len(self.prefix_tokens)\n",
    "        prefix_ids = torch.tensor(self.prefix_tokens, device=input_ids.device)\n",
    "\n",
    "        \n",
    "        # make sure that the actual prefix matches expectation \n",
    "        if self.step == self.warmup:  # (only need to check once)\n",
    "            prefix_matches = (input_ids[0, :prefix_len] == prefix_ids).all()\n",
    "            if not prefix_matches:\n",
    "                print(f\"WARNING: prefix mismatch:\\nexpected {self.prefix_tokens}, got {input_ids[0, :prefix_len].tolist()}\")\n",
    "        \n",
    "        # make sure we dont have special tokens emitted AFTER decoder prefix\n",
    "        has_special_after_prefix = (input_ids[:, prefix_len:] > self.eos_id).any()\n",
    "        if has_special_after_prefix:\n",
    "            print(f\"WARNING: special tokens found after prefix at step {self.step}\")\n",
    "        \n",
    "        clean_ids = input_ids[:, prefix_len:]\n",
    "        lm_logits = self.lm(input_ids=clean_ids).logits[:, -1, :]\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "        # FUSION STEP \n",
    "        # P_fused(y|x) = log P_ASR(y|x) + [HAS_EOS_MASK] * alpha * log P_LM(y)\n",
    "        fused = w_lp.clone()\n",
    "        if self.use_mask: \n",
    "            # this condition is to help exclude terminated sequences from getting revived by our lm\n",
    "            has_eos = (input_ids == self.eos_id).any(dim=1)\n",
    "            fusion_mask = (~has_eos).float().unsqueeze(-1) \n",
    "            fused[:, :self.eos_id] += fusion_mask*self.alpha * lm_lp[:, :self.eos_id]\n",
    "        else: \n",
    "            fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        \n",
    "        # print(input_ids)\n",
    "        # optional normalization step\n",
    "        # fused -= torch.logsumexp(fused, dim=-1, keepdim=True)\n",
    "        self.step += 1\n",
    "        return fused\n",
    "  \n",
    "fusion_proc = ShallowFusion(\n",
    "    lm=gpt2,\n",
    "    eos_id=EOS_ID,\n",
    "    prefix_tokens=PREFIX_TOK_IDS,\n",
    "    alpha=0.1,\n",
    "    warmup=2\n",
    ")\n",
    "fusion_proc.use_mask = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d2dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding:   0%|          | 0/17 [00:00<?, ?it/s]Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Decoding: 100%|██████████| 17/17 [00:51<00:00,  3.02s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "from tqdm import tqdm \n",
    "\n",
    "fused = []\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        fused_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "            num_beams=2,\n",
    "            return_timestamps=False,\n",
    "            return_token_timestamps=False,\n",
    "            # do_sample=False,\n",
    "            # length_penalty=1.1,\n",
    "            # repetition_penalty=1.1,\n",
    "            # max_length=100,  # Safety limit\n",
    "        )\n",
    "\n",
    "    decoded = processor.batch_decode(\n",
    "        fused_ids, \n",
    "        skip_special_tokens=True, \n",
    "        # output_word_offsets=True\n",
    "        )\n",
    "    \n",
    "    fused.extend(decoded)\n",
    "    fusion_proc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f466239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 17/17 [00:34<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "noop_processor = NoOpLogitsProcessor()\n",
    "vanilla = []\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        vanilla_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([noop_processor]),\n",
    "            num_beams=2,\n",
    "            do_sample=False,\n",
    "            return_timestamps=False,\n",
    "            return_token_timestamps=False\n",
    "            # do_sample=False,\n",
    "            # length_penalty=1.1,\n",
    "            # repetition_penalty=1.1,\n",
    "            # max_length=100,  # Safety limit\n",
    "        )\n",
    "\n",
    "    decoded = processor.batch_decode(\n",
    "        vanilla_ids, \n",
    "        skip_special_tokens=True, \n",
    "        # output_word_offsets=True\n",
    "        )\n",
    "    \n",
    "    vanilla.extend(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f06c8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"vanilla\":[i.strip() for i in vanilla], \n",
    "        \"fused\":[i.strip() for i in fused], \n",
    "        \"reference\":ds['text'],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3afaf542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base  WER (punct-insensitive): 0.08241219037403308\n",
      "Fused WER (punct-insensitive): 0.07002387319332835\n"
     ]
    }
   ],
   "source": [
    "from jiwer import (\n",
    "    Compose,\n",
    "    ToLowerCase,\n",
    "    RemovePunctuation,\n",
    "    RemoveMultipleSpaces,\n",
    "    Strip,\n",
    "    ReduceToListOfListOfWords,\n",
    "    wer\n",
    ")\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "# helper to handle both str and list[str]\n",
    "def _map(func, x):\n",
    "    return [func(t) for t in x] if isinstance(x, list) else func(x)\n",
    "\n",
    "def remove_diacritics(x):\n",
    "    return _map(unidecode, x)\n",
    "\n",
    "def split_hyphens_and_slashes(x):\n",
    "    # replace any dash or slash with a space so we never glue words together\n",
    "    return _map(lambda t: re.sub(r\"[-–—/]\", \" \", t), x)\n",
    "\n",
    "def normalize_nums(x):\n",
    "    # unify 12–16 → 12-16\n",
    "    return _map(lambda t: re.sub(r\"(\\d)[-–—-](\\d)\", r\"\\1-\\2\", t), x)\n",
    "\n",
    "transform = Compose([\n",
    "    ToLowerCase(),\n",
    "    remove_diacritics,\n",
    "    split_hyphens_and_slashes,    # ← split first\n",
    "    normalize_nums,\n",
    "    RemovePunctuation(),           # now drop commas, periods, etc.\n",
    "    RemoveMultipleSpaces(),\n",
    "    Strip(),\n",
    "    ReduceToListOfListOfWords(),   # produce [[“word”,…],…]\n",
    "])\n",
    "\n",
    "def compute_wer(ref, hyp):\n",
    "    return wer(\n",
    "        ref, hyp,\n",
    "        reference_transform=transform,\n",
    "        hypothesis_transform=transform,\n",
    "    )\n",
    "\n",
    "# rename & score\n",
    "results_df = results_df.rename(columns={\"gt\": \"reference\"})\n",
    "results_df[\"wer_base\"]  = results_df.apply(\n",
    "    lambda r: compute_wer(r[\"reference\"], r[\"vanilla\"]), axis=1\n",
    ")\n",
    "results_df[\"wer_fused\"] = results_df.apply(\n",
    "    lambda r: compute_wer(r[\"reference\"], r[\"fused\"]), axis=1\n",
    ")\n",
    "\n",
    "print(\"Base  WER (punct-insensitive):\", results_df[\"wer_base\"].mean())\n",
    "print(\"Fused WER (punct-insensitive):\", results_df[\"wer_fused\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1ac6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['diff'] = abs(results_df.wer_base - results_df.wer_fused)\n",
    "top_diffs = results_df.sort_values(by='diff', ascending=False)\n",
    "top_diffs = top_diffs[top_diffs['diff']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f577d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GT:    The urinary bladder is distended with smooth wall thickening. Bilateral hydroureteronephrosis present. Foley catheter placement recommended.\n",
      "Base:  The urinary bladder is distended with smooth wall thickening. Bilateral hydrouriterinophrosis present. Foley catheter placement recommended.\n",
      "Fused: The urinary bladder is distended with smooth wall thickening.\n",
      "base err - fused err: -0.375\n",
      "\n",
      "GT:    Endoscopic retrograde cholangiopancreatography revealed type I choledochal cyst with anomalous pancreaticobiliary junction.\n",
      "Base:  Endoscopic retrograde colangiopancreatography revealed type 1 collodocal cyst with anomalous pancreatic obiliary junction.\n",
      "Fused: Endoscopic retrograde cholangiopancreatography revealed type 1 collodocal cyst with anomalous pancreaticobiliary junction.\n",
      "base err - fused err: 0.25\n",
      "\n",
      "GT:    Multiparametric MRI revealed infiltrating ductal carcinoma with peritumoral lymphovascular invasion and axillary lymphadenopathy measuring 2.3 cm.\n",
      "Base:  Multi-parametric MRI revealed infiltrating ductal carcinoma with peritumeral lymphovascular invasion and axillary lymphadenopathy measuring 2.3 centimeters.\n",
      "Fused: Multiparametric MRI revealed infiltrating ductal carcinoma with peritumoral lymphovascular invasion and axillary lymphadenopathy, measuring 2.3 centimeters.\n",
      "base err - fused err: 0.1875\n",
      "\n",
      "GT:    Dual-energy CT characterized crystal deposition consistent with tophaceous pseudogout involving the atlantoaxial joint.\n",
      "Base:  Dual energy CT characterized crystal deposition consistent with tofacious pseudogout involving the Atlanta axial joint.\n",
      "Fused: Dual energy CT characterized crystal deposition consistent with tofacious pseudogout, involving the atlantoaxial joint.\n",
      "base err - fused err: 0.14285714285714285\n",
      "\n",
      "GT:    MRI spectroscopy detected elevated taurine peaks in a medulloblastoma of the desmoplastic/nodular subtype.\n",
      "Base:  MRI spectroscopy detected elevated taurine peaks in a medulloblastoma of the decimal plastic slash nodular subtype.\n",
      "Fused: MRI spectroscopy detected elevated taurine peaks in a medulloblastoma of the desmoplastic-slash-nodular subtype.\n",
      "base err - fused err: 0.14285714285714285\n",
      "\n",
      "GT:    Transesophageal echocardiography revealed papillary fibroelastoma on the aortic valve with mobile frond-like projections measuring 8 mm.\n",
      "Base:  Transosophageal echocardiography milled papillary fibrolastoma on the aortic valve with mobile, frond-like projections measuring 8 millimeters.\n",
      "Fused: Transosophageal echocardiography milled papillary fibroelastoma on the aortic valve with mobile, frond-like projections measuring 8 mm.\n",
      "base err - fused err: 0.11764705882352941\n",
      "\n",
      "GT:    Muscle biopsy revealed inclusion body myositis with rimmed vacuoles and TDP-43 positive cytoplasmic inclusions on immunofluorescence.\n",
      "Base:  Muscle biopsy revealed inclusion body myositis with rimmed vacuoles and TDP43-positive cytoplasmic inclusions on immunofluorescence.\n",
      "Fused: Muscle biopsy revealed inclusion body myositis with rimmed vacuoles and TDP-43-positive cytoplasmic inclusions on immunofluorescence.\n",
      "base err - fused err: 0.11764705882352941\n",
      "\n",
      "GT:    Histopathology demonstrated Rosai-Dorfman disease with emperipolesis of histiocytes.\n",
      "Base:  Histopathology demonstrated Rosi-Dorfman disease with empiricalesis of histiocytes.\n",
      "Fused: Histopathology demonstrated Rosai-Dorfman disease with Empiricalisis of Histiocytes.\n",
      "base err - fused err: 0.1111111111111111\n",
      "\n",
      "GT:    Dacryocystography revealed complete nasolacrimal duct obstruction with presaccal stenosis requiring dacryocystorhinostomy.\n",
      "Base:  Dacryocystography revealed complete nasolacrymal duct obstruction with pre-sacryl stenosis requiring dacryocysto rhinostomy.\n",
      "Fused: Dacryocystography revealed complete nasolacrimal duct obstruction with pre-sacral stenosis requiring dacryocysto-rhinostomy.\n",
      "base err - fused err: 0.09090909090909088\n",
      "\n",
      "GT:    Contrast-enhanced ultrasound demonstrated portal vein thrombosis with cavernous transformation and hepatopetal collateral circulation.\n",
      "Base:  Contrast-enhanced ultrasound demonstrated portal vein thrombosis with cavernous transformation and hepatopaedal collateral circulation.\n",
      "Fused: Contrast-enhanced ultrasound demonstrated portal vein thrombosis with cavernous transformation and hepatopetal collateral circulation.\n",
      "base err - fused err: 0.07142857142857142\n",
      "\n",
      "GT:    Bone scintigraphy exhibited the 'hot skull' sign in polyostotic Paget disease involving the calvarium.\n",
      "Base:  bone centigraphy exhibited the hot skull sign in polyaustodic pageant disease involving the calvarium.\n",
      "Fused: bone centigraphy exhibited the hot skull sign in polyaustodic Paget disease involving the calvarium.\n",
      "base err - fused err: 0.07142857142857142\n",
      "\n",
      "GT:    Optical coherence tomography angiography detected choroidal neovascularization secondary to pathologic myopia with subfoveal fluid accumulation.\n",
      "Base:  Optical coherence tomography and geography detected choroidal neovascularization, secondary to pathologic myopia with subphovial fluid accumulation.\n",
      "Fused: Optical coherence tomography and geography detected choroidal neovascularization, secondary to pathologic myopia with subfoveal fluid accumulation.\n",
      "base err - fused err: 0.06666666666666668\n",
      "\n",
      "GT:    High-resolution manometry revealed type II achalasia with panesophageal pressurization and incomplete lower esophageal sphincter relaxation.\n",
      "Base:  High-resolution monometry revealed Type II acholasia with panosophageal pressurization and incomplete lower esophageal sphincter relaxation.\n",
      "Fused: High-resolution manometry revealed Type II acholasia with panosophageal pressurization and incomplete lower esophageal sphincter relaxation.\n",
      "base err - fused err: 0.0625\n",
      "\n",
      "GT:    Bilateral ground glass opacities in a peripheral and basilar distribution typical for COVID pneumonia. No pleural effusion or pneumothorax.\n",
      "Base:  bilateral ground glass opacities in a peripheral and basilar distribution typical for COVID pneumonia. No plural effusion or pneumothorax.\n",
      "Fused: bilateral ground-glass opacities in a peripheral and basilar distribution typical for COVID pneumonia. No pleural effusion or pneumothorax.\n",
      "base err - fused err: 0.05263157894736842\n",
      "\n",
      "GT:    Prominent lymph nodes in the right axilla with loss of fatty hilum largest measuring 2.5 centimeters. Recommend tissue sampling.\n",
      "Base:  prominent lymph nodes in the right axilla with loss of fatty hilum, largest measuring 2.5 centimeters. Recommend tissue sampling.\n",
      "Fused: prominent lymph nodes in the right axilla with loss of fatty hilum, largest measuring 2.5 cm, recommend tissue sampling.\n",
      "base err - fused err: -0.05263157894736842\n",
      "\n",
      "GT:    The gallbladder is distended with wall thickening measuring up to 5 millimeters and pericholecystic fluid. Multiple shadowing gallstones are noted.\n",
      "Base:  The gallbladder is distended with wall thickening measuring up to 5 millimeters and pericolosystic fluid. Multiple shadowing gallstones are noted.\n",
      "Fused: The gallbladder is distended with wall thickening measuring up to five millimeters and pericolocystic fluid. Multiple shadowing gallstones are noted.\n",
      "base err - fused err: -0.05\n",
      "\n",
      "GT:    There is a 2.3 centimeter enhancing mass in the right hepatic lobe segment seven demonstrating arterial hyperenhancement with washout on delayed phases.\n",
      "Base:  There is a 2.3 cm enhancing mass in the right hepatic lobe segment 7, demonstrating arterial hyper enhancement with washout on delayed phases.\n",
      "Fused: There is a 2.3-centimeter enhancing mass in the right hepatic lobe segment 7, demonstrating arterial hyper-enhancement with washout on delayed phases.\n",
      "base err - fused err: 0.04545454545454547\n"
     ]
    }
   ],
   "source": [
    "print_str = '''\n",
    "GT:    {}\n",
    "Base:  {}\n",
    "Fused: {}\n",
    "base err - fused err: {}'''\n",
    "\n",
    "for idx, row in top_diffs.iterrows():\n",
    "    row_str = print_str.format(\n",
    "        row['reference'], \n",
    "        row['vanilla'], \n",
    "        row['fused'],\n",
    "        row['wer_base'] - row['wer_fused']\n",
    "    )\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32e310",
   "metadata": {},
   "source": [
    "# BONEYARD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332dfbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, librosa, numpy as np, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "print(str(CACHE_DIR).split('/')[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3161254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    GPT2LMHeadModel, AutoTokenizer\n",
    ")\n",
    "\n",
    "SR = 16_000\n",
    "BATCH_SIZE = 5\n",
    "WHISPER_ID = \"openai/whisper-small.en\"\n",
    "GPT2_ID = \"cwestnedge/gpt2-small-pubmed\"\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\").resolve()\n",
    "MANIFEST = \"../data/output/manifest.jsonl\"\n",
    "AUDIO_DIR = \"../data/output\"  \n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# fast tokenizers will show token mismatch between models and will be auto loaded when we run on colab A100 set flag to false to avoid annoyingness\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "EOS_ID = gpt2_tok.eos_token_id # 50256 (unchanged)\n",
    "\n",
    "print(\"Decoder start token ID:\", whisper.generation_config.decoder_start_token_id)\n",
    "print(\"Decoder start token:\", processor.decode([whisper.generation_config.decoder_start_token_id]))\n",
    "\n",
    "PREFIX_TOK_IDS = [whisper.generation_config.decoder_start_token_id]\n",
    "for position, token_id in whisper.generation_config.forced_decoder_ids:\n",
    "    PREFIX_TOK_IDS.append(token_id)\n",
    "\n",
    "print(f\"\\nComplete prefix: {PREFIX_TOK_IDS}\")\n",
    "print(f\"Decoded: '{processor.decode(PREFIX_TOK_IDS)}'\")\n",
    "print(f\"Total prefix length: {len(PREFIX_TOK_IDS)}\")\n",
    "\n",
    "def build_dataset(manifest_path: str, batch_size: int, num_proc: int = 4) -> Dataset:\n",
    "    with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "\n",
    "    def add_audio(batch):\n",
    "        batch[\"audio\"] = [\n",
    "            librosa.load(f\"{AUDIO_DIR}/{fname}\", sr=SR, mono=True)[0].astype(np.float32)\n",
    "            for fname in batch[\"file\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return ds.map(add_audio, batched=True, batch_size=batch_size, num_proc=num_proc)\n",
    "\n",
    "def encode_audio(batch):\n",
    "    # batch[\"audio\"] is List[np.ndarray], each at its natural length\n",
    "    feats = processor.feature_extractor(\n",
    "        batch[\"audio\"], # for whatever reason processor doesnt support PT tensors so numpy array or list for now.\n",
    "        sampling_rate=SR,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=processor.feature_extractor.n_samples,  # n_samples = chunk_length * sampling_rate\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    #  input_features : Tensor (B, T_max, 80)\n",
    "    #  attention_mask : Tensor (B, T_max)\n",
    "    batch[\"input_features\"] = feats.input_features\n",
    "    batch[\"attention_mask\"] = feats.attention_mask\n",
    "    return batch\n",
    "\n",
    "ds = build_dataset(MANIFEST, batch_size=BATCH_SIZE).select([21,22,23,24,25])\n",
    "\n",
    "# choosing NOT to overwrite ds with removed fields so we can eval on text field later,\n",
    "# could also create a collator and pass fields we care about through, but seems like \n",
    "# too much extra code tbh, indices will still match if we dont shuffle\n",
    "ds_processed = ds.map(\n",
    "    encode_audio, \n",
    "    batch_size=2, \n",
    "    batched=True,\n",
    "    remove_columns=list(ds.features.keys())\n",
    "    )\n",
    "\n",
    "ds_processed.set_format(type=\"torch\", columns=[\"input_features\",\"attention_mask\"])\n",
    "loader = DataLoader(ds_processed, batch_size=2, shuffle=False)\n",
    "\n",
    "for i in range(len(gpt2_tok.get_vocab())):\n",
    "    a = processor.tokenizer.decode([i])\n",
    "    b = gpt2_tok.decode([i])\n",
    "    if a != b:\n",
    "        print(f\"Token mismatch at index {i}\\nwhisper token: {a}\\n   gpt2 token: {b} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0635234f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Decoder start token ID:\", whisper.config.decoder_start_token_id)\n",
    "# print(\"BOS token ID:\", processor.tokenizer.bos_token_id)\n",
    "# print(\"Suppress tokens:\", whisper.config.suppress_tokens if hasattr(whisper.config, 'suppress_tokens') else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09978102",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor, LogitsProcessorList\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "class HelloWorldLP(LogitsProcessor):\n",
    "    def __init__(self, alpha=0.0, warmup=2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.step  = 0\n",
    "        self.warmup = warmup\n",
    "\n",
    "    def reset(self): self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        if self.step < self.warmup: \n",
    "            self.step+=1 \n",
    "            return w_lp\n",
    "        self.step+=1\n",
    "\n",
    "        return scores\n",
    "\n",
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, eos_id, alpha=0.25, warmup=3):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "        self.eos_id = eos_id # should be 50256\n",
    "        self.alpha = alpha \n",
    "        self.warmup = warmup\n",
    "        self.step = 0 \n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        if self.step < self.warmup: \n",
    "            self.step += 1 \n",
    "            return scores  # Return RAW SCORES, not w_lp!\n",
    "        \n",
    "        self.step += 1 \n",
    "\n",
    "        whisper_start_ids = [50257, 50362]\n",
    "        lm_input_ids = input_ids[:, len(whisper_start_ids):]\n",
    "\n",
    "        lm_logits = self.lm(\n",
    "            input_ids=lm_input_ids,\n",
    "        ).logits[:, -1, :]  # just want next token logits\n",
    "\n",
    "        # Convert to log probs\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "        # Create fusion mask: shape [batch_size, 1]\n",
    "        # 1.0 for sequences that DON'T want EOS, 0.0 for those that do\n",
    "        \n",
    "        # Apply fusion\n",
    "        fused = w_lp.clone()\n",
    "        fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        \n",
    "        return fused  # RETU\n",
    "\n",
    "class FixedShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, eos_id, prefix_tokens, alpha=0.25, warmup=2):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False)\n",
    "        self.eos_id = eos_id\n",
    "        self.prefix_tokens = prefix_tokens\n",
    "        self.alpha = alpha\n",
    "        self.warmup = warmup\n",
    "        self.step = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        \n",
    "        if self.step < self.warmup:\n",
    "            self.step += 1\n",
    "            return w_lp\n",
    "        \n",
    "        prefix_len = len(self.prefix_tokens)\n",
    "        \n",
    "        if input_ids.shape[1] <= prefix_len:\n",
    "            self.step += 1\n",
    "            return w_lp\n",
    "        \n",
    "        # CRITICAL FIX: Remove EOS tokens from sequence\n",
    "        clean_ids = input_ids[:, prefix_len:]\n",
    "        \n",
    "        # Remove EOS tokens - truncate at first EOS\n",
    "        for i in range(clean_ids.shape[0]):\n",
    "            seq = clean_ids[i]\n",
    "            eos_mask = seq == self.eos_id\n",
    "            if eos_mask.any():\n",
    "                first_eos = eos_mask.nonzero(as_tuple=True)[0][0].item()\n",
    "                clean_ids = clean_ids[:, :first_eos]\n",
    "                break\n",
    "        \n",
    "        if clean_ids.shape[1] == 0:\n",
    "            self.step += 1\n",
    "            return w_lp\n",
    "        \n",
    "        # Now GPT-2 gets clean input without EOS\n",
    "        lm_logits = self.lm(input_ids=clean_ids).logits[:, -1, :]\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "        \n",
    "        # Simple fusion\n",
    "        fused = w_lp.clone()\n",
    "        fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        \n",
    "        self.step += 1\n",
    "        return fused\n",
    "    \n",
    "    \n",
    "hw_proc = HelloWorldLP(warmup=2)\n",
    "\n",
    "fusion_proc = ShallowFusion(\n",
    "    lm=gpt2,\n",
    "    eos_id=50256,\n",
    "    alpha=0.15,\n",
    "    warmup=2\n",
    ")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    out1 = whisper.generate(\n",
    "        input_features=feats,\n",
    "        attention_mask=masks,\n",
    "        logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        return_timestamps=False,\n",
    "        num_beams=2,\n",
    "        # max_new_tokens= 10\n",
    "    )\n",
    "    fusion_proc.reset()\n",
    "    # out2 = whisper.generate(\n",
    "    #     input_features=feats,\n",
    "    #     attention_mask=masks,\n",
    "    #     return_dict_in_generate=True,\n",
    "    #     output_scores=True,\n",
    "    # )\n",
    "    \n",
    "# print((out1.scores[-1] != out2.scores[-1]).sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb8897",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [torch.tensor([[50257, 50362,  6930,   329],\n",
    "        [50257, 50362,  6930,    13],\n",
    "        [50257, 50362,   383,  7976],\n",
    "        [50257, 50362,   262,  7976],\n",
    "        [50257, 50362,  6952,   345],\n",
    "        [50257, 50362,  1318,   318],\n",
    "        [50257, 50362,  6952,   345],\n",
    "        [50257, 50362,  6952,   921],\n",
    "        [50257, 50362,   329,   262],\n",
    "        [50257, 50362,   329,   345]], device='mps:0'),\n",
    "torch.tensor([[50257, 50362,  6930,   329,  4964],\n",
    "        [50257, 50362,  6930,   329,   262],\n",
    "        [50257, 50362,   383,  7976,  2436],\n",
    "        [50257, 50362,   262,  7976,  2436],\n",
    "        [50257, 50362,  1318,   318,   257],\n",
    "        [50257, 50362,  6952,   345,    13],\n",
    "        [50257, 50362,  6952,   345,    13],\n",
    "        [50257, 50362,  6952,   345,   329],\n",
    "        [50257, 50362,   329,   345,    13],\n",
    "        [50257, 50362,   329,   262,  1306]], device='mps:0')]\n",
    "\n",
    "\n",
    "prefix_len = len(PREFIX_TOK_IDS)\n",
    "gpt2_ids = sequences[0][:,prefix_len:]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    lm_scores = gpt2(\n",
    "        input_ids = gpt2_ids\n",
    "    ).logits[:,-1,:]\n",
    "\n",
    "\n",
    "lm_lp = torch.log_softmax(lm_scores, dim=-1)\n",
    "lm_lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154595f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # clean_ids = input_ids[:, prefix_len:]\n",
    "    # lm_logits = self.lm(input_ids=clean_ids).logits[:, -1, :]\n",
    "    # lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "    # # FUSION STEP \n",
    "    # # P_fused(y|x) = log P_ASR(y|x) + [HAS_EOS_MASK] * alpha * log P_LM(y)\n",
    "    # fused = w_lp.clone()\n",
    "    # if self.use_mask: \n",
    "    #     # this condition is to help exclude terminated sequences from getting revived by our lm\n",
    "    #     has_eos = (input_ids == self.eos_id).any(dim=1)\n",
    "    #     fusion_mask = (~has_eos).float().unsqueeze(-1) \n",
    "    #     fused[:, :self.eos_id] += fusion_mask*self.alpha * lm_lp[:, :self.eos_id]\n",
    "    # else: \n",
    "    #     fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "    \n",
    "    # print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66234358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what generation strategy was used\n",
    "print(f\"Do sample: {whisper.config.do_sample if hasattr(whisper.config, 'do_sample') else 'N/A'}\")\n",
    "print(f\"Temperature: {whisper.config.temperature if hasattr(whisper.config, 'temperature') else 'N/A'}\")\n",
    "print(f\"Num beams: {whisper.config.num_beams if hasattr(whisper.config, 'num_beams') else 'N/A'}\")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# Force greedy decoding (argmax) to match your analysis\n",
    "out1_greedy = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=False,  # Force greedy/argmax\n",
    "    num_beams=1,      # No beam search\n",
    "    return_timestamps=False\n",
    "    # temperature=1.0,  # No temperature scaling\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b488d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_len = out1_greedy.sequences.shape[1] - len(out1_greedy.scores)\n",
    "print(f\"Prefix length: {prefix_len}\")\n",
    "\n",
    "t = 30\n",
    "input_ids_at_t = out1_greedy.sequences[:, :prefix_len + t].clone()\n",
    "scores_at_t = out1_greedy.scores[t]\n",
    "\n",
    "in_scope_ids = input_ids_at_t[:, 2:]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gpt2_scores = gpt2(\n",
    "        input_ids=in_scope_ids.to(DEVICE),\n",
    "    ).logits[:, -1, :]\n",
    "\n",
    "g_lp = torch.log_softmax(gpt2_scores, dim=-1)\n",
    "w_lp = torch.log_softmax(scores_at_t, dim=-1)\n",
    "\n",
    "# Apply fusion (using alpha=0.3 as in your code)\n",
    "alpha = 0.3\n",
    "\n",
    "fused = scores_at_t.clone()\n",
    "fused[:, :EOS_ID] +=  alpha * g_lp[:, :EOS_ID]\n",
    "\n",
    "# Get next tokens with different strategies\n",
    "next_token_fused = fused.argmax(dim=-1)\n",
    "next_token_raw = scores_at_t.argmax(dim=-1)\n",
    "next_token_gpt2 = g_lp.argmax(dim=-1)\n",
    "\n",
    "# Decode for comparison\n",
    "print(\"\\nToken choices:\")\n",
    "print(f\"Raw ASR:{processor.decode(next_token_raw[0].item())}\")\n",
    "print(f\"GPT-2  :{processor.decode(next_token_gpt2[0].item())}\")\n",
    "print(f\"Fusion :{processor.decode(next_token_fused[0].item())}\")\n",
    "\n",
    "# Show the actual sequences\n",
    "actual_next = out1_greedy.sequences[:, prefix_len + t]\n",
    "print(f\"Actual next token in sequence: {processor.decode(actual_next[0].item())}\")\n",
    "\n",
    "# Compare full sequences\n",
    "inputs_with_raw = torch.cat([input_ids_at_t, next_token_raw.unsqueeze(1)], dim=-1)\n",
    "inputs_with_fused = torch.cat([input_ids_at_t, next_token_fused.unsqueeze(1)], dim=-1)\n",
    "inputs_actual = out1_greedy.sequences[:, :prefix_len + t + 1]\n",
    "\n",
    "print(\"\\nFull sequences:\")\n",
    "print(f\"Context: {processor.batch_decode(input_ids_at_t)[0]}\")\n",
    "print(f\"With raw ASR: {processor.batch_decode(inputs_with_raw)[0]}\")\n",
    "print(f\"With fusion: {processor.batch_decode(inputs_with_fused)[0]}\")\n",
    "print(f\"Actual sequence: {processor.batch_decode(inputs_actual)[0]}\")\n",
    "\n",
    "print(f\"\\nNote: Step {t} was pure ASR during generation (no LogitsProcessor used)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROPER FUSION TESTING - During Active Generation (Not After EOS)\n",
    "\n",
    "def test_fusion_during_generation():\n",
    "    \"\"\"Test fusion at the right time - during active generation\"\"\"\n",
    "    \n",
    "    print(\"=== TESTING FUSION DURING ACTIVE GENERATION ===\")\n",
    "    \n",
    "    # Get a fresh generation\n",
    "    batch = next(iter(loader))\n",
    "    feats = batch['input_features'][:1].to(DEVICE)\n",
    "    masks = batch['attention_mask'][:1].to(DEVICE)\n",
    "    \n",
    "    # Generate with return_dict to get intermediate scores\n",
    "    with torch.no_grad():\n",
    "        result = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=20\n",
    "        )\n",
    "    \n",
    "    prefix_len = result.sequences.shape[1] - len(result.scores)\n",
    "    print(f\"Prefix length: {prefix_len}\")\n",
    "    print(f\"Total generation steps: {len(result.scores)}\")\n",
    "    \n",
    "    # Test fusion at different steps DURING generation\n",
    "    test_steps = [2, 5, 8, 10] if len(result.scores) > 10 else list(range(min(len(result.scores), 5)))\n",
    "    \n",
    "    for step in test_steps:\n",
    "        if step >= len(result.scores):\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- TESTING STEP {step} (Active Generation) ---\")\n",
    "        \n",
    "        # Get the context at this step\n",
    "        input_ids_at_step = result.sequences[:, :prefix_len + step]\n",
    "        scores_at_step = result.scores[step]\n",
    "        \n",
    "        # Extract clean context for GPT-2\n",
    "        clean_context = input_ids_at_step[:, prefix_len:]\n",
    "        \n",
    "        print(f\"Context: '{processor.decode(clean_context[0])}'\")\n",
    "        print(f\"Contains EOS: {(clean_context == EOS_ID).any().item()}\")\n",
    "        \n",
    "        # Only test if no EOS in context (active generation)\n",
    "        if not (clean_context == EOS_ID).any():\n",
    "            # Get predictions\n",
    "            with torch.no_grad():\n",
    "                gpt2_logits = gpt2(clean_context).logits[:, -1, :]\n",
    "            \n",
    "            w_lp = torch.log_softmax(scores_at_step, dim=-1)\n",
    "            g_lp = torch.log_softmax(gpt2_logits, dim=-1)\n",
    "            \n",
    "            # Test different alpha values\n",
    "            alphas = [0.2, 0.5, 0.8]\n",
    "            \n",
    "            whisper_pred = w_lp.argmax(dim=-1)[0].item()\n",
    "            gpt2_pred = g_lp.argmax(dim=-1)[0].item()\n",
    "            \n",
    "            print(f\"  Whisper wants: '{processor.decode(whisper_pred)}'\")\n",
    "            print(f\"  GPT-2 wants: '{processor.decode(gpt2_pred)}'\")\n",
    "            \n",
    "            for alpha in alphas:\n",
    "                fused_scores = w_lp.clone()\n",
    "                fused_scores[:, :EOS_ID] += alpha * g_lp[:, :EOS_ID]\n",
    "                fused_pred = fused_scores.argmax(dim=-1)[0].item()\n",
    "                \n",
    "                if fused_pred != whisper_pred:\n",
    "                    print(f\"  α={alpha}: '{processor.decode(fused_pred)}' 🔄 CHANGED!\")\n",
    "                else:\n",
    "                    print(f\"  α={alpha}: '{processor.decode(fused_pred)}' ➡️ Same\")\n",
    "        else:\n",
    "            print(f\"  Skipping - sequence already ended\")\n",
    "\n",
    "def compare_fusion_effectiveness():\n",
    "    \"\"\"Compare vanilla vs fusion with proper alpha\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"=== COMPARING VANILLA VS FUSION (Proper Alpha) ===\")\n",
    "    \n",
    "    batch = next(iter(loader))\n",
    "    feats = batch['input_features'][:1].to(DEVICE)\n",
    "    masks = batch['attention_mask'][:1].to(DEVICE)\n",
    "    \n",
    "    # Vanilla generation\n",
    "    with torch.no_grad():\n",
    "        vanilla_result = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=30,\n",
    "            return_timestamps=False\n",
    "        )\n",
    "    \n",
    "    # Fusion with higher alpha\n",
    "    fusion_proc = ShallowFusion(\n",
    "        lm=gpt2,\n",
    "        eos_id=EOS_ID,\n",
    "        prefix_tokens=PREFIX_TOK_IDS,\n",
    "        alpha=0.7,  # Higher alpha to see effect\n",
    "        warmup=2\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fusion_result = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            max_new_tokens=30,\n",
    "            return_timestamps=False\n",
    "        )\n",
    "    \n",
    "    # Compare results\n",
    "    vanilla_text = processor.decode(vanilla_result[0], skip_special_tokens=True)\n",
    "    fusion_text = processor.decode(fusion_result[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Reference: {ds['text'][0]}\")\n",
    "    print(f\"Vanilla:   {vanilla_text}\")\n",
    "    print(f\"Fusion α=0.7: {fusion_text}\")\n",
    "    print(f\"Different: {vanilla_text != fusion_text}\")\n",
    "    \n",
    "    if vanilla_text != fusion_text:\n",
    "        print(\"✅ SUCCESS: Fusion with proper alpha shows differences!\")\n",
    "        \n",
    "        # Show character-level diff\n",
    "        from difflib import unified_diff\n",
    "        diff = list(unified_diff(\n",
    "            vanilla_text.split(), \n",
    "            fusion_text.split(),\n",
    "            fromfile='vanilla',\n",
    "            tofile='fusion',\n",
    "            lineterm=''\n",
    "        ))\n",
    "        if diff:\n",
    "            print(\"\\nWord-level differences:\")\n",
    "            for line in diff:\n",
    "                print(f\"  {line}\")\n",
    "    else:\n",
    "        print(\"⚠️ Still identical - try even higher alpha\")\n",
    "    \n",
    "    return vanilla_text, fusion_text\n",
    "\n",
    "# RUN THE PROPER TESTS\n",
    "test_fusion_during_generation()\n",
    "compare_fusion_effectiveness()\n",
    "\n",
    "print(\"\\n🎯 KEY INSIGHTS:\")\n",
    "print(\"1. Test fusion DURING generation, not after EOS\")\n",
    "print(\"2. GPT-2 predicting 'The' after EOS is normal - it doesn't understand stopping context\")\n",
    "print(\"3. Your fusion correctly chooses to end rather than continue inappropriately\")\n",
    "print(\"4. Use higher alpha (0.5-0.8) to see LM influence during active generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.encode('<|startoftranscript|><|notimestamps|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69053e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out1_greedy.sequences\n",
    "oob_mask = input_ids_at_t >= EOS_ID  # 50256\n",
    "filtered_ids = input_ids_at_t.masked_fill(oob_mask, PAD_ID)  # 50257\n",
    "attention_mask = (filtered_ids != PAD_ID).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa34385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what generation strategy was used\n",
    "print(f\"Do sample: {whisper.config.do_sample if hasattr(whisper.config, 'do_sample') else 'N/A'}\")\n",
    "print(f\"Temperature: {whisper.config.temperature if hasattr(whisper.config, 'temperature') else 'N/A'}\")\n",
    "print(f\"Num beams: {whisper.config.num_beams if hasattr(whisper.config, 'num_beams') else 'N/A'}\")\n",
    "\n",
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# Force greedy decoding (argmax) to match your analysis\n",
    "out1_greedy = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    do_sample=False,  # Force greedy/argmax\n",
    "    num_beams=1,      # No beam search\n",
    ")\n",
    "\n",
    "# First, determine the prefix length\n",
    "prefix_len = out1_greedy.sequences.shape[1] - len(out1_greedy.scores)\n",
    "print(f\"Prefix length: {prefix_len}\")\n",
    "\n",
    "# Choose which step to analyze\n",
    "t = 10\n",
    "\n",
    "# Get the correct input sequence that was used to generate scores[t]\n",
    "input_ids_at_t = out1_greedy.sequences[:, :prefix_len + t].clone()\n",
    "scores_at_t = out1_greedy.scores[t]\n",
    "\n",
    "print(f\"Analyzing step {t}:\")\n",
    "print(f\"Input shape: {input_ids_at_t.shape}\")\n",
    "print(f\"This input was used to generate token at position {prefix_len + t}\")\n",
    "\n",
    "# NEW: Properly handle special tokens for GPT-2\n",
    "# Extract only valid GPT-2 tokens (< EOS_ID) for each sequence\n",
    "batch_size = input_ids_at_t.shape[0]\n",
    "gpt2_inputs = []\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Get only tokens that are valid for GPT-2 (< 50256)\n",
    "    valid_mask = input_ids_at_t[i] < EOS_ID\n",
    "    valid_tokens = input_ids_at_t[i][valid_mask]\n",
    "    gpt2_inputs.append(valid_tokens)\n",
    "\n",
    "# Pad sequences to same length for batching\n",
    "max_len = max(len(seq) for seq in gpt2_inputs)\n",
    "filtered_ids = torch.full((batch_size, max_len), PAD_ID, device=DEVICE)\n",
    "attention_mask = torch.zeros((batch_size, max_len), device=DEVICE)\n",
    "\n",
    "for i, seq in enumerate(gpt2_inputs):\n",
    "    filtered_ids[i, :len(seq)] = seq\n",
    "    attention_mask[i, :len(seq)] = 1\n",
    "\n",
    "print(f\"GPT-2 input (first sequence): {processor.decode(gpt2_inputs[0])}\")\n",
    "print(f\"Valid token IDs: {gpt2_inputs[0].tolist()}\")\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gpt2_scores = gpt2(\n",
    "        input_ids=filtered_ids.to(DEVICE),\n",
    "        attention_mask=attention_mask.to(DEVICE)\n",
    "    ).logits\n",
    "\n",
    "# Get log probabilities\n",
    "g_lp = torch.log_softmax(gpt2_scores[:, -1, :], dim=-1)\n",
    "w_lp = torch.log_softmax(scores_at_t, dim=-1)\n",
    "\n",
    "# Apply fusion\n",
    "alpha = 0.1\n",
    "fused = scores_at_t.clone()\n",
    "fused[:, :EOS_ID] += alpha * g_lp[:, :EOS_ID]\n",
    "\n",
    "# Get next tokens with different strategie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8130bad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get next tokens with different strategies\n",
    "next_token_fused = fused.argmax(dim=-1)\n",
    "next_token_raw = scores_at_t.argmax(dim=-1)\n",
    "next_token_gpt2 = g_lp.argmax(dim=-1)\n",
    "\n",
    "# Decode for comparison\n",
    "print(\"\\nToken choices:\")\n",
    "print(f\"Raw ASR:{processor.decode(next_token_raw[0].item())}\")\n",
    "print(f\"GPT-2  :{processor.decode(next_token_gpt2[0].item())}\")\n",
    "print(f\"Fusion :{processor.decode(next_token_fused[0].item())}\")\n",
    "\n",
    "# Show the actual sequences\n",
    "actual_next = out1_greedy.sequences[:, prefix_len + t]\n",
    "print(f\"Actual next token in sequence: {processor.decode(actual_next[0].item())}\")\n",
    "\n",
    "# Compare full sequences\n",
    "inputs_with_raw = torch.cat([input_ids_at_t, next_token_raw.unsqueeze(1)], dim=-1)\n",
    "inputs_with_fused = torch.cat([input_ids_at_t, next_token_fused.unsqueeze(1)], dim=-1)\n",
    "inputs_actual = out1_greedy.sequences[:, :prefix_len + t + 1]\n",
    "\n",
    "print(\"\\nFull sequences:\")\n",
    "print(f\"Context: {processor.batch_decode(input_ids_at_t)[0]}\")\n",
    "print(f\"With raw ASR: {processor.batch_decode(inputs_with_raw)[0]}\")\n",
    "print(f\"With fusion: {processor.batch_decode(inputs_with_fused)[0]}\")\n",
    "print(f\"Actual sequence: {processor.batch_decode(inputs_actual)[0]}\")\n",
    "\n",
    "print(f\"\\nNote: Step {t} was pure ASR during generation (no LogitsProcessor used)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5a143",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_token_id = next_token_gpt2[0].item()\n",
    "print(f\"GPT-2 predicted token ID: {gpt2_token_id}\")\n",
    "\n",
    "# Try different decode methods\n",
    "print(f\"Decode with processor: '{processor.decode(gpt2_token_id)}'\")\n",
    "print(f\"Decode with tokenizer: '{processor.tokenizer.decode([gpt2_token_id])}'\")\n",
    "print(f\"Token string: '{processor.tokenizer.convert_ids_to_tokens([gpt2_token_id])[0]}'\")\n",
    "\n",
    "# Check if it's a space or special character\n",
    "if gpt2_token_id < 50257:\n",
    "    token = processor.tokenizer.convert_ids_to_tokens([gpt2_token_id])[0]\n",
    "    print(f\"Token repr: {repr(token)}\")  # This will show \\n, \\t, spaces etc\n",
    "    print(f\"Token bytes: {token.encode('utf-8')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd81a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e89853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what GPT-2 actually received as input\n",
    "print(f\"Input to GPT-2: {processor.decode(filtered_ids[0])}\")\n",
    "print(f\"Raw token IDs: {filtered_ids[0].tolist()}\")\n",
    "\n",
    "# Let's manually test GPT-2 with clear medical context\n",
    "test_sequence = gpt2_tok.encode(\"The patient had pain in the abdomen and pel\", return_tensors=\"pt\").to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    test_output = gpt2(test_sequence).logits[0, -1, :]\n",
    "    test_probs = torch.softmax(test_output, dim=0)\n",
    "    top5 = test_probs.topk(5)\n",
    "    \n",
    "print(\"\\nGPT-2 predictions for 'abdomen and pel':\")\n",
    "for prob, idx in zip(top5.values, top5.indices):\n",
    "    if idx < 50257:\n",
    "        print(f\"  {processor.decode(idx)}: {prob:.3f}\")\n",
    "\n",
    "# Also check if the model weights look reasonable\n",
    "print(f\"\\nGPT-2 weight stats:\")\n",
    "print(f\"Mean: {gpt2.lm_head.weight.mean().item():.4f}\")\n",
    "print(f\"Std: {gpt2.lm_head.weight.std().item():.4f}\")\n",
    "\n",
    "# Test a few medical terms\n",
    "medical_tests = [\n",
    "    \"The patient's hep\",  # -> hepatic/hepatitis\n",
    "    \"The cardiac cath\",   # -> catheterization  \n",
    "    \"Diagnosed with pneum\" # -> pneumonia\n",
    "]\n",
    "\n",
    "for test in medical_tests:\n",
    "    tokens = processor.tokenizer.encode(test, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = gpt2(tokens).logits[0, -1, :]\n",
    "    next_token = processor.decode(out.argmax().item())\n",
    "    print(f\"\\n'{test}' -> '{next_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, pad_id, eos_id, alpha=0.25, warmup=3):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False).to(DEVICE)\n",
    "        self.pad_id = pad_id # should be 50257 \n",
    "        self.eos_id = eos_id # should be 50256\n",
    "        self.alpha = alpha \n",
    "        self.warmup = warmup\n",
    "        self.step = 0 \n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "\n",
    "        if self.step < self.warmup: \n",
    "            self.step+=1 \n",
    "            return scores\n",
    "        self.step+=1 \n",
    "\n",
    "        oov_mask = input_ids >= self.eos_id # gpt2 and whispers EOS token\n",
    "        padded_input_ids = input_ids.masked_fill(oov_mask, self.pad_id) # PAD_ID\n",
    "        attention_mask = (padded_input_ids != self.pad_id).long()\n",
    "        \n",
    "        lm_logits = self.lm(\n",
    "            input_ids=padded_input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        ).logits[:,-1,:] # just want next token logits\n",
    "\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)\n",
    "        \n",
    "        fused = scores.clone()\n",
    "        fused[:, :self.eos_id] += self.alpha * lm_lp[:, :self.eos_id]\n",
    "        # optional normalization step\n",
    "        # fused -= torch.logsumexp(fused, dim=-1, keepdim=True)\n",
    "        return fused\n",
    "\n",
    "alpha=0.3\n",
    "warmup = 1\n",
    "fusion_proc = ShallowFusion(gpt2, PAD_ID, EOS_ID, alpha=alpha, warmup=warmup)\n",
    "\n",
    "out = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    logits_processor=[fusion_proc],\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "\n",
    "prefix_len = out.sequences.shape[1] - len(out.scores)  # Should be 2\n",
    "pure_asr_step0 = out.scores[0]  # This is pure ASR (no fusion)\n",
    "\n",
    "input_for_step0 = out.sequences[:, :prefix_len]\n",
    "\n",
    "oov = input_for_step0 >= EOS_ID\n",
    "gpt2_inp = input_for_step0.masked_fill(oov, PAD_ID)\n",
    "attn = (gpt2_inp != PAD_ID).long()\n",
    "\n",
    "lm_logits = gpt2(gpt2_inp, attention_mask=attn).logits[:, -1]\n",
    "lm_logp = torch.log_softmax(lm_logits, dim=-1)\n",
    "\n",
    "fused_manual = pure_asr_step0.clone()\n",
    "fused_manual[:, :EOS_ID] += alpha * lm_logp[:, :EOS_ID]\n",
    "\n",
    "# Now run AGAIN with warmup=0 to get actual fused scores at step 0\n",
    "fusion_proc2 = ShallowFusion(gpt2, PAD_ID, EOS_ID, alpha=alpha, warmup=0)\n",
    "out2 = whisper.generate(\n",
    "    input_features=feats,\n",
    "    attention_mask=masks,\n",
    "    logits_processor=[fusion_proc2],\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    ")\n",
    "\n",
    "# Compare!\n",
    "fused_actual = out2.scores[0]\n",
    "diff = (fused_actual - fused_manual).abs()\n",
    "valid_mask = ~torch.isnan(diff)\n",
    "print(f\"Max diff: {diff[valid_mask].max().item():.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "(out3.scores[t] != fused).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0355ec4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5463f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba41e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a986a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 5\n",
    "input_ids_at_t = out1.sequences[:,:t].clone()\n",
    "scores_at_t = out1.scores[t-1]\n",
    "texts = processor.batch_decode(\n",
    "    input_ids_at_t,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "# texts = [t.strip() for t in texts] # this doesnt seem to make a difference\n",
    "gpt2_inputs = gpt2_tok(\n",
    "    texts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,           # pads to longest in batch\n",
    "    truncation=False,       # adjust as you like\n",
    ").input_ids.to(DEVICE)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    gpt2_scores = gpt2(\n",
    "        input_ids = gpt2_inputs,\n",
    "        attention_mask = torch.ones_like(gpt2_inputs).to(DEVICE)\n",
    "    ).logits[:,-1]\n",
    "\n",
    "g_lp = torch.log_softmax(gpt2_scores, dim=-1)\n",
    "w_lp = torch.log_softmax(scores_at_t, dim=-1)\n",
    "\n",
    "fused = w_lp.clone()\n",
    "fused[:, :g_lp.size(1)] += 0 * g_lp\n",
    "next_token = fused.argmax(dim=-1).unsqueeze(1)\n",
    "next_token_raw = scores_at_t.argmax(dim=-1).unsqueeze(1)\n",
    "\n",
    "inputs_raw = torch.cat([input_ids_at_t, next_token_raw], dim=-1)\n",
    "inputs_fused = torch.cat([input_ids_at_t, next_token], dim=-1)\n",
    "# inputs_fused -= torch.logsumexp(inputs_fused, dim=-1, keepdim=True)\n",
    "\n",
    "a = processor.batch_decode(input_ids_at_t)\n",
    "b = processor.batch_decode(inputs_fused)\n",
    "c = processor.batch_decode(inputs_raw)\n",
    "d = processor.batch_decode(out1.sequences[:,:t+1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147a785",
   "metadata": {},
   "source": [
    "### ------------ TESTING END ---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8ba6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # =============================================================\n",
    "# #  One-cell evaluation (uses your original jiwer transform)\n",
    "# #  -------------------------------------------------------------\n",
    "# #  Metrics per model:\n",
    "# #    • Global WER                --> same as your old script\n",
    "# #    • Medical-Term Recall (MTR) --> fraction of terms perfectly present\n",
    "# #    • Medical-Term-only WER     --> WER on tokens that belong to terms\n",
    "# #\n",
    "# #  Expects a DataFrame `results_df` with columns:\n",
    "# #        reference, vanilla, fused, medical_terms\n",
    "# #  where medical_terms is list[str]  (or a string repr like \"['a','b']\").\n",
    "# # =============================================================\n",
    "\n",
    "# import re, ast, itertools, pandas as pd\n",
    "# from jiwer import (\n",
    "#     Compose, ToLowerCase, RemovePunctuation, RemoveMultipleSpaces,\n",
    "#     Strip, ReduceToListOfListOfWords, wer\n",
    "# )\n",
    "# from unidecode import unidecode\n",
    "\n",
    "\n",
    "# # ---------- helper to handle both str & list[str] -------------------\n",
    "# def _map(func, x):\n",
    "#     return [func(t) for t in x] if isinstance(x, list) else func(x)\n",
    "\n",
    "# def remove_diacritics(x):\n",
    "#     return _map(unidecode, x)\n",
    "\n",
    "# def split_hyphens_and_slashes(x):\n",
    "#     return _map(lambda t: re.sub(r\"[-–—/]\", \" \", t), x)\n",
    "\n",
    "# def normalize_nums(x):\n",
    "#     return _map(lambda t: re.sub(r\"(\\d)[-–—-](\\d)\", r\"\\1-\\2\", t), x)\n",
    "\n",
    "# # ---------- your original jiwer transform ---------------------------\n",
    "# transform = Compose([\n",
    "#     ToLowerCase(),\n",
    "#     remove_diacritics,\n",
    "#     split_hyphens_and_slashes,\n",
    "#     normalize_nums,\n",
    "#     RemovePunctuation(),\n",
    "#     RemoveMultipleSpaces(),\n",
    "#     Strip(),\n",
    "#     ReduceToListOfListOfWords(),   # -> [[\"word\", ...], ...]\n",
    "# ])\n",
    "\n",
    "# def compute_wer(ref, hyp):\n",
    "#     return wer(\n",
    "#         ref, hyp,\n",
    "#         reference_transform=transform,\n",
    "#         hypothesis_transform=transform,\n",
    "#     )\n",
    "\n",
    "# # ---------- lightweight normaliser for term metrics -----------------\n",
    "# _punc_rx   = re.compile(r\"[^\\w\\s]\")\n",
    "# _range_rx  = re.compile(r\"(\\d)[-–—-](\\d)\")\n",
    "# _split_rx  = re.compile(r\"[-–—/]\")\n",
    "\n",
    "# def _normalise(text: str) -> str:\n",
    "#     text = unidecode(text.lower())\n",
    "#     text = _range_rx.sub(r\"\\1-\\2\", text)\n",
    "#     text = _split_rx.sub(\" \", text)\n",
    "#     text = _punc_rx.sub(\" \", text)\n",
    "#     return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "# def _term_recall(row, hyp_text):\n",
    "#     hyp_norm = _normalise(hyp_text)\n",
    "#     hits = sum(1 for t in row[\"medical_terms\"] if _normalise(t) in hyp_norm)\n",
    "#     return hits / len(row[\"medical_terms\"])\n",
    "\n",
    "# def _extract_term_tokens(row, text):\n",
    "#     tokens = _normalise(text).split()\n",
    "#     keep   = [False] * len(tokens)\n",
    "#     for term in row[\"medical_terms\"]:\n",
    "#         ttoks = _normalise(term).split()\n",
    "#         for i in range(len(tokens) - len(ttoks) + 1):\n",
    "#             if tokens[i:i+len(ttoks)] == ttoks:\n",
    "#                 for j in range(i, i+len(ttoks)):\n",
    "#                     keep[j] = True\n",
    "#     return \" \".join(tok for tok, flag in zip(tokens, keep) if flag)\n",
    "\n",
    "# # ---------- main evaluation routine ---------------------------------\n",
    "# def evaluate(df: pd.DataFrame) -> None:\n",
    "#     # ensure list[str] in medical_terms\n",
    "#     if isinstance(df[\"medical_terms\"].iloc[0], str):\n",
    "#         df[\"medical_terms\"] = df[\"medical_terms\"].apply(ast.literal_eval)\n",
    "\n",
    "#     # Global WER (your existing metric)\n",
    "#     df[\"wer_vanilla\"] = df.apply(\n",
    "#         lambda r: compute_wer(r[\"reference\"], r[\"vanilla\"]), axis=1)\n",
    "#     df[\"wer_fused\"]   = df.apply(\n",
    "#         lambda r: compute_wer(r[\"reference\"], r[\"fused\"]), axis=1)\n",
    "\n",
    "#     # Medical-Term Recall\n",
    "#     df[\"mtr_vanilla\"] = df.apply(\n",
    "#         lambda r: _term_recall(r, r[\"vanilla\"]), axis=1)\n",
    "#     df[\"mtr_fused\"]   = df.apply(\n",
    "#         lambda r: _term_recall(r, r[\"fused\"]), axis=1)\n",
    "\n",
    "#     # Medical-Term-only WER\n",
    "#     df[\"mtwer_vanilla\"] = df.apply(\n",
    "#         lambda r: wer(\n",
    "#             _extract_term_tokens(r, r[\"reference\"]),\n",
    "#             _extract_term_tokens(r, r[\"vanilla\"]),\n",
    "#             reference_transform=transform,\n",
    "#             hypothesis_transform=transform), axis=1)\n",
    "#     df[\"mtwer_fused\"]   = df.apply(\n",
    "#         lambda r: wer(\n",
    "#             _extract_term_tokens(r, r[\"reference\"]),\n",
    "#             _extract_term_tokens(r, r[\"fused\"]),\n",
    "#             reference_transform=transform,\n",
    "#             hypothesis_transform=transform), axis=1)\n",
    "\n",
    "#     # -------- summary printout --------------------------------------\n",
    "#     print(\"\\n=== Global WER ===\")\n",
    "#     print(f\"  vanilla : {df['wer_vanilla'].mean():.4f}\")\n",
    "#     print(f\"  fused   : {df['wer_fused'].mean():.4f}\")\n",
    "\n",
    "#     print(\"\\n=== Medical-Term Recall ===\")\n",
    "#     print(f\"  vanilla : {df['mtr_vanilla'].mean():.4f}\")\n",
    "#     print(f\"  fused   : {df['mtr_fused'].mean():.4f}\")\n",
    "\n",
    "#     print(\"\\n=== Medical-Term-only WER ===\")\n",
    "#     print(f\"  vanilla : {df['mtwer_vanilla'].mean():.4f}\")\n",
    "#     print(f\"  fused   : {df['mtwer_fused'].mean():.4f}\")\n",
    "\n",
    "# # ---------- run on your DataFrame -----------------------------------\n",
    "\n",
    "# import pandas as pd \n",
    "\n",
    "# results_df = pd.DataFrame(\n",
    "#     {\n",
    "#         \"vanilla\":[i.strip() for i in vanilla], \n",
    "#         \"fused\":[i.strip() for i in fused], \n",
    "#         \"reference\":ds['text'],\n",
    "#         \"medical_terms\":ds['medical_terms']\n",
    "#     }\n",
    "# )\n",
    "\n",
    "# evaluate(results_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LogitsProcessor, LogitsProcessorList\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ShallowFusion(LogitsProcessor):\n",
    "#     def __init__(self, lm, shared_vocab, eos, alpha=0.3, warmup_steps=3):\n",
    "#         super().__init__()\n",
    "#         self.lm = lm.eval().requires_grad_(False)\n",
    "#         self.V = shared_vocab\n",
    "#         self.eos = eos\n",
    "#         self.alpha = alpha\n",
    "#         self.warmup = warmup_steps\n",
    "#         self.step = 0\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         print('printing input_ids.size(), scores.size(), step, input_ids, dec_ids')\n",
    "#         print(input_ids.size(), scores.size(), self.step, input_ids, processor.batch_decode(input_ids))\n",
    "#         self.step+=1 \n",
    "\n",
    "#         return scores\n",
    "    \n",
    "# fusion_proc = ShallowFusion(\n",
    "#     lm=gpt2,\n",
    "#     shared_vocab=gpt2.config.vocab_size,\n",
    "#     eos=EOS_ID,\n",
    "#     alpha=0.3\n",
    "# )\n",
    "\n",
    "# batch = next(iter(loader))\n",
    "# feats = batch['input_features'].to(DEVICE)\n",
    "# masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     out1 = whisper.generate(\n",
    "#         input_features=feats,\n",
    "#         attention_mask=masks,\n",
    "#         logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "#         return_dict_in_generate=True,\n",
    "#         output_scores=True,\n",
    "#         num_beams=2,\n",
    "#     )\n",
    "\n",
    "#     # out2 = whisper.generate(\n",
    "#     #     input_features=feats,\n",
    "#     #     attention_mask=masks,\n",
    "#     #     return_dict_in_generate=True,\n",
    "#     #     output_scores=True,\n",
    "#     #     num_beams=2\n",
    "#     # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14230833",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# Generate to get sequences\n",
    "with torch.no_grad():\n",
    "    out = whisper.generate(\n",
    "        input_features=feats,\n",
    "        attention_mask=masks,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=5\n",
    "    )\n",
    "# Compare step 0\n",
    "decoder_ids = out.sequences[:,0:-1]  # Just the start token\n",
    "with torch.no_grad():\n",
    "    direct_logits = whisper(feats, decoder_input_ids=decoder_ids).logits[:, -1, :].to(DEVICE)\n",
    "    direct_lp = torch.log_softmax(direct_logits, dim=-1)\n",
    "\n",
    "gen_lp = out.scores[-1].to(DEVICE)\n",
    "\n",
    "print(gen_lp)\n",
    "print(direct_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d3e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd359960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter truly out of bounds vocab >=EOS\n",
    "oob_mask = decoder_ids > EOS_ID # create mask for gpt2 OOV tokens emitted by whisper\n",
    " # replace with gpt2 pad token\n",
    "filtered = decoder_ids.masked_fill(oob_mask, gpt2_tok.pad_token_id)\n",
    "attention_mask = (filtered != gpt2_tok.pad_token_id).long()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_new = gpt2(input_ids=filtered, attention_mask=attention_mask).logits[:,-1, :]\n",
    "\n",
    "# because we dont want gpt2 to impact or determine termination just ASR model\n",
    "logits_new[:,:EOS_ID-1].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
