{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7588133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SF_EVAL', '.models', 'hfcache']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, librosa, numpy as np, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\" / \"hfcache\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "print(str(CACHE_DIR).split('/')[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be2e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    GPT2LMHeadModel, AutoTokenizer\n",
    ")\n",
    "\n",
    "SR = 16_000\n",
    "BATCH_SIZE = 5\n",
    "WHISPER_ID = \"openai/whisper-small.en\"\n",
    "GPT2_ID = \"cwestnedge/gpt2-small-pubmed\"\n",
    "SHARED_VOCAB = 50257\n",
    "ALPHA = 0.3\n",
    "INIT_W_STEPS = 2\n",
    "MAX_STEPS = 256\n",
    "\n",
    "MANIFEST = \"../data/output/manifest.jsonl\"\n",
    "AUDIO_DIR = \"../data/output\"  \n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8f72bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = WhisperProcessor.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "EOS_ID = processor.tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be085a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 30/30 [00:00<00:00, 30.65 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 43.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(manifest_path: str, batch_size: int, num_proc: int = 4) -> Dataset:\n",
    "    with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "\n",
    "    def add_audio(batch):\n",
    "        batch[\"audio\"] = [\n",
    "            librosa.load(f\"{AUDIO_DIR}/{fname}\", sr=SR, mono=True)[0].astype(np.float32)\n",
    "            for fname in batch[\"file\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return ds.map(add_audio, batched=True, batch_size=batch_size, num_proc=num_proc)\n",
    "\n",
    "def encode_audio(batch):\n",
    "    # batch[\"audio\"] is List[np.ndarray], each at its natural length\n",
    "    feats = processor.feature_extractor(\n",
    "        batch[\"audio\"],\n",
    "        sampling_rate=SR,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=processor.feature_extractor.n_samples,  # n_samples = chunk_length * sampling_rate\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    #  input_features : Tensor (B, T_max, 80)\n",
    "    #  attention_mask : Tensor (B, T_max)\n",
    "    batch[\"input_features\"] = feats.input_features\n",
    "    batch[\"attention_mask\"] = feats.attention_mask\n",
    "    return batch\n",
    "\n",
    "ds = build_dataset(MANIFEST, batch_size=BATCH_SIZE)\n",
    "ds_processed = ds.map(\n",
    "    encode_audio, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    batched=True,\n",
    "    remove_columns=['uuid', 'file', 'category', 'index', 'text', 'audio']   # keep \"text\" for WER\n",
    "    )\n",
    "\n",
    "ds_processed.set_format(type=\"torch\", columns=[\"input_features\",\"attention_mask\"])\n",
    "loader = DataLoader(ds_processed, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ee48b3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor, LogitsProcessorList\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, lm_tok, shared_vocab, special_mask, alpha=0.3):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False)\n",
    "        self.tok = lm_tok\n",
    "        self.V = shared_vocab\n",
    "        self.special_mask = special_mask\n",
    "        self.alpha = alpha\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        self.step += 1\n",
    "        if self.step <= 2:\n",
    "            return scores\n",
    "\n",
    "        B = input_ids.size(0) # batch_size * num_beams\n",
    "        device = scores.device\n",
    "        mask = self.special_mask.to(device)\n",
    "\n",
    "        for beam in input_ids:\n",
    "            print(processor.batch_decode(beam))\n",
    "        keep = ~mask[input_ids[0]]\n",
    "        filtered = input_ids[0, keep]\n",
    "        gpt_ids = filtered.unsqueeze(0) if filtered.numel() else input_ids[:, -1:].clone()\n",
    "        \n",
    "        # if still no lexical token, skip fusion\n",
    "        if (gpt_ids < self.V).sum() == 0:\n",
    "            return scores\n",
    "        gpt_ids = gpt_ids.to(device)\n",
    "\n",
    "        w_lp_full = torch.log_softmax(scores, dim=-1)\n",
    "        w_lp_shared = w_lp_full[:, : self.V]\n",
    "\n",
    "        g_logits = self.lm(gpt_ids).logits[:, -1, :]\n",
    "        g_lp = torch.log_softmax(g_logits, dim=-1)\n",
    "\n",
    "        fused_shared = w_lp_shared + self.alpha * g_lp\n",
    "        fused_shared = torch.where(\n",
    "            mask[: self.V].unsqueeze(0), \n",
    "            w_lp_shared, \n",
    "            fused_shared\n",
    "        )\n",
    "        \n",
    "        fused = torch.cat([fused_shared, w_lp_full[:, self.V:]], dim=-1)\n",
    "        return fused # still log‑probs; generate() is fine with that?\n",
    "    \n",
    "\n",
    "special_ids = set(processor.tokenizer.all_special_ids)\n",
    "special_mask = torch.tensor(\n",
    "    [i in special_ids for i in range(processor.tokenizer.vocab_size)],\n",
    "    dtype=torch.bool,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "fusion_proc = ShallowFusion(\n",
    "    lm=gpt2, lm_tok=gpt2_tok,\n",
    "    shared_vocab=gpt2.config.vocab_size,\n",
    "    special_mask=special_mask,\n",
    "    alpha=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61d2dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', 'operative']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' resonance']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' we', ' started']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', 'operative', ' pathology']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha', 'em']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' resonance', ' imaging']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' C']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', 'operative', ' pathology', ' confirmed']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha', 'em', 'oglobin']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' resonance', ' imaging', ' revealed']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' C', 'ef']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', 'operative', ' pathology', ' confirmed', ' a']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha', 'em', 'oglobin', ' A']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' resonance', ' imaging', ' revealed', ' a']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'r']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', 'operative', ' pathology', ' confirmed', ' a', ' stage']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha', 'em', 'oglobin', ' A', '1']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' resonance', ' imaging', ' revealed', ' a', ' 3']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'r', 'ach']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha', 'em', 'oglobin', ' A', '1', 'c']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' resonance', ' imaging', ' revealed', ' a', ' 3', '-']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'r', 'ach', 's']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha', 'em', 'oglobin', ' A', '1', 'c', ' has']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cm']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha', 'em', 'oglobin', ' A', '1', 'c', ' has', ' stabilized']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cm', ' dem']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha', 'em', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cm', ' dem', 'y']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha', 'em', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cm', ' dem', 'y', 'el']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' ha', 'em', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cm', ' dem', 'y', 'el', 'inating']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', ' percent']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cm', ' dem', 'y', 'el', 'inating', ' plaque']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', ' percent', ' after']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cm', ' dem', 'y', 'el', 'inating', ' plaque', ' in']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', ' percent', ' after', ' switching']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cm', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', ' percent', ' after', ' switching', ' to']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cm', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to', ' sem']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', ' percent', ' after', ' switching', ' to', ' sem']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cm', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'i']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to', ' sem', 'ag']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', ' percent', ' after', ' switching', ' to', ' sem', 'ag']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'i']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'iv']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to', ' sem', 'ag', 'l']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', ' percent', ' after', ' switching', ' to', ' sem', 'ag', 'l']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'i', '-']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'iv', 'entric']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines', 'is']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines', 'is']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm', 'oid']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm', 'oid']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to', ' sem', 'ag', 'l', 'ut']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', ' percent', ' after', ' switching', ' to', ' sem', 'ag', 'l', 'ut']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'iv', 'entric', 'ular']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'i', '-', 'vent']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected', ' bacterial']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected', ' bacterial']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines', 'is', '.']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines', 'is', '.']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm', 'oid', ' colon']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm', 'oid', ' colon']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to', ' sem', 'ag', 'l', 'ut', 'ide']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', ' percent', ' after', ' switching', ' to', ' sem', 'ag', 'l', 'ut', 'ide']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'iv', 'entric', 'ular', ' white']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'i', '-', 'vent', 'ricular']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected', ' bacterial', ' men']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected', ' bacterial', ' men']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines', 'is', '.', ' [']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines', 'is', '.', ' [']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm', 'oid', ' colon', '.']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm', 'oid', ' colon', '.']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to', ' sem', 'ag', 'l', 'ut', 'ide', '.']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', ' percent', ' after', ' switching', ' to', ' sem', 'ag', 'l', 'ut', 'ide', '.']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'iv', 'entric', 'ular', ' white', ' matter']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'i', '-', 'vent', 'ricular', ' white']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected', ' bacterial', ' men', 'ing']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected', ' bacterial', ' men', 'ing']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines', 'is', '.', ' [', 'BL']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines', 'is', '.', ' [', 'BL']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm', 'oid', ' colon', '.', ' [']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' Stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm', 'oid', ' colon', '.', ' [']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to', ' sem', 'ag', 'l', 'ut', 'ide', '.', ' [']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to', ' sem', 'ag', 'l', 'ut', 'ide', '.', ' (']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'iv', 'entric', 'ular', ' white', ' matter', '.']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'i', '-', 'vent', 'ricular', ' white', ' matter']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected', ' bacterial', ' men', 'ing', 'itis']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected', ' bacterial', ' men', 'ing', 'itis']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' The', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines', 'is', '.', ' [', 'BL', 'ANK']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' the', ' e', 'ch', 'oc', 'ardi', 'ogram', ' shows', ' an', ' eject', 'ion', ' fraction', ' of', ' 35', '%', ' with', ' global', ' hyp', 'ok', 'ines', 'is', '.', ' [', 'BL', 'ANK']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm', 'oid', ' colon', '.', ' [', 'BL']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Post', '-', 'operative', ' pathology', ' confirmed', ' a', ' stage', ' 2', 'a', ' ad', 'en', 'oc', 'arc', 'in', 'oma', ' of', ' the', ' s', 'igm', 'oid', ' colon', '.', ' [', ' P']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to', ' sem', 'ag', 'l', 'ut', 'ide', '.', ' [', 'M']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Her', ' hem', 'oglobin', ' A', '1', 'c', ' has', ' stabilized', ' at', ' 7', '.', '1', '%', ' after', ' switching', ' to', ' sem', 'ag', 'l', 'ut', 'ide', '.', ' [', 'END']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'i', '-', 'vent', 'ricular', ' white', ' matter', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding:   0%|          | 0/6 [00:07<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|startoftranscript|>', '<|notimestamps|>', ' Magnetic', ' Reson', 'ance', ' Imaging', ' revealed', ' a', ' 3', ' cent', 'imeter', ' dem', 'y', 'el', 'inating', ' plaque', ' in', ' the', ' per', 'iv', 'entric', 'ular', ' white', ' matter', '.', ' (']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' 2', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected', ' bacterial', ' men', 'ing', 'itis', '.']\n",
      "['<|startoftranscript|>', '<|notimestamps|>', ' We', ' started', ' ce', 'ft', 'ri', 'ax', 'one', ' two', ' grams', ',', ' given', ' intraven', 'ously', ',', ' every', ' 24', ' hours', ' for', ' suspected', ' bacterial', ' men', 'ing', 'itis', '.']\n",
      "[' The echocardiogram shows an ejection fraction of 35% with global hypokinesis.', ' Post-operative pathology confirmed a stage 2a adenocarcinoma of the sigmoid colon.', ' Her hemoglobin A1c has stabilized at 7.1% after switching to semaglutide.', ' Magnetic Resonance Imaging revealed a 3 centimeter demyelinating plaque in the periventricular white matter.', ' We started ceftriaxone 2 grams, given intravenously, every 24 hours for suspected bacterial meningitis.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GenerationConfig\n",
    "from tqdm import tqdm \n",
    "\n",
    "fused = []\n",
    "refs = []\n",
    "\n",
    "gen_cfg = GenerationConfig(\n",
    "    num_beams=2,\n",
    "    do_sample=False,\n",
    "    max_length=448,\n",
    ")\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        fused_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "            generation_config=gen_cfg,\n",
    "        )\n",
    "    decoded = processor.batch_decode(fused_ids, skip_special_tokens=True)\n",
    "    fused.extend(decoded)\n",
    "    print(decoded)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f466239",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla = []\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        vanilla_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            generation_config=gen_cfg\n",
    "        )\n",
    "    decoded = processor.batch_decode(vanilla_ids, skip_special_tokens=True)\n",
    "    vanilla.extend(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06c8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "print(pd.DataFrame({'vanilla':vanilla, 'fused':fused, 'gt':ds['text']}).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32e310",
   "metadata": {},
   "source": [
    "# BONEYARD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd805d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ShallowFusion(LogitsProcessor):\n",
    "#     def __init__(self, lm, tok, shared_vocab, special_mask, alpha=0.25):\n",
    "#         super().__init__()\n",
    "#         self.lm   = lm.eval().requires_grad_(False)\n",
    "#         self.tok  = tok\n",
    "#         self.V    = shared_vocab\n",
    "#         self.mask = special_mask\n",
    "#         self.alpha = alpha\n",
    "#         self.step  = 0\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         self.step += 1\n",
    "#         if self.step <= 2:\n",
    "#             return scores                    # warm‑up\n",
    "\n",
    "#         B = input_ids.size(0)                # batch*beams\n",
    "#         dev = scores.device\n",
    "#         mask = self.mask.to(dev)\n",
    "\n",
    "#         # —— build GPT context for **each** beam independently ——\n",
    "#         gpt_ctx = []\n",
    "#         for seq in input_ids:                # loop over beams\n",
    "#             keep = ~mask[seq]\n",
    "#             filtered = seq[keep]\n",
    "#             ctx = filtered if filtered.numel() else seq[-1:]\n",
    "#             gpt_ctx.append(ctx)\n",
    "\n",
    "#         # pad to equal length & stack\n",
    "#         gpt_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "#             gpt_ctx, batch_first=True,\n",
    "#             padding_value=self.tok.eos_token_id\n",
    "#         ).to(dev)                            # (B, L_ctx)\n",
    "\n",
    "#         # —— LM logits for every beam ——\n",
    "#         g_logits = self.lm(gpt_ids).logits[:, -1, :]   # (B, V)\n",
    "\n",
    "#         # —— Whisper & GPT log‑probs ——\n",
    "#         w_lp_full   = torch.log_softmax(scores, dim=-1)     # (B, V_whisper)\n",
    "#         w_lp_shared = w_lp_full[:, : self.V]                # (B, V)\n",
    "\n",
    "#         g_lp = torch.log_softmax(g_logits, dim=-1)          # (B, V)\n",
    "\n",
    "#         fused = w_lp_shared + self.alpha * g_lp             # (B, V)\n",
    "#         fused = torch.where(\n",
    "#             mask[: self.V].unsqueeze(0),    # keep Whisper specials intact\n",
    "#             w_lp_shared,\n",
    "#             fused,\n",
    "#         )\n",
    "\n",
    "#         return torch.cat([fused, w_lp_full[:, self.V:]], dim=-1)\n",
    "    \n",
    "# special_ids = set(processor.tokenizer.all_special_ids)\n",
    "# special_mask = torch.tensor(\n",
    "#     [i in special_ids for i in range(processor.tokenizer.vocab_size)],\n",
    "#     dtype=torch.bool,\n",
    "#     device=DEVICE,\n",
    "# )\n",
    "\n",
    "# fusion_proc = ShallowFusion(\n",
    "#     lm=gpt2, tok=gpt2_tok,\n",
    "#     shared_vocab=gpt2.config.vocab_size,\n",
    "#     special_mask=special_mask,\n",
    "#     alpha=0.3\n",
    "# )\n",
    "\n",
    "# from transformers import GenerationConfig\n",
    "# from tqdm import tqdm \n",
    "\n",
    "# fused = []\n",
    "# refs = []\n",
    "\n",
    "# gen_cfg = GenerationConfig(\n",
    "#     num_beams=4,\n",
    "#     do_sample=False,\n",
    "#     max_length=448,\n",
    "# )\n",
    "\n",
    "# for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "#     feats = batch['input_features'].to(DEVICE)\n",
    "#     masks = batch['attention_mask'].to(DEVICE)\n",
    "#     with torch.no_grad():\n",
    "#         fused_ids = whisper.generate(\n",
    "#             input_features=feats,\n",
    "#             attention_mask=masks,\n",
    "#             logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "#             generation_config=gen_cfg,\n",
    "#         )\n",
    "#     decoded = processor.batch_decode(fused_ids, skip_special_tokens=True)\n",
    "#     fused.extend(decoded)\n",
    "#     print(decoded)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb73333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LogitsProcessor, LogitsProcessorList\n",
    "# import torch.nn.functional as F\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# class HelloWorldProcessor(LogitsProcessor):\n",
    "#     \"\"\"\n",
    "#     A toy processor that adds +5.0 to the logit for token ID 7 at every step.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, elm, elm_tokenizer, shared_vocab, special_mask, alpha):\n",
    "#         self.elm = elm\n",
    "#         self.elm_tokenizer = elm_tokenizer\n",
    "#         self.shared_vocab = shared_vocab\n",
    "#         self.special_mask = special_mask\n",
    "#         self.alpha = alpha\n",
    "#         self.counter = 0 \n",
    "\n",
    "#     def __call__(self, input_ids, scores):\n",
    "\n",
    "#         self.counter +=1\n",
    "#         if self.counter > 2:\n",
    "\n",
    "#             keep = ~self.special_mask[input_ids[0]]\n",
    "#             filtered = input_ids[0, keep]\n",
    "            \n",
    "#             if filtered.numel() == 0: \n",
    "#                 elm_ids = input_ids[:, -1:].clone()\n",
    "\n",
    "#             else:\n",
    "#                 elm_ids = filtered.unsqueeze(0)\n",
    "#             with torch.no_grad():\n",
    "#                 w_lp_full = F.log_softmax(scores, dim=-1)\n",
    "#                 w_lp_shared = w_lp_full[:, :SHARED_VOCAB]\n",
    "\n",
    "#                 elm_logits = self.elm(elm_ids).logits[:, -1, :]\n",
    "#                 elm_lp = F.log_softmax(elm_logits, dim=-1)\n",
    "\n",
    "#                 fused_shared = w_lp_shared + self.alpha * elm_lp\n",
    "#                 fused_logits = torch.cat([\n",
    "#                     fused_shared,\n",
    "#                     w_lp_full[:, SHARED_VOCAB:],\n",
    "#                 ], dim=-1)\n",
    "#         else:\n",
    "#             fused_logits = scores.clone()\n",
    "#         return fused_logits\n",
    "    \n",
    "# special_ids = set(processor.tokenizer.all_special_ids)\n",
    "# special_mask = torch.tensor(\n",
    "#     [i in special_ids for i in range(processor.tokenizer.vocab_size)],\n",
    "#     dtype=torch.bool,\n",
    "#     device=DEVICE,\n",
    "# )\n",
    "\n",
    "# b = next(iter(loader))\n",
    "# feats = b['input_features'].to(DEVICE)\n",
    "# mask = b['attention_mask'].to(DEVICE)\n",
    "\n",
    "# hello_proc = HelloWorldProcessor(\n",
    "#     elm=gpt2, \n",
    "#     elm_tokenizer=gpt2_tok, \n",
    "#     shared_vocab=SHARED_VOCAB, \n",
    "#     special_mask=special_mask,\n",
    "#     alpha=0.3\n",
    "#     )\n",
    "\n",
    "# lp_list = LogitsProcessorList([hello_proc])\n",
    "\n",
    "# all_outputs = []\n",
    "# all_references = []\n",
    "# for batch_idx, batch in enumerate(tqdm(loader, desc=\"Processing batches\")):\n",
    "#     feats = batch['input_features'].to(DEVICE)\n",
    "#     masks = batch['attention_mask'].to(DEVICE)\n",
    "#     with torch.no_grad():\n",
    "#         out_ids = whisper.generate(\n",
    "#             input_features=feats,\n",
    "#             attention_mask=mask,\n",
    "#             logits_processor=lp_list,\n",
    "#             max_new_tokens=50,\n",
    "#             num_beams=3,\n",
    "#             no_repeat_ngram_size=3,\n",
    "#         )\n",
    "#     decoded = processor.batch_decode(out_ids, skip_special_tokens=True)\n",
    "#     all_outputs.extend(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6604a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if i wanted to keep uuid in batch id have to use collator and do the following... \n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     uuids = [item[\"uuid\"] for item in batch]\n",
    "#     feats = torch.stack([\n",
    "#         torch.as_tensor(item[\"input_features\"], dtype=torch.float32)\n",
    "#         for item in batch\n",
    "#     ], dim=0)\n",
    "#     masks = torch.stack([\n",
    "#         torch.as_tensor(item[\"attention_mask\"], dtype=torch.long)\n",
    "#         for item in batch\n",
    "#     ], dim=0)  # shape (B, T)\n",
    "\n",
    "#     return {\n",
    "#         \"uuid\":           uuids,\n",
    "#         \"input_features\": feats,\n",
    "#         \"attention_mask\": masks,\n",
    "#     }\n",
    "\n",
    "# loader = DataLoader(\n",
    "#     ds,\n",
    "#     collate_fn=collate_fn,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=False,   # or True if you want\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
