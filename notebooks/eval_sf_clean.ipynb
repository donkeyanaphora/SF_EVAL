{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7588133c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SHALLOW_FUSION_EVAL', 'SF_EVAL', '.models']\n"
     ]
    }
   ],
   "source": [
    "import os, glob, librosa, numpy as np, torch, json\n",
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = (Path.cwd().parent / \".models\").resolve()\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n",
    "os.environ[\"HF_HOME\"] = str(CACHE_DIR)\n",
    "print(str(CACHE_DIR).split('/')[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3be2e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/collinswestnedge/Desktop/programming/SHALLOW_FUSION_EVAL/SF_EVAL/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    GPT2LMHeadModel, AutoTokenizer\n",
    ")\n",
    "\n",
    "SR = 16_000\n",
    "BATCH_SIZE = 5\n",
    "WHISPER_ID = \"openai/whisper-small.en\"\n",
    "GPT2_ID = \"cwestnedge/gpt2-small-pubmed\"\n",
    "\n",
    "MANIFEST = \"../data/output/manifest.jsonl\"\n",
    "AUDIO_DIR = \"../data/output\"  \n",
    "\n",
    "DEVICE = (\n",
    "    \"cuda\" if torch.cuda.is_available()\n",
    "    else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    ")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f72bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50257 50256 50257\n"
     ]
    }
   ],
   "source": [
    "# fast tokenizers will show token mismatch between models and will be auto loaded when we run on colab A100 set flag to false to avoid annoyingness\n",
    "processor = WhisperProcessor.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "whisper = WhisperForConditionalGeneration.from_pretrained(WHISPER_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "gpt2_tok = AutoTokenizer.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR, use_fast=False)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(GPT2_ID, cache_dir=CACHE_DIR).to(DEVICE).eval()\n",
    "\n",
    "if gpt2_tok.pad_token is None:\n",
    "    gpt2_tok.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
    "    gpt2.resize_token_embeddings(len(gpt2_tok))\n",
    "\n",
    "PAD_ID = gpt2_tok.pad_token_id # e.g. 50257\n",
    "EOS_ID = gpt2_tok.eos_token_id # 50256 (unchanged)\n",
    "SHARED_VOCAB = EOS_ID + 1\n",
    "\n",
    "print(PAD_ID, EOS_ID, SHARED_VOCAB)\n",
    "# 50257 50256 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5188f51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(SHARED_VOCAB):\n",
    "  a = processor.tokenizer.decode([i])\n",
    "  b = gpt2_tok.decode([i])\n",
    "  if a != b:\n",
    "    print(f\"Token mismatch at index {i}\\nwhisper token: {a}\\n   gpt2 token: {b} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be085a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=4): 100%|██████████| 85/85 [00:01<00:00, 69.68 examples/s]\n",
      "Map: 100%|██████████| 85/85 [00:02<00:00, 38.50 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(manifest_path: str, batch_size: int, num_proc: int = 4) -> Dataset:\n",
    "    with open(manifest_path, encoding=\"utf-8\") as f:\n",
    "        rows = [json.loads(line) for line in f]\n",
    "\n",
    "    ds = Dataset.from_list(rows)\n",
    "\n",
    "    def add_audio(batch):\n",
    "        batch[\"audio\"] = [\n",
    "            librosa.load(f\"{AUDIO_DIR}/{fname}\", sr=SR, mono=True)[0].astype(np.float32)\n",
    "            for fname in batch[\"file\"]\n",
    "        ]\n",
    "        return batch\n",
    "\n",
    "    return ds.map(add_audio, batched=True, batch_size=batch_size, num_proc=num_proc)\n",
    "\n",
    "def encode_audio(batch):\n",
    "    # batch[\"audio\"] is List[np.ndarray], each at its natural length\n",
    "    feats = processor.feature_extractor(\n",
    "        batch[\"audio\"], # for whatever reason processor doesnt support PT tensors so numpy array or list for now.\n",
    "        sampling_rate=SR,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True, \n",
    "        max_length=processor.feature_extractor.n_samples,  # n_samples = chunk_length * sampling_rate\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    #  input_features : Tensor (B, T_max, 80)\n",
    "    #  attention_mask : Tensor (B, T_max)\n",
    "    batch[\"input_features\"] = feats.input_features\n",
    "    batch[\"attention_mask\"] = feats.attention_mask\n",
    "    return batch\n",
    "\n",
    "ds = build_dataset(MANIFEST, batch_size=BATCH_SIZE)\n",
    "\n",
    "# choosing NOT to overwrite ds with removed fields so we can eval on text field later,\n",
    "# could also create a collator and pass fields we care about through, but seems like \n",
    "# too much extra code tbh, indices will still match if we dont shuffle\n",
    "ds_processed = ds.map(\n",
    "    encode_audio, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    batched=True,\n",
    "    remove_columns=['uuid', 'file', 'category', 'index', 'text', 'audio']\n",
    "    )\n",
    "\n",
    "ds_processed.set_format(type=\"torch\", columns=[\"input_features\",\"attention_mask\"])\n",
    "loader = DataLoader(ds_processed, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d74862e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b3ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "\n",
    "class ShallowFusion(LogitsProcessor):\n",
    "    def __init__(self, lm, fusion_exclusive, pad_id, alpha=0.3, warmup_steps=3, temperature=0.05):\n",
    "        super().__init__()\n",
    "        self.lm = lm.eval().requires_grad_(False)\n",
    "        self.fusion_excl = fusion_exclusive  # e.g. EOS_ID = 50256\n",
    "        self.pad_id = pad_id\n",
    "        self.alpha = alpha\n",
    "        self.warmup = warmup_steps\n",
    "        self.temp = temperature\n",
    "        self.step = 0\n",
    "        self.alpha_scale = 0.35\n",
    "        self.entropy_threshold = 1.3\n",
    "    \n",
    "    def reset(self):\n",
    "        self.step = 0\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, input_ids, scores):\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "\n",
    "        if self.step < self.warmup: \n",
    "            self.step+=1 \n",
    "            return w_lp\n",
    "        self.step+=1 \n",
    "\n",
    "        # ----DYNAMIC ALPHA------\n",
    "        # w_probs = w_lp.exp()\n",
    "        # # Compute entropy safely: ignore zero probabilities\n",
    "        # ent_contrib = torch.where(w_probs > 0,\n",
    "        #                           w_probs * w_lp,\n",
    "        #                           torch.zeros_like(w_lp))\n",
    "        # w_entropy = -(ent_contrib.sum(dim=-1, keepdim=True))  # shape: [B,1]\n",
    "        # # Smooth gating: map entropy to [alpha, alpha*alpha_scale]\n",
    "        # gate = torch.sigmoid((w_entropy - self.entropy_threshold) * 2)\n",
    "        # ----DYNAMIC ALPHA------\n",
    "\n",
    "        # replace OOV tokens with pad tokens and exclude from attention scores\n",
    "        oob_mask = input_ids > self.fusion_excl\n",
    "        filtered_ids = input_ids.masked_fill(oob_mask, self.pad_id)\n",
    "        attn_mask = (filtered_ids != self.pad_id).long()\n",
    "\n",
    "        lm_logits = self.lm(\n",
    "            input_ids=filtered_ids, \n",
    "            attention_mask=attn_mask\n",
    "        ).logits[:,-1,:] # only want logits for next token\n",
    "        lm_lp = torch.log_softmax(lm_logits, dim=-1)[:, :self.fusion_excl]\n",
    "\n",
    "        fused_slice = w_lp[:, :self.fusion_excl] + self.alpha * lm_lp\n",
    "        fused_lp = w_lp.clone()\n",
    "        fused_lp[:, :self.fusion_excl] = fused_slice\n",
    "        \n",
    "        # renormalize\n",
    "        fused_lp -= torch.logsumexp(fused_lp, dim=-1, keepdim=True)\n",
    "        \n",
    "        return fused_lp # / self.temp\n",
    "    \n",
    "fusion_proc = ShallowFusion(\n",
    "    lm=gpt2,\n",
    "    fusion_exclusive=EOS_ID, # e.g. EOS_ID = 50256\n",
    "    pad_id=PAD_ID,# <— 50257\n",
    "    alpha=0.15,\n",
    "    warmup_steps=4,\n",
    "    temperature = 1.0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61d2dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding:   0%|          | 0/17 [00:00<?, ?it/s]Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "Decoding: 100%|██████████| 17/17 [00:41<00:00,  2.43s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LogitsProcessorList\n",
    "from tqdm import tqdm \n",
    "\n",
    "fused = []\n",
    "refs = []\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fused_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "    decoded = processor.batch_decode(fused_ids, skip_special_tokens=True)\n",
    "    fused.extend(decoded)\n",
    "    fusion_proc.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f466239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 17/17 [00:26<00:00,  1.56s/it]\n"
     ]
    }
   ],
   "source": [
    "class NoOpLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    A simple logits processor that performs no modifications to the logits.\n",
    "    \"\"\"\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # Return the scores unchanged\n",
    "\n",
    "        w_lp = torch.log_softmax(scores, dim=-1)\n",
    "        return w_lp\n",
    "\n",
    "\n",
    "noop_processor = NoOpLogitsProcessor()\n",
    "vanilla = []\n",
    "\n",
    "for idx, batch in enumerate(tqdm(loader, total=len(loader), desc=\"Decoding\")):\n",
    "    feats = batch['input_features'].to(DEVICE)\n",
    "    masks = batch['attention_mask'].to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        vanilla_ids = whisper.generate(\n",
    "            input_features=feats,\n",
    "            attention_mask=masks,\n",
    "            logits_processor=LogitsProcessorList([noop_processor]),\n",
    "            num_beams=1,\n",
    "            do_sample=False,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "    decoded = processor.batch_decode(vanilla_ids, skip_special_tokens=True)\n",
    "    vanilla.extend(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06c8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {'vanilla':[i.strip() for i in vanilla], \n",
    "     'fused':[i.strip() for i in fused], \n",
    "     'reference':ds['text']}\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3afaf542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base  WER (punct-insensitive): 0.07469183832477919\n",
      "Fused WER (punct-insensitive): 0.06996960771336627\n"
     ]
    }
   ],
   "source": [
    "from jiwer import (\n",
    "    Compose,\n",
    "    ToLowerCase,\n",
    "    RemovePunctuation,\n",
    "    RemoveMultipleSpaces,\n",
    "    Strip,\n",
    "    ReduceToListOfListOfWords,\n",
    "    wer\n",
    ")\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "# helper to handle both str and list[str]\n",
    "def _map(func, x):\n",
    "    return [func(t) for t in x] if isinstance(x, list) else func(x)\n",
    "\n",
    "def remove_diacritics(x):\n",
    "    return _map(unidecode, x)\n",
    "\n",
    "def split_hyphens_and_slashes(x):\n",
    "    # replace any dash or slash with a space so we never glue words together\n",
    "    return _map(lambda t: re.sub(r\"[-–—/]\", \" \", t), x)\n",
    "\n",
    "def normalize_nums(x):\n",
    "    # unify 12–16 → 12-16\n",
    "    return _map(lambda t: re.sub(r\"(\\d)[-–—-](\\d)\", r\"\\1-\\2\", t), x)\n",
    "\n",
    "transform = Compose([\n",
    "    ToLowerCase(),\n",
    "    remove_diacritics,\n",
    "    split_hyphens_and_slashes,    # ← split first\n",
    "    normalize_nums,\n",
    "    RemovePunctuation(),           # now drop commas, periods, etc.\n",
    "    RemoveMultipleSpaces(),\n",
    "    Strip(),\n",
    "    ReduceToListOfListOfWords(),   # produce [[“word”,…],…]\n",
    "])\n",
    "\n",
    "def compute_wer(ref, hyp):\n",
    "    return wer(\n",
    "        ref, hyp,\n",
    "        reference_transform=transform,\n",
    "        hypothesis_transform=transform,\n",
    "    )\n",
    "\n",
    "# rename & score\n",
    "results_df = results_df.rename(columns={\"gt\": \"reference\"})\n",
    "results_df[\"wer_base\"]  = results_df.apply(\n",
    "    lambda r: compute_wer(r[\"reference\"], r[\"vanilla\"]), axis=1\n",
    ")\n",
    "results_df[\"wer_fused\"] = results_df.apply(\n",
    "    lambda r: compute_wer(r[\"reference\"], r[\"fused\"]), axis=1\n",
    ")\n",
    "\n",
    "print(\"Base  WER (punct-insensitive):\", results_df[\"wer_base\"].mean())\n",
    "print(\"Fused WER (punct-insensitive):\", results_df[\"wer_fused\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1ac6469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vanilla</th>\n",
       "      <th>fused</th>\n",
       "      <th>reference</th>\n",
       "      <th>wer_base</th>\n",
       "      <th>wer_fused</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Dual energy CT characterized crystal depositio...</td>\n",
       "      <td>Dual energy CT characterized crystal depositio...</td>\n",
       "      <td>Dual-energy CT characterized crystal depositio...</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>High-resolution MRI identified leptominin-geom...</td>\n",
       "      <td>High-resolution MRI identified leptomeningeal ...</td>\n",
       "      <td>High-resolution MRI identified leptomeningeal ...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Large heterogeneously enhancing mass centered ...</td>\n",
       "      <td>Large heterogeneously enhancing mass, centered...</td>\n",
       "      <td>Large heterogeneously enhancing mass centered ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.136364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Muscle biopsy revealed inclusion body myositis...</td>\n",
       "      <td>Muscle biopsy revealed inclusion body myositis...</td>\n",
       "      <td>Muscle biopsy revealed inclusion body myositis...</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Next-generation sequencing identified pathogen...</td>\n",
       "      <td>Next-generation sequencing identified pathogen...</td>\n",
       "      <td>Next-generation sequencing identified pathogen...</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Histopathology demonstrated Rosai Dorfman dise...</td>\n",
       "      <td>Histopathology demonstrated Rosi-Dorfman disea...</td>\n",
       "      <td>Histopathology demonstrated Rosai-Dorfman dise...</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>No evidence of pulmonary embolism to the subse...</td>\n",
       "      <td>No evidence of pulmonary embolism to the sub-s...</td>\n",
       "      <td>No evidence of pulmonary embolism to the subse...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>High resolution CT delineated pulmonary alveol...</td>\n",
       "      <td>High resolution CT delineated pulmonary alveol...</td>\n",
       "      <td>High-resolution CT delineated pulmonary alveol...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>MR neurography revealed hourglass-like constri...</td>\n",
       "      <td>MR neurography revealed hourglass-like constri...</td>\n",
       "      <td>MR neurography revealed hourglass-like constri...</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Bone centigraphy exhibited the hot skull sign ...</td>\n",
       "      <td>Bone centigraphy exhibited the hot skull sign ...</td>\n",
       "      <td>Bone scintigraphy exhibited the 'hot skull' si...</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.071429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              vanilla  \\\n",
       "51  Dual energy CT characterized crystal depositio...   \n",
       "30  High-resolution MRI identified leptominin-geom...   \n",
       "6   Large heterogeneously enhancing mass centered ...   \n",
       "69  Muscle biopsy revealed inclusion body myositis...   \n",
       "50  Next-generation sequencing identified pathogen...   \n",
       "29  Histopathology demonstrated Rosai Dorfman dise...   \n",
       "17  No evidence of pulmonary embolism to the subse...   \n",
       "41  High resolution CT delineated pulmonary alveol...   \n",
       "40  MR neurography revealed hourglass-like constri...   \n",
       "45  Bone centigraphy exhibited the hot skull sign ...   \n",
       "\n",
       "                                                fused  \\\n",
       "51  Dual energy CT characterized crystal depositio...   \n",
       "30  High-resolution MRI identified leptomeningeal ...   \n",
       "6   Large heterogeneously enhancing mass, centered...   \n",
       "69  Muscle biopsy revealed inclusion body myositis...   \n",
       "50  Next-generation sequencing identified pathogen...   \n",
       "29  Histopathology demonstrated Rosi-Dorfman disea...   \n",
       "17  No evidence of pulmonary embolism to the sub-s...   \n",
       "41  High resolution CT delineated pulmonary alveol...   \n",
       "40  MR neurography revealed hourglass-like constri...   \n",
       "45  Bone centigraphy exhibited the hot skull sign ...   \n",
       "\n",
       "                                            reference  wer_base  wer_fused  \\\n",
       "51  Dual-energy CT characterized crystal depositio...  0.214286   0.071429   \n",
       "30  High-resolution MRI identified leptomeningeal ...  0.142857   0.000000   \n",
       "6   Large heterogeneously enhancing mass centered ...  0.000000   0.136364   \n",
       "69  Muscle biopsy revealed inclusion body myositis...  0.117647   0.000000   \n",
       "50  Next-generation sequencing identified pathogen...  0.222222   0.111111   \n",
       "29  Histopathology demonstrated Rosai-Dorfman dise...  0.111111   0.222222   \n",
       "17  No evidence of pulmonary embolism to the subse...  0.000000   0.090909   \n",
       "41  High-resolution CT delineated pulmonary alveol...  0.000000   0.083333   \n",
       "40  MR neurography revealed hourglass-like constri...  0.076923   0.000000   \n",
       "45  Bone scintigraphy exhibited the 'hot skull' si...  0.214286   0.142857   \n",
       "\n",
       "        diff  \n",
       "51  0.142857  \n",
       "30  0.142857  \n",
       "6   0.136364  \n",
       "69  0.117647  \n",
       "50  0.111111  \n",
       "29  0.111111  \n",
       "17  0.090909  \n",
       "41  0.083333  \n",
       "40  0.076923  \n",
       "45  0.071429  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df['diff'] = abs(results_df.wer_base - results_df.wer_fused)\n",
    "top_diffs = results_df.sort_values(by='diff', ascending=False).head(10)\n",
    "top_diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f577d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GT:    Dual-energy CT characterized crystal deposition consistent with tophaceous pseudogout involving the atlantoaxial joint.\n",
      "Base:  Dual energy CT characterized crystal deposition consistent with topaceous pseudogout involving the Atlanta axial joint.\n",
      "Fused: Dual energy CT characterized crystal deposition consistent with topatious pseudogout involving the atlantoaxial joint.\n",
      "\n",
      "GT:    High-resolution MRI identified leptomeningeal melanocytosis with diffuse T1 hyperintensity along the cerebellar folia.\n",
      "Base:  High-resolution MRI identified leptominin-geomelanocytosis with diffuse T1 hyperintensity along the cerebellar folia.\n",
      "Fused: High-resolution MRI identified leptomeningeal melanocytosis with diffuse T1 hyperintensity along the cerebellar folia.\n",
      "\n",
      "GT:    Large heterogeneously enhancing mass centered in the right parotid gland measuring 4.2 by 3.8 by 3.5 centimeters with areas of internal necrosis.\n",
      "Base:  Large heterogeneously enhancing mass centered in the right parotid gland measuring 4.2 by 3.8 by 3.5 centimeters with areas of internal necrosis.\n",
      "Fused: Large heterogeneously enhancing mass, centered in the right parotid gland, measuring 4.2 x 3.8 x 3.5 cm, with areas of internal necrosis.\n",
      "\n",
      "GT:    Muscle biopsy revealed inclusion body myositis with rimmed vacuoles and TDP-43 positive cytoplasmic inclusions on immunofluorescence.\n",
      "Base:  Muscle biopsy revealed inclusion body myositis with rimmed vacuoles and TDP43 positive cytoplasmic inclusions on immunofluorescence.\n",
      "Fused: Muscle biopsy revealed inclusion body myositis with rimmed vacuoles and TDP-43 positive cytoplasmic inclusions on immunofluorescence.\n",
      "\n",
      "GT:    Next-generation sequencing identified pathogenic variants in BRCA2 and PALB2 genes conferring hereditary breast and ovarian cancer syndrome.\n",
      "Base:  Next-generation sequencing identified pathogenic variants in Bure-2 and PALB-2 genes, conferring hereditary breast and ovarian cancer syndrome.\n",
      "Fused: Next-generation sequencing identified pathogenic variants in BER-A2 and PALB2 genes, conferring hereditary breast and ovarian cancer syndrome.\n",
      "\n",
      "GT:    Histopathology demonstrated Rosai-Dorfman disease with emperipolesis of histiocytes.\n",
      "Base:  Histopathology demonstrated Rosai Dorfman disease with empropolisis of histiocytes.\n",
      "Fused: Histopathology demonstrated Rosi-Dorfman disease with empropalesis of histiocytes.\n",
      "\n",
      "GT:    No evidence of pulmonary embolism to the subsegmental level. Incidental note made of enlarged mediastinal lymph nodes measuring up to 1.5 centimeters.\n",
      "Base:  No evidence of pulmonary embolism to the subsegmental level. Incidental note made of enlarged mediastinal lymph nodes measuring up to 1.5 centimeters.\n",
      "Fused: No evidence of pulmonary embolism to the sub-segmental level. Incidental note made of enlarged mediastinal lymph nodes measuring up to 1.5 centimeters.\n",
      "\n",
      "GT:    High-resolution CT delineated pulmonary alveolar microlithiasis with diffuse sand-like calcifications.\n",
      "Base:  High resolution CT delineated pulmonary alveolar microlithiasis with diffuse sand-like calcifications.\n",
      "Fused: High resolution CT delineated pulmonary alveolar microlithiases with diffuse sand-like calcifications.\n",
      "\n",
      "GT:    MR neurography revealed hourglass-like constriction of the suprascapular nerve implicating neuralgic amyotrophy.\n",
      "Base:  MR neurography revealed hourglass-like constriction of the suprascapular nerve implicating neuralgic amiotrophy.\n",
      "Fused: MR neurography revealed hourglass-like constriction of the suprascapular nerve implicating neuralgic amyotrophy.\n",
      "\n",
      "GT:    Bone scintigraphy exhibited the 'hot skull' sign in polyostotic Paget disease involving the calvarium.\n",
      "Base:  Bone centigraphy exhibited the hot skull sign in polyaesthetic pageant disease involving the calvarium.\n",
      "Fused: Bone centigraphy exhibited the hot skull sign in polyaesthetic-paget disease involving the calvarium.\n"
     ]
    }
   ],
   "source": [
    "print_str = '''\n",
    "GT:    {}\n",
    "Base:  {}\n",
    "Fused: {}'''\n",
    "\n",
    "for idx, row in top_diffs.iterrows():\n",
    "    row_str = print_str.format(\n",
    "        row['reference'], \n",
    "        row['vanilla'], \n",
    "        row['fused']\n",
    "    )\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad32e310",
   "metadata": {},
   "source": [
    "# BONEYARD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16fb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import LogitsProcessor, LogitsProcessorList\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class ShallowFusion(LogitsProcessor):\n",
    "#     def __init__(self, lm, shared_vocab, eos, alpha=0.3, warmup_steps=3):\n",
    "#         super().__init__()\n",
    "#         self.lm = lm.eval().requires_grad_(False)\n",
    "#         self.V = shared_vocab\n",
    "#         self.eos = eos\n",
    "#         self.alpha = alpha\n",
    "#         self.warmup = warmup_steps\n",
    "#         self.step = 0\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def __call__(self, input_ids, scores):\n",
    "#         print('printing input_ids.size(), scores.size(), step, input_ids, dec_ids')\n",
    "#         print(input_ids.size(), scores.size(), self.step, input_ids, processor.batch_decode(input_ids))\n",
    "#         self.step+=1 \n",
    "\n",
    "#         return scores\n",
    "    \n",
    "# fusion_proc = ShallowFusion(\n",
    "#     lm=gpt2,\n",
    "#     shared_vocab=gpt2.config.vocab_size,\n",
    "#     eos=EOS_ID,\n",
    "#     alpha=0.3\n",
    "# )\n",
    "\n",
    "# batch = next(iter(loader))\n",
    "# feats = batch['input_features'].to(DEVICE)\n",
    "# masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# with torch.no_grad():\n",
    "\n",
    "#     out1 = whisper.generate(\n",
    "#         input_features=feats,\n",
    "#         attention_mask=masks,\n",
    "#         logits_processor=LogitsProcessorList([fusion_proc]),\n",
    "#         return_dict_in_generate=True,\n",
    "#         output_scores=True,\n",
    "#         num_beams=2,\n",
    "#     )\n",
    "\n",
    "#     # out2 = whisper.generate(\n",
    "#     #     input_features=feats,\n",
    "#     #     attention_mask=masks,\n",
    "#     #     return_dict_in_generate=True,\n",
    "#     #     output_scores=True,\n",
    "#     #     num_beams=2\n",
    "#     # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14230833",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader))\n",
    "feats = batch['input_features'].to(DEVICE)\n",
    "masks = batch['attention_mask'].to(DEVICE)\n",
    "\n",
    "# Generate to get sequences\n",
    "with torch.no_grad():\n",
    "    out = whisper.generate(\n",
    "        input_features=feats,\n",
    "        attention_mask=masks,\n",
    "        num_beams=1,\n",
    "        do_sample=False,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=5\n",
    "    )\n",
    "# Compare step 0\n",
    "decoder_ids = out.sequences[:,0:-1]  # Just the start token\n",
    "with torch.no_grad():\n",
    "    direct_logits = whisper(feats, decoder_input_ids=decoder_ids).logits[:, -1, :].to(DEVICE)\n",
    "    direct_lp = torch.log_softmax(direct_logits, dim=-1)\n",
    "\n",
    "gen_lp = out.scores[-1].to(DEVICE)\n",
    "\n",
    "print(gen_lp)\n",
    "print(direct_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4d3e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd359960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter truly out of bounds vocab >=EOS\n",
    "oob_mask = decoder_ids > EOS_ID # create mask for gpt2 OOV tokens emitted by whisper\n",
    " # replace with gpt2 pad token\n",
    "filtered = decoder_ids.masked_fill(oob_mask, gpt2_tok.pad_token_id)\n",
    "attention_mask = (filtered != gpt2_tok.pad_token_id).long()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_new = gpt2(input_ids=filtered, attention_mask=attention_mask).logits[:,-1, :]\n",
    "\n",
    "# because we dont want gpt2 to impact or determine termination just ASR model\n",
    "logits_new[:,:EOS_ID-1].size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
